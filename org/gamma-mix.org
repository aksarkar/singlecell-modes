#+TITLE: Poisson-unimodal Gamma mixture model
#+SETUPFILE: setup.org

* Introduction

  We have previously investigated the use of the following model for scRNA-seq
  counts coming from a single gene:

  \begin{align*}
    x_{ij} &\sim \operatorname{Poisson}(x_i^+ \lambda_{ij})\\
    \lambda_{ij} &\sim g_j(\cdot) = \sum_k \pi_{jk} \operatorname{Uniform}(\cdot; \lambda_{0j}, a_{jk})
  \end{align*}

  where \(x_i^+ = \sum_j x_{ij}\) and we abuse notation to allow \(a_{jk} <
  \lambda_{0j}\). In this model, expression variation is assumed to follow a
  unimodal non-parametric distribution \(g_j(\cdot)\), approximated as a large,
  but finite mixture of uniforms. The main challenge in fitting this model is
  estimating \(\lambda_{0j}\) (with \(\lambda_{0j}\) known, estimating \(g_j\)
  is a convex optimization problem solved by ~mixsqp~). The strategy taken in
  ~ashr~ is to treat the ~mixsqp~ problem as a subroutine in line search over
  \(\lambda_{0j}\).

  Here, we investigate using a unimodal mixture of Gammas (previously
  considered in [[https://dx.doi.org/10.1093%252Fbioinformatics%252Fbtw483][Lu & Stephens 2016]]), which is a slightly less flexible
  assumption on expression variation, but could allow faster mode estimation.

* Setup
  :PROPERTIES:
  :CUSTOM_ID: setup
  :END:

  #+BEGIN_SRC emacs-lisp :exports none
    (add-to-list 'python-shell-completion-native-disabled-interpreters "jupyter")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(venv="scmodes",partition="mstephens") :exports none :dir /scratch/midway2/aksarkar/modes

  #+RESULTS:
  : Submitted batch job 63622207

  #+BEGIN_SRC ipython
    import numpy as np
    import pandas as pd
    import rpy2.robjects.packages
    import rpy2.robjects.pandas2ri
    import scipy.optimize as so
    import scipy.stats as st

    ashr = rpy2.robjects.packages.importr('ashr')
    mixsqp = rpy2.robjects.packages.importr('mixsqp')
    rpy2.robjects.pandas2ri.activate()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[10]:
  :END:

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[2]:
  :END:

  #+BEGIN_SRC ipython
    import matplotlib.pyplot as plt
    plt.rcParams['figure.facecolor'] = 'w'
    plt.rcParams['font.family'] = 'Nimbus Sans'
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[3]:
  :END:

* Methods
** Unimodal mixture of Gammas

   Consider \(\lambda \sim \operatorname{Gamma}(\alpha, \beta)\). Let 
   \(\lambda_0 = (\alpha - 1) / \beta\) (the mode), and \(v = \alpha /
   \beta^2\) (the variance). Then,

   \begin{align*}
     \beta &= \frac{\lambda_0 + \sqrt{\lambda_0^2 + 4 v}}{2 v}\\
     \alpha &= \lambda_0 \frac{\lambda_0 + \sqrt{\lambda_0^2 + 4 v}}{2 v} + 1
   \end{align*}

   and for choice of mode \(\lambda_{0j}\) and grid of variances \(v_{j1}, \ldots,
   v_{jK}\) we have
   
   \[ \lambda_{ij} \sim g_j(\cdot) = \sum_{k=1}^K \pi_{jk} \operatorname{Gamma}(\cdot; \alpha_{jk}, \beta_{jk}) \]

   As an illustrative example,

   #+BEGIN_SRC ipython :ipyfile figure/gamma-mix.org/gamma-mix-ex.png
     lam0 = 1e-3
     # 1/s -> max(x/s)
     mixvar = np.exp(np.arange(np.log(1 / 1e5), np.log(5e-3), step=.5 * np.log(2)))
     grid = np.linspace(0, 1e-2, 1000)

     cm = plt.get_cmap('Paired')
     plt.clf()
     plt.gcf().set_size_inches(3, 3)
     for i, v in enumerate(mixvar[:5]):
       b = (lam0 + np.sqrt(lam0 * lam0 + 4 * v)) / (2 * v)
       a = lam0 * b + 1
       plt.plot(grid, st.gamma(a=a, scale=1 / b).pdf(grid), lw=1, c=cm(i))
     plt.axvline(x=lam0, c='r', ls=':', lw=1)
     plt.xlabel('$\lambda$')
     plt.ylabel('$f(\lambda)$')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[29]:
   [[file:figure/gamma-mix.org/gamma-mix-ex.png]]
   :END:

   The main advantage of assuming a mixture of Gammas is that the marginal
   likelihood of \(x_{ij}\) is then a mixture of Negative Binomial
   distributions

   \[ p(x_{1j}, \ldots, x_{nj} \mid x_1^+, \ldots, x_n^+, g) = \prod_i \sum_k
   \pi_{jk} \operatorname{NB}(x_{ij}; \alpha_{jk}, \frac{x_i^+}{x_i^+ +
   \beta_{jk}}). \]

** Expectation-maximization algorithm

   Introduce indicator variable \(z_i \in \{1, \ldots, K\}\) denoting which
   component \(\lambda_i\) was drawn from (dropping index \(j\)), and let
   \(\zeta_{ik} = p(z_i = k \mid x_i, x_i^+, g)\). Then

   \begin{multline*}
     l(\cdot) = \sum_i E_{z_i \mid x_i, x_i^+, g}[\ln p(x_i, z_i \mid x_i^+, g)] = \\
     \sum_{i, k} \zeta_{ik} \left( \ln \pi_k + x_i \ln\left(\frac{x_i^+}{x_i^+ + \beta_k}\right) + \alpha_k \ln\left(\frac{\beta_k}{x_i^+ + \beta_k}\right) + \ln\Gamma(x_i + \alpha_k) - \ln\Gamma(x_i + 1) - \ln\Gamma(\alpha_k) \right)  
   \end{multline*}

   suggesting an EM algorithm:

   *E step:* 

   \[ \zeta_{ik} \propto \pi_k \operatorname{NB}(x_{ij}; \alpha_{k}, \frac{x_i^+}{x_i^+ + \beta_{k}}) \]

   *M step:* 

   1. Update \(\boldsymbol\pi\) (solution to ~mixsqp~)
   2. Update \(\lambda_0\) (e.g. backtracking line search in direction of the
      gradient):

      \begin{align*}
        \frac{\partial l}{\partial \alpha_k} &= \sum_i \zeta_{ik}\left( \ln\left(\frac{\beta_k}{x_i^+ + \beta_k}\right) + \psi(x_i + \alpha_k) - \psi(\alpha_k) \right)\\
        \frac{\partial l}{\partial \beta_k} &= \sum_i \zeta_{ik} \left(\frac{\alpha_k - x_i}{x_i^+ + \beta_k} + \frac{\alpha_k}{\beta_k}\right)\\
        \frac{\partial \alpha_k}{\partial \beta_k} &= \lambda_0\\
        \frac{\partial \beta_k}{\partial \lambda_0} &= \frac{1}{2 v} + \frac{\lambda_0}{v \sqrt{\lambda_0^2 + 4 v}}
      \end{align*}

      where \(\psi\) denotes the [[https://en.wikipedia.org/wiki/Digamma_function][digamma function]].

* Results
** Tracing mode estimation of Poisson-unimodal Uniform mixture

   As an illustrative example, simulate a problem where the true \(g_j\) is a
   point mass, with large \(n\).

   #+BEGIN_SRC ipython
     np.random.seed(0)
     n = 50000
     s = np.random.poisson(lam=1e4, size=n)
     mu = 1e-4
     x = np.random.poisson(lam=s * mu)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[7]:
   :END:

   Trace how many function evaluations are used in estimating the mode.

   #+BEGIN_SRC ipython :async t
     def _ash(mode, x, s):
       return np.array(ashr.ash(
         pd.Series(np.zeros(n)),
         1,
         lik=ashr.lik_pois(y=pd.Series(x), scale=pd.Series(s), link='identity'),
         mixcompdist='halfuniform',
         outputlevel='loglik',
         mode=mode).rx2('loglik'))

     res = so.minimize_scalar(_ash, bracket=[(x / s).min(), (x / s).max()], args=(x, s), method='golden')
     res
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   0 - 61452f35-cdae-4963-b444-78319e5f5feb
   :END:
