#+TITLE: Low rank approximation
#+SETUPFILE: setup.org

* Introduction

  Suppose we have observations \(x_{ij} \sim f(\lambda_{ij})\). Rather than
  making a distributional assumption \(\lambda_j \sim g_j(\cdot)\) (as in
  [[file:deconvolution.org][expression deconvolution]]), we might make a /low
  rank assumption/ \(h(\lambda_{ij}) = [\mathbf{L F}]_{ij}\).

  This assumption can be interpreted as regularizing cells/genes towards each
  other, or as discovering clusters in the data.

* Setup

  #+BEGIN_SRC emacs-lisp :exports none
    (add-to-list 'python-shell-completion-native-disabled-interpreters "jupyter")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(venv="scmodes",partition="mstephens",mem="16G") :dir /scratch/midway2/aksarkar/modes

  #+RESULTS:
  : Submitted batch job 60042226

  #+NAME: imports
  #+BEGIN_SRC ipython
    import numpy as np
    import pandas as pd
    import scmodes
  #+END_SRC

  #+RESULTS: imports
  :RESULTS:
  # Out[1]:
  :END:

  #+RESULTS:
  :RESULTS:
  # Out[1]:
  :END:

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[2]:
  :END:

  #+BEGIN_SRC ipython
    import colorcet
    import matplotlib.pyplot as plt
    plt.rcParams['figure.facecolor'] = 'w'
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[4]:
  :END:

* Methods
** Low rank approximation of count data

   A number of methods have been proposed to estimate low rank structure from
   count data, several specialized for scRNA-seq data. 

   - Non-negative matrix factorization
     ([[https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf][Lee
     and Seung 2001]], [[https://arxiv.org/abs/1010.1763][FÃ©votte and Idier
     2011]])
   - Latent dirichlet allocation
     ([[http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf][Blei et al
     2003]],
     [[https://papers.nips.cc/paper/3902-online-learning-for-latent-dirichlet-allocation][Hoffman
     et al. 2010]], [[http://proceedings.mlr.press/v22/taddy12.html][Taddy
     2012]])
   - Hierarchical Bayesian Poisson Factorization
     ([[http://www.cs.columbia.edu/~blei/papers/GopalanHofmanBlei2015.pdf][Gopalan
     et al. 2015]],
     [[https://onlinelibrary.wiley.com/doi/full/10.15252/msb.20188557][Levitin
     et al. 2019]])
   - ZINB-WAVE ([[https://www.nature.com/articles/s41467-017-02554-5][Risso et
     al. 2018]])
   - scVI ([[https://www.nature.com/articles/s41592-018-0229-2][Lopez et al.
     2018]])
   - DCA ([[https://www.nature.com/articles/s41467-018-07931-2][Eraslan et
     al. 2019]])
   - GLM-PCA ([[https://www.biorxiv.org/content/10.1101/574574v1][Townes et
     al. 2019]])

   There is one fundamental modelling decision which is not obvious from first
   principles: what precisely is low rank? In other words, what is the
   functional form of \(h(\cdot)\)? The methods above use either: \(h(x) = x\),
   \(h(x) = \ln(x)\), or learn \(h\) from the data.

   Here, we evaluate methods on their ability to generalize to new data. We use
   real data, assuming:

   \[ x_{ij} \sim \mathrm{Poisson}(R_i \lambda_{ij}) \]

   \[ \lambda_{ij} = [\mathbf{L F}]_{ij} \]

   and hold out molecules by randomly thinning the observed counts:

   \[ y_{ij} \sim \mathrm{Binomial}(x_{ij}, 0.5) \]

   \[ \tilde{y}_{ij} = x_{ij} - y_{ij} \]

   This approach leaves the relative abundance of the transcripts unchanged in
   expectation, implying that the low rank structure learned in \(\mathbf{Y}\)
   should explain the data in \(\tilde{\mathbf{Y}}\).

   Our metric is then the likelihood of the held-out data. We simply need to
   re-scale to account for different size factors \(R_i\).

** LRA of transformed data

   We can write principal components analysis as a generative model (Tipping
   1999):

   \[ \mathbf{y}_i \sim \mathcal{N}(\mathbf{y}_i; \mathbf{W z}_i, \sigma^2 \mathbf{I}) \]

   Now, suppose \(y_{ij} = \ln(x_{ij} + \epsilon)\). Then, via
   change of variables we have:

   \[ p(\mathbf{x}_i) = \frac{1}{\mathbf{x}_i + \epsilon}
   \mathcal{N}(\ln(\mathbf{x}_i + \epsilon); \mathbf{W z}_i, \sigma^2
   \mathbf{I}) \]

   where we abuse notation for element-wise operations.

   In general, this means we can compare the likelihood of the observed data
   under models of different transformations of the observations.

** Datasets

   We use real scRNA-seq datasets in heterogeneous tissues to benchmark
   methods:

   1. Mouse neuron cells sequenced on the Fluidigm C1 platform
      ([[https://science.sciencemag.org/content/347/6226/1138.full][Zeisel et
      al. 2015]])

   2. Fresh PBMCs sequenced on the 10X platform
      ([[https://www.nature.com/articles/ncomms14049][Zheng et al. 2017]])

* Results
** iPSCs
   :PROPERTIES:
   :CUSTOM_ID: ipsc
   :END:

   #+NAME: pca
   #+BEGIN_SRC ipython :eval never
     <<imports>>
     import scipy.stats as st
     import sklearn.decomposition as skd

     ipsc = scmodes.dataset.ipsc('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/')
     res0 = skd.PCA(n_components=10).fit(ipsc)
     z_0 = res0.transform(ipsc)
     llik0 = st.norm(loc=z_0.dot(res0.components_), scale=np.sqrt(res0.noise_variance_)).logpdf(ipsc).sum()

     log_ipsc = np.log(ipsc + 1)
     res1 = skd.PCA(n_components=10).fit(log_ipsc)
     z_1 = res1.transform(ipsc)
     llik1 = (-log_ipsc + st.norm(loc=z_1.dot(res1.components_), scale=np.sqrt(res1.noise_variance_)).logpdf(log_ipsc)).sum()
     print(f'PCA of untransformed counts: {llik0:.4g}')
     print(f'PCA of log1p counts: {llik1:.4g}')
   #+END_SRC

   #+BEGIN_SRC sh :noweb eval :dir /scratch/midway2/aksarkar/modes/
     sbatch --partition=mstephens --mem=8G
     #!/bin/bash
     source activate scmodes
     python <<EOF
     <<pca>>
     EOF
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 60034134

   #+BEGIN_EXAMPLE
     PCA of untransformed counts: -4.712e+08
     PCA of log1p counts: -3.57e+09
   #+END_EXAMPLE

** Cortex
   :PROPERTIES:
   :CUSTOM_ID: cortex
   :END:

   #+NAME: cortex-bench
   #+BEGIN_SRC ipython :eval never
     <<imports>>
     import os
     tasks = ['nmf', 'glmpca']
     task = tasks[int(os.environ['SLURM_ARRAY_TASK_ID'])]
     cortex = scmodes.dataset.cortex('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/zeisel-2015/GSE60361_C1-3005-Expression.txt.gz', return_df=True)
     res = scmodes.benchmark.evaluate_lra_generalization(cortex, methods=[task], n_trials=1)
     res.to_csv(f'/scratch/midway2/aksarkar/modes/lra-generalization/cortex-{task}.txt.gz', compression='gzip', sep='\t')
   #+END_SRC

   #+BEGIN_SRC sh
     sbatch --partition=broadwl -a 1 -n1 -c28 --exclusive --mem=16G --time=60:00 --job-name=lra-benchmark --out=lra-benchmark.out   
     #!/bin/bash
     source activate scmodes
     python <<EOF
     <<cortex-bench>>
     EOF
   #+END_SRC

   #+BEGIN_SRC sh :noweb eval :session mstephens :dir /scratch/midway2/aksarkar/modes :results none
     python -i <<EOF
     <<imports>>
     cortex = scmodes.dataset.cortex('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/zeisel-2015/GSE60361_C1-3005-Expression.txt.gz', return_df=True)
     res = scmodes.benchmark.evaluate_lra_generalization(cortex, methods=['glmpca'], n_trials=1) 
     print(res)
     EOF
   #+END_SRC

** PBMCs
   :PROPERTIES:
   :CUSTOM_ID: pbmc
   :END:

   #+NAME: pbmcs-bench
   #+BEGIN_SRC ipython :eval never
     <<imports>>
     import os
     tasks = ['nmf', 'glmpca']
     task = tasks[int(os.environ['SLURM_ARRAY_TASK_ID'])]
     data = scmodes.dataset.read10x('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/fresh_68k_pbmc_donor_a/filtered_matrices_mex/hg19/')
     res = scmodes.benchmark.evaluate_lra_generalization(data, methods=[task])
     res.to_csv('/scratch/midway2/aksarkar/modes/lra-generalization/pbmcs-{task}.txt.gz')
   #+END_SRC

   #+BEGIN_SRC sh :noweb eval :dir /scratch/midway2/aksarkar/modes
     sbatch --partition=broadwl -a 0 -n1 -c28 --exclusive --mem=16G --time=60:00 --job-name=lra-benchmark --out=lra-benchmark.out
     #!/bin/bash
     source activate scmodes
     python <<EOF
     <<cortex-bench>>
     EOF
   #+END_SRC

   
