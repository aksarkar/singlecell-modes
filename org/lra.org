#+TITLE: Low rank approximation
#+SETUPFILE: setup.org

* Introduction

  The key idea of our approach to modeling scRNA-seq is to separate sampling
  variation and expression variation. This approach leads to the following
  multi-gene model for scRNA-seq data: \(
  \newcommand\ml{\mathbf{L}}
  \newcommand\mf{\mathbf{F}}
  \newcommand\my{\mathbf{Y}}
  \)

  \begin{align*}
    x_{ij} &\sim \operatorname{Poisson}(\lambda_{ij})\\
    \lambda_{ij} &= h^{-1}((\ml\mf')_{ij})
  \end{align*}

  where \(i = 1, \ldots, n\), \(j = 1, \ldots, p\), \(\ml\) is an \(n \times
  K\) matrix, and \(\mf\) is a \(p \times K\) matrix. (Here, we absorb the size
  factor into \(\ml\).) A number of methods have been proposed to fit this
  model (or extensions to it), several specialized for scRNA-seq data.

  - Non-negative matrix factorization
    ([[https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf][Lee
    and Seung 2001]], [[https://arxiv.org/abs/1010.1763][FÃ©votte and Idier
    2011]])
  - Negative Binomial Matrix Factorization
    ([[https://arxiv.org/abs/1801.01708][Gouvert et al. 2018]])
  - Latent dirichlet allocation
    ([[http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf][Blei et al
    2003]],
    [[https://papers.nips.cc/paper/3902-online-learning-for-latent-dirichlet-allocation][Hoffman
    et al. 2010]], [[http://proceedings.mlr.press/v22/taddy12.html][Taddy
    2012]])
  - Hierarchical Bayesian Poisson Factorization
    ([[https://www.hindawi.com/journals/cin/2009/785152/][Cemgil 2009]],
    [[http://www.cs.columbia.edu/~blei/papers/GopalanHofmanBlei2015.pdf][Gopalan
    et al. 2015]],
    [[https://onlinelibrary.wiley.com/doi/full/10.15252/msb.20188557][Levitin
    et al. 2019]])
  - ZINB-WAVE ([[https://www.nature.com/articles/s41467-017-02554-5][Risso et
    al. 2018]])
  - scVI ([[https://www.nature.com/articles/s41592-018-0229-2][Lopez et al.
    2018]])
  - DCA ([[https://www.nature.com/articles/s41467-018-07931-2][Eraslan et
    al. 2019]])
  - GLM-PCA ([[https://www.biorxiv.org/content/10.1101/574574v1][Townes et
    al. 2019]])

  Here, we evaluate methods on their ability to estimate \(\lambda\). We use
  Poisson thinning ([[https://arxiv.org/abs/1705.08393][Gerard and Stephens
  2017]], [[https://www.biorxiv.org/content/10.1101/758524v1][Gerard 2019]]) of
  real data

  \begin{align*}
    x_{ij} &\sim \mathrm{Poisson}(\lambda_{ij})\\
    y_{ij} &\sim \mathrm{Binomial}(x_{ij}, 0.5)\\
    \tilde{y}_{ij} &= x_{ij} - y_{ij}
  \end{align*}

  resulting in two matrices \(\my\) and \(\tilde{\my}\) with identical
  \(\lambda\). Our benchmark estimates \(\ml, \mf\) from \(\my\) and evaluates
  the estimate using the log likelihood of \(\tilde{\my}\).

* Setup

  #+BEGIN_SRC emacs-lisp :exports none
    (add-to-list 'python-shell-completion-native-disabled-interpreters "jupyter")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
    (org-babel-lob-ingest "lra.org")
  #+END_SRC

  #+RESULTS:
  : 3

  #+CALL: ipython3(venv="scmodes",partition="mstephens",memory="16G") :dir /scratch/midway2/aksarkar/modes

  #+RESULTS:
  : Submitted batch job 64164248

  #+NAME: imports
  #+BEGIN_SRC ipython
    import numpy as np
    import pandas as pd
    import scmodes
  #+END_SRC

  #+RESULTS: imports
  :RESULTS:
  # Out[1]:
  :END:

  #+BEGIN_SRC ipython
    import sklearn.decomposition as skd
    import scipy.stats as st
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[2]:
  :END:

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[3]:
  :END:

  #+BEGIN_SRC ipython
    import colorcet
    import matplotlib.pyplot as plt
    plt.rcParams['figure.facecolor'] = 'w'
    plt.rcParams['font.family'] = 'Nimbus Sans'
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[4]:
  :END:

* Methods
** Datasets

   #+NAME: data
   #+BEGIN_SRC ipython
     def _read_10x(k, min_detect=0.01, n_cells=1000, seed=1):
       return scmodes.dataset.read_10x(f'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/{k}/filtered_matrices_mex/hg19/', min_detect=0.01, return_df=True).sample(n=n_cells, axis=0, random_state=seed)

     def _mix_10x(k1, k2, min_detect=0.01, n_cells=1000, seed=1):
       x1 = scmodes.dataset.read_10x(f'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/{k1}/filtered_matrices_mex/hg19/', return_df=True, min_detect=0)
       x2 = scmodes.dataset.read_10x(f'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/{k2}/filtered_matrices_mex/hg19/', return_df=True, min_detect=0)
       return scmodes.dataset.synthetic_mix(x1, x2, min_detect=min_detect)[0].sample(n=n_cells, axis=0, random_state=seed)

     def _cd8_cd19_mix(**kwargs):
       return _mix_10x('cytotoxic_t', 'b_cells', **kwargs)

     def _cyto_naive_mix(**kwargs):
       return _mix_10x('cytotoxic_t', 'naive_t', **kwargs)

     data = {
       'cytotoxic_t': lambda: _read_10x('cytotoxic_t'),
       'b_cells': lambda: _read_10x('b_cells'),
       'ipsc': lambda: scmodes.dataset.ipsc('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/', return_df=True).sample(n=1000, axis=0, random_state=1),
       'cytotoxic_t-b_cells': _cd8_cd19_mix,
       'cytotoxic_t-naive_t': _cyto_naive_mix,
       'pbmcs_68k': lambda: _read_10x('fresh_68k_pbmc_donor_a'),
     }
   #+END_SRC

   Report the data dimensions.

   #+BEGIN_SRC ipython :async t
     pd.DataFrame([data[k]().shape for k in data],
                  columns=['num_cells', 'num_genes'],
                  index=data.keys())
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[87]:
   #+BEGIN_EXAMPLE
     num_cells  num_genes
     b_cells                  10085        375
     cytotoxic_t              10209        461
     naive_t                  10479        386
     cytotoxic_t-b_cells      20294        404
     cytotoxic_t-naive_t      20688        419
     pbmcs_68k                68579        404
   #+END_EXAMPLE
   :END:

* Results
** PCA on (log) counts
   :PROPERTIES:
   :CUSTOM_ID: ipsc
   :END:

   Investigate whether PCA on counts or log counts gives better log likelihood.
   We can write principal components analysis as a generative model (Tipping
   1999):

   \[ \mathbf{y}_i \sim \mathcal{N}(\mathbf{y}_i; \mathbf{W z}_i, \sigma^2 \mathbf{I}) \]

   Now, suppose \(y_{ij} = \ln(x_{ij} + \epsilon)\). Then, via
   change of variables we have:

   \[ p(\mathbf{x}_i) = \frac{1}{\mathbf{x}_i + \epsilon}
   \mathcal{N}(\ln(\mathbf{x}_i + \epsilon); \mathbf{W z}_i, \sigma^2
   \mathbf{I}) \]

   where we abuse notation for element-wise operations. In general, this means
   we can compare the likelihood of the observed data under models of different
   transformations of the observations.

   #+NAME: pca
   #+BEGIN_SRC ipython :eval never
     <<imports>>
     import scipy.stats as st
     import sklearn.decomposition as skd

     ipsc = scmodes.dataset.ipsc('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/')
     res0 = skd.PCA(n_components=10).fit(ipsc)
     z_0 = res0.transform(ipsc)
     llik0 = st.norm(loc=z_0.dot(res0.components_), scale=np.sqrt(res0.noise_variance_)).logpdf(ipsc).sum()

     log_ipsc = np.log(ipsc + 1)
     res1 = skd.PCA(n_components=10).fit(log_ipsc)
     z_1 = res1.transform(ipsc)
     llik1 = (-log_ipsc + st.norm(loc=z_1.dot(res1.components_), scale=np.sqrt(res1.noise_variance_)).logpdf(log_ipsc)).sum()
     print(f'PCA of untransformed counts: {llik0:.4g}')
     print(f'PCA of log1p counts: {llik1:.4g}')
   #+END_SRC

   #+BEGIN_SRC sh :noweb eval :dir /scratch/midway2/aksarkar/modes/
     sbatch --partition=mstephens --mem=8G
     #!/bin/bash
     source activate scmodes
     python <<EOF
     <<pca>>
     EOF
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 60034134

   #+BEGIN_EXAMPLE
     PCA of untransformed counts: -4.712e+08
     PCA of log1p counts: -3.57e+09
   #+END_EXAMPLE

** Poisson thinning benchmark

   Run the CPU methods.

   #+BEGIN_SRC sh :noweb eval :dir /scratch/midway2/aksarkar/modes/
     sbatch --partition=mstephens -n1 -c8 --time=3:00:00 --job-name=lra-generalization -a 0-2%1
     #!/bin/bash
     source activate scmodes
     python <<EOF
     <<imports>>
     import os
     <<data>>
     ranks = [1, 2, 4, 8]
     methods = ['nmf', 'glmpca']
     tasks = [(d, m, r) for d in data for m in methods for r in ranks]
     d, m, r = tasks[int(os.environ['SLURM_ARRAY_TASK_ID'])]
     x = data[d]()
     res = scmodes.benchmark.evaluate_lra_generalization(x, methods=[m], n_components=r, n_trials=1)
     res.to_csv(f'/scratch/midway2/aksarkar/modes/lra-generalization/{d}-{m}-{r}.txt.gz', compression='gzip', sep='\t')
     EOF
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 66251200

   Run the GPU methods.

   #+BEGIN_SRC sh :noweb eval :dir /scratch/midway2/aksarkar/modes/
     sbatch --partition=gpu2 --gres=gpu:1 --mem=16G --time=10:00 --job-name=lra-generalization -a 0-24
     #!/bin/bash
     source activate scmodes
     python <<EOF
     <<imports>>
     import os
     <<data>>
     methods = ['pvae']
     ranks = [1, 2, 4, 8]
     tasks = [(d, m, r) for d in data for m in methods for r in ranks]
     d, m, r = tasks[int(os.environ['SLURM_ARRAY_TASK_ID'])]
     x = data[d]()
     res = scmodes.benchmark.evaluate_lra_generalization(x, methods=[m], n_components=r, n_trials=1)
     res.to_csv(f'/scratch/midway2/aksarkar/modes/lra-generalization/{d}-{m}-{r}.txt.gz', compression='gzip', sep='\t')
     EOF
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 66251235

   Move the results to permanent storage.

   #+BEGIN_SRC sh
     rsync -au /scratch/midway2/aksarkar/modes/lra-generalization/ /project2/mstephens/aksarkar/projects/singlecell-modes/data/lra-generalization/
   #+END_SRC

   #+RESULTS:

   Read the results.

   #+BEGIN_SRC ipython
     methods = ['nmf', 'wglmpca', 'pvae']
     titles = ['B cell', 'Cytotoxic T', 'Naive T', 'B cell/T cell', 'Naive/cytotoxic T', 'PBMC']
     poisthin_res = pd.concat(
       {k: pd.concat([pd.read_csv(f'/project2/mstephens/aksarkar/projects/singlecell-modes/data/lra-generalization/{k}-{m}.txt.gz', header=[0, 1], index_col=0, sep='\t') for m in methods if not (k == 'pbmcs_68k' and m == 'wglmpca')], axis=1) for k in data})
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[44]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/lra.org/lra-bench.png
     plt.clf()
     fig, ax = plt.subplots(2, 3, sharex=True)
     fig.set_size_inches(5, 4)
     for a, k, t in zip(ax.ravel(), data, titles):
       llik = poisthin_res.loc[k]['validation']
       for x, m in enumerate(llik):
         a.scatter(x + np.random.normal(scale=0.1, size=llik.shape[0]), llik[m].values, s=1, c='k', zorder=3)
       a.grid(c='0.8', lw=1, axis='x')
       a.set_xlim(-0.5, 2.5)
       a.set_xticks(np.arange(3))
       a.set_title(t)
     for a in ax[-1]:
       a.set_xlabel('Method')
       a.set_xticklabels([m.upper() for m in llik.columns], rotation=90)
     for a in ax:
       a[0].set_ylabel('Validation log lik')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[60]:
   [[file:figure/lra.org/lra-bench.png]]
   :END:
