#+TITLE: Comparison of expression deconvolution approaches
#+SETUPFILE: setup.org
#+OPTIONS: toc:2

* Introduction

  Suppose we have observations \(x_i \sim f(\theta_i), i = 1, \ldots, n\), and
  \(\theta_i \sim g(\cdot)\). /Distribution deconvolution/ is the problem of
  estimating \(g \in \mathcal{G}\) from \(x_1, \ldots, x_n\), assuming \(f\) is known
  ([[https://academic.oup.com/biomet/article/103/1/1/2390141][Efron
  2016]]).

  Recent work suggests that scRNA-seq data follows this generative model
  ([[http://dx.doi.org/10.1073/pnas.1721085115][Wang et al. 2018]]). Here, we
  investigate the trade-off between model complexity/flexibility and
  generalization for different choices of \(\mathcal{G}\) in real data.

* Setup

  #+BEGIN_SRC emacs-lisp :exports none
    (add-to-list 'python-shell-completion-native-disabled-interpreters "jupyter")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(mem="8G",partition="mstephens",venv="scmodes") :dir /scratch/midway2/aksarkar/modes :exports none

  #+RESULTS:
  : Submitted batch job 59283152

  #+BEGIN_SRC ipython :tangle /project2/mstephens/aksarkar/projects/singlecell-modes/code/benchmark.py
    import functools as ft
    import gc
    import gzip
    import multiprocessing as mp
    import numpy as np
    import pandas as pd
    import scipy.io
    import scipy.stats as st
    import scipy.special as sp
    import sklearn.model_selection as skms

    import rpy2.robjects.packages
    import rpy2.robjects.pandas2ri
    import rpy2.robjects.numpy2ri

    rpy2.robjects.pandas2ri.activate()
    rpy2.robjects.numpy2ri.activate()

    ashr = rpy2.robjects.packages.importr('ashr')
    descend = rpy2.robjects.packages.importr('descend')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[1]:
  :END:

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[2]:
  :END:

  #+BEGIN_SRC ipython
    import colorcet
    import matplotlib.pyplot as plt
    plt.rcParams['figure.facecolor'] = 'w'
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[3]:
  :END:

* Methods
** Distribution deconvolution

   The general form of distribution deconvolution for scRNA-seq is:

   \[ x_{ij} \sim \mathrm{Poisson}(\exp(\mathbf{z}_i' \mathbf{b}_j) \lambda_{ij}) \]

   \[ \lambda_{ij} \sim g_j(\cdot) \]

   where:

   - \(x_{ij}\) is the count of molecules of gene \(j\) in cell \(i\)
   - \(\mathbf{z}_i\) is a \(q\)-vector of covariates for cell \(i\)
   - \(\mathbf{b}_j\) is a \(q\)-vector of confounding effects on gene \(j\)
   - \(\lambda_{ij}\) is proportional to the relative abundance of gene \(j\)
     in cell \(i\)

   The primary inference goal is to recover \(g_j\). A secondary goal could be
   to recover \(\lambda_{ij}\). We can trade off flexibility and complexity of
   \(g_j\) for ease of implementation and speed.

   1. *Point mass:* \(g_j = \delta_\mu\). We mention it for completeness.
   2. *Gamma:* \(g_j = \mathrm{Gamma}(\cdot)\). This leads to the negative
      binomial marginal likelihood, and can be motivated by the empirical
      observation that the counts are overdispersed.
   3. *Point-Gamma:* \(g_j = \pi_j \delta_0(\cdot) + (1 - \pi_j)
      \mathrm{Gamma}(\cdot)\). This leads to the zero-inflated negative
      binomial marginal likelihood, which is still analytic and therefore
      computationally favorable. The inclusion of the point mass can be
      motivated by theory suggesting a biological mechanism for bimodal gene
      expression ([[http://dx.doi.org/10.1126/science.1216379][Munsky et
      al. 2013]], [[http://dx.doi.org/10.1186/gb-2013-14-1-r7][Kim and Marioni
      2013]]).
   4. *Unimodal mixture of uniforms:* \(g_j = \pi_0 \delta_0(\cdot) + \sum_k
      \pi_k \mathrm{Uniform}(\cdot; \lambda_0, a_{jk})\), where \(\lambda_0\)
      is the mode ([[http://dx.doi.org/10.1093/biostatistics/kxw041][Stephens
      2016]]). The inclusion of the point mass technically makes this
      distribution bimodal.
   5. *Exponential family:* \(g_j = \exp(\mathbf{Q}\alpha - \phi(\alpha))\),
      where \(\mathbf{Q}\) is a [[https://en.wikipedia.org/wiki/B-spline][B
      spline spline basis matrix]] for a
      [[https://en.wikipedia.org/wiki/Cubic_Hermite_spline][natural cubic
      spline]]
      ([[https://www.rdocumentation.org/packages/splines/topics/ns][~ns~
      function]];
      [[https://academic.oup.com/biomet/article/103/1/1/2390141][Efron
      2016]]). The key idea of the method is use spline regression to find the
      sufficient statistic and natural parameters which maximizes the penalized
      likelihood of the observed data. The method has been extended to include
      a point mass on zero ([[http://dx.doi.org/10.1073/pnas.1721085115][Wang
      et al. 2018]]).
   6. *Nonparametric:* \(g_j = \sum_k \pi_k \mathrm{Uniform}(\cdot; ak, a(k +
      1))\), where \(a\) is a fixed step size
      ([[https://projecteuclid.org/euclid.aoms/1177728066][Kiefer and Wolfowitz
      1956]],
      [[https://amstat.tandfonline.com/doi/abs/10.1080/01621459.2013.869224][Koenker
      and Mizera 2014]]).

   In order to evaluate methods on their ability to estimate \(g_j\), we hold
   out a validation set, and compute the validation set log likelihood. To set
   a common baseline, we compare against the saturated model \(\hat\lambda_{ij}
   = x_{ij}\).

   #+NAME: score-impl
   #+BEGIN_SRC ipython :tangle /project2/mstephens/aksarkar/projects/singlecell-modes/code/benchmark.py
     def nb_llik(x, mean, inv_disp):
       return (x * np.log(mean / inv_disp) -
               x * np.log(1 + mean / inv_disp) -
               inv_disp * np.log(1 + mean / inv_disp) +
               sp.gammaln(x + inv_disp) -
               sp.gammaln(inv_disp) -
               sp.gammaln(x + 1))

     def score_nb(x_train, x_test, **kwargs):
       import scqtl
       onehot = np.ones((x_train.shape[0], 1))
       size_factor = x_train.sum(axis=1).reshape(-1, 1)
       design = np.zeros((x_train.shape[0], 1))
       log_mu, log_phi, *_ = scqtl.tf.fit(
         umi=x_train.astype(np.float32),
         onehot=onehot.astype(np.float32),
         design=design.astype(np.float32),
         size_factor=size_factor.astype(np.float32),
         learning_rate=1e-3,
         max_epochs=30000)
       return nb_llik(x_test, x_test.sum(axis=1, keepdims=True) * np.exp(log_mu), np.exp(-log_phi)).sum(axis=0)

     def softplus(x):
       return np.where(x > 30, x, np.log(1 + np.exp(x)))

     def zinb_llik(x, mean, inv_disp, logodds):
       case_zero = -softplus(-logodds) + softplus(nb_llik(x, mean, inv_disp) - logodds)
       case_non_zero = -softplus(logodds) + nb_llik(x, mean, inv_disp)
       return np.where(x < 1, case_zero, case_non_zero)

     def score_zinb(x_train, x_test, **kwargs):
       import scqtl
       onehot = np.ones((x_train.shape[0], 1))
       size_factor = x_train.sum(axis=1).reshape(-1, 1)
       init = scqtl.tf.fit(
         umi=x_train.astype(np.float32),
         onehot=onehot.astype(np.float32),
         size_factor=size_factor.astype(np.float32),
         learning_rate=1e-3,
         max_epochs=30000)
       log_mu, log_phi, logodds, *_ = scqtl.tf.fit(
         umi=x_train.astype(np.float32),
         onehot=onehot.astype(np.float32),
         size_factor=size_factor.astype(np.float32),
         learning_rate=1e-3,
         max_epochs=30000,
         warm_start=init[:3])
       return zinb_llik(x_test, x_test.sum(axis=1, keepdims=True) * np.exp(log_mu), np.exp(-log_phi), logodds).sum(axis=0)

     def _score_unimix(train, test, train_size_factor, test_size_factor):
       lam = train / train_size_factor
       try:
         res0 = ashr.ash_workhorse(
           # these are ignored by ash
           pd.Series(np.zeros(train.shape)),
           1,
           # numpy2ri doesn't DTRT, so we need to use pandas
           lik=ashr.lik_pois(y=pd.Series(train), scale=train_size_factor, link='identity'),
           mode=pd.Series([lam.min(), lam.max()]))
         res = ashr.ash_workhorse(
           pd.Series(np.zeros(test.shape)),
           1,
           lik=ashr.lik_pois(y=pd.Series(test), scale=test_size_factor, link='identity'),
           fixg=True,
           g=res0.rx2('fitted_g'))
         ret = np.array(res.rx2('loglik'))
       except:
         ret = -np.inf
       return ret

     def score_unimix(x_train, x_test, pool, **kwargs):
       result = []
       train_size_factor = pd.Series(x_train.sum(axis=1))
       test_size_factor = pd.Series(x_test.sum(axis=1))
       f = ft.partial(_score_unimix, train_size_factor=train_size_factor,
                      test_size_factor=test_size_factor)
       # np iterates over rows
       result = pool.starmap(f, zip(x_train.T, x_test.T))
       return np.array(result).ravel()

     def _score_descend(train, test, train_size_factor, test_size_factor):
       res = descend.deconvSingle(pd.Series(train), scaling_consts=train_size_factor, verbose=False)
       # DESCEND returns NA on errors
       if tuple(res.rclass) != ('DESCEND',):
         return -np.inf
       g = np.array(res.slots['distribution'])[:,:2]
       # Don't marginalize over lambda = 0 for x > 0, because p(x > 0 | lambda =
       # 0) = 0
       case_nonzero = (st.poisson(mu=test_size_factor * g[1:,0])
                       .logpmf(test.reshape(-1, 1))
                       .dot(g[1:,1]))
       case_zero = (st.poisson(mu=test_size_factor * g[:,0])
                    .logpmf(test.reshape(-1, 1))
                    .dot(g[:,1]))
       llik = np.where(test > 0, case_nonzero, case_zero).sum()
       return llik

     def score_descend(x_train, x_test, pool, **kwargs):
       result = []
       # numpy2ri doesn't DTRT, so we need to use pandas
       train_size_factor = pd.Series(x_train.sum(axis=1))
       test_size_factor = x_test.sum(axis=1).reshape(-1, 1)
       f = ft.partial(_score_descend, train_size_factor=train_size_factor,
                      test_size_factor=test_size_factor)
       result = pool.starmap(f, zip(x_train.T, x_test.T))
       return np.array(result).ravel()

     def _score_npmle(train, test, train_size_factor, test_size_factor, K):
       lam = train / train_size_factor
       grid = np.linspace(lam.min(), 2 * lam.max(), K + 1)
       try:
         res0 = ashr.ash_workhorse(
           # these are ignored by ash
           pd.Series(np.zeros(train.shape)),
           1,
           # numpy2ri doesn't DTRT, so we need to use pandas
           lik=ashr.lik_pois(y=pd.Series(train), scale=train_size_factor, link='identity'),
           g=ashr.unimix(pd.Series(np.ones(K) / K), pd.Series(grid[:-1]), pd.Series(grid[1:])))
         res = ashr.ash_workhorse(
           pd.Series(np.zeros(test.shape)),
           1,
           lik=ashr.lik_pois(y=pd.Series(test), scale=test_size_factor, link='identity'),
           fixg=True,
           g=res0.rx2('fitted_g'))
         ret = res.rx2('loglik')
       except:
         ret = -np.inf
       return ret

     def score_npmle(x_train, x_test, pool, K=100, **kwargs):
       result = []
       train_size_factor = pd.Series(x_train.sum(axis=1))
       test_size_factor = pd.Series(x_test.sum(axis=1))
       f = ft.partial(_score_npmle, train_size_factor=train_size_factor,
                      test_size_factor=test_size_factor, K=K)
       result = pool.starmap(f, zip(x_train.T, x_test.T))
       return np.array(result).ravel()

     def score_saturated(x_train, x_test, **kwargs):
       return st.poisson(mu=x_test).logpmf(x_test).sum(axis=0)

     def evaluate_generalization(x, pool, methods=None, **kwargs):
       result = {}
       train, val = skms.train_test_split(x, **kwargs)
       if methods is None:
         methods = ['nb', 'zinb', 'unimix', 'descend', 'npmle', 'saturated']
       for m in methods:
         # Hack: get functions by name
         result[m] = globals()[f'score_{m}'](train, val, pool=pool)
       return pd.DataFrame.from_dict(result, orient='columns')
   #+END_SRC

   #+RESULTS: score-impl
   :RESULTS:
   # Out[4]:
   :END:

* Results
** Homogeneous cell populations
*** Examples
    :PROPERTIES:
    :CUSTOM_ID: examples
    :END:

   Use sorted cells from [[https://www.nature.com/articles/ncomms14049][Zheng
   et al. 2017]].

   #+NAME: read-zheng
   #+BEGIN_SRC ipython :tangle /project2/mstephens/aksarkar/projects/singlecell-modes/code/benchmark.py
     def read_10x(prefix, min_detect=0.25, return_df=False):
       counts = scipy.io.mmread(f'{prefix}/matrix.mtx.gz').tocsr()
       keep = ((counts > 0).mean(axis=1) >= min_detect).A.ravel()
       counts = counts[keep].T.A.astype(np.int)
       if return_df:
         genes = pd.read_csv(f'{prefix}/genes.tsv.gz', sep='\t', header=None)
         return pd.DataFrame(counts, columns=genes.loc[keep, 0])
       else:
         return counts

     def cd8_cytotoxic_t_cells(**kwargs):
       return read_10x('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/cytotoxic_t/filtered_matrices_mex/hg19', **kwargs)

     def cd19_b_cells(**kwargs):
       return read_10x('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/b_cells/filtered_matrices_mex/hg19/', **kwargs)
   #+END_SRC

   #+RESULTS: read-zheng
   :RESULTS:
   # Out[20]:
   :END:

   Look at some examples.

   #+BEGIN_SRC ipython
     x = cd8_cytotoxic_t_cells()
     xj = pd.Series(x[:,x.mean(axis=0).argmax()])
     size_factor = pd.Series(x.sum(axis=1))
     lam = xj / size_factor
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[4]:
   :END:

   Fit Poisson ash.

   #+BEGIN_SRC ipython
     unimix_res = ashr.ash_workhorse(
       pd.Series(np.zeros(x.shape[0])),
       1,
       lik=ashr.lik_pois(y=xj, scale=size_factor, link='identity'),
       mode=pd.Series([lam.min(), lam.max()]))
     unimix_cdf = ashr.cdf_ash(unimix_res, np.linspace(lam.min(), lam.max(), 1000))
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[7]:
   :END:

   Fit NPMLE.

   #+BEGIN_SRC ipython
     K = 100
     grid = np.linspace(lam.min(), lam.max(), K + 1)
     npmle_res = ashr.ash_workhorse(
       pd.Series(np.zeros(x.shape[0])),
       1,
       lik=ashr.lik_pois(y=xj, scale=size_factor, link='identity'),
       g=ashr.unimix(pd.Series(np.ones(K) / K), pd.Series(grid[:-1]), pd.Series(grid[1:])))
     npmle_cdf = ashr.cdf_ash(npmle_res, np.linspace(lam.min(), lam.max(), 1000))
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[12]:
   :END:

   Fit DESCEND.

   #+BEGIN_SRC ipython
     descend_res = descend.deconvSingle(xj, scaling_consts=size_factor, verbose=False)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[14]:
   :END:

   Fit NB/ZINB.

   #+BEGIN_SRC ipython :eval never :noweb tangle :tangle /project2/mstephens/aksarkar/projects/singlecell-modes/code/fit-nb.py
     import numpy as np
     import pandas as pd
     import scipy.io
     import scqtl
     <<read-zheng>>
     x = cd8_cytotoxic_t_cells()
     size_factor = x.sum(axis=1).reshape(-1, 1)
     onehot = np.ones((x.shape[0], 1))
     design = np.zeros((x.shape[0], 1))
     init = scqtl.tf.fit(
       umi=x.astype(np.float32),
       onehot=onehot.astype(np.float32),
       design=design.astype(np.float32),
       size_factor=size_factor.astype(np.float32),
       learning_rate=1e-3,
       max_epochs=30000,
       verbose=True)
     pd.DataFrame(init[0]).to_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-nb-log-mu.txt.gz', compression='gzip', sep='\t')
     pd.DataFrame(init[1]).to_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-nb-log-phi.txt.gz', compression='gzip', sep='\t')
     log_mu, log_phi, logodds, nb_llik, zinb_llik = scqtl.tf.fit(
       umi=x.astype(np.float32),
       onehot=onehot.astype(np.float32),
       design=design.astype(np.float32),
       size_factor=size_factor.astype(np.float32),
       learning_rate=1e-3,
       max_epochs=30000,
       warm_start=init[:3],
       verbose=True)
     pd.DataFrame(log_mu).to_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-zinb-log-mu.txt.gz', compression='gzip', sep='\t')
     pd.DataFrame(log_phi).to_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-zinb-log-phi.txt.gz', compression='gzip', sep='\t')
     pd.DataFrame(logodds).to_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-zinb-logodds.txt.gz', compression='gzip', sep='\t')
   #+END_SRC

   #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/modes/
     sbatch --partition=gpu2 --gres=gpu:1 --mem=16G --time=60:00 --job-name=fit-nb
     #!/bin/bash
     source activate scmodes
     python /project2/mstephens/aksarkar/projects/singlecell-modes/code/fit-nb.py
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 58803475

   #+BEGIN_SRC ipython
     j = str(x.mean(axis=0).argmax())
     nb_log_mu = pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-nb-log-mu.txt.gz', sep='\t')
     nb_log_phi = pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-nb-log-phi.txt.gz', sep='\t')
     # Gamma (Use MATLAB and MATHEMATICA (b=theta=scale, a=alpha=shape) definition)
     # https://github.com/scipy/scipy/blob/v1.2.1/scipy/stats/_continuous_distns.py#L2479     
     gamma_cdf = st.gamma(a=np.exp(-nb_log_phi[j]), scale=np.exp(nb_log_mu[j] + nb_log_phi[j])).cdf(np.linspace(lam.min(), lam.max(), 1000))
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[15]:
   :END:

   #+BEGIN_SRC ipython
     zinb_log_mu = pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-zinb-log-mu.txt.gz', sep='\t')
     zinb_log_phi = pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-zinb-log-phi.txt.gz', sep='\t')
     zinb_logodds = pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-zinb-log-phi.txt.gz', sep='\t')
     point_gamma_cdf = st.gamma(a=np.exp(-zinb_log_phi[j]), scale=np.exp(zinb_log_mu[j] + zinb_log_phi[j])).cdf(np.linspace(lam.min(), lam.max(), 1000))
     point_gamma_cdf *= sp.expit(-zinb_logodds[j].values)
     point_gamma_cdf += sp.expit(zinb_logodds[j].values)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[16]:
   :END:

   Plot the observed counts and deconvolved distributions.

   #+BEGIN_SRC ipython :ipyfile figure/deconvolution.org/ash-example.png
     cm = plt.get_cmap('Dark2').colors
     fig, ax = plt.subplots(2, 1)
     ax[0].hist(xj, bins=100, color='k')
     ax[0].set_xlabel('Molecule count')
     ax[0].set_ylabel('Number of cells')

     ax[1].plot(np.linspace(lam.min(), lam.max(), 1000), gamma_cdf, color=cm[0], lw=1, label='Gamma')
     ax[1].plot(np.linspace(lam.min(), lam.max(), 1000), point_gamma_cdf, color=cm[1], lw=1, label='Point-Gamma')
     ax[1].plot(np.array(unimix_cdf.rx2('x')),
                np.array(unimix_cdf.rx2('y')).ravel(), c=cm[2], lw=1, label='Unif mix')
     F = np.cumsum(np.array(descend_res.slots['density.points'])[:,1])
     ax[1].plot(np.array(descend_res.slots['density.points'])[:,0],
                F / F.max(), c=cm[3], lw=1, label='DESCEND')
     ax[1].plot(np.array(npmle_cdf.rx2('x')),
                np.array(npmle_cdf.rx2('y')).ravel(), c=cm[4], lw=1, label='NPMLE')
     ax[1].set_xlabel('Latent gene expression')
     ax[1].set_ylabel('CDF')

     ax[1].legend(frameon=False)

     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[20]:
   [[file:figure/deconvolution.org/ash-example.png]]
   :END:

*** Speed up ash mode estimation
    :PROPERTIES:
    :CUSTOM_ID: mode-estimation
    :END:

    To estimate the mode \(\lambda_{0j}\) for gene \(j\), we find:

    \[ \lambda_{0j}^* = \arg\max_{\lambda_{0j}} \sum_i \int f(x_i \mid \lambda_i) g_j(\lambda_i \mid \pi, \lambda_{0j})\ d\lambda_i \]

    using
    [[https://www.rdocumentation.org/packages/stats/versions/3.5.3/topics/optimize][golden
    section search]]. Investigate two issues:

    *First,* Mengyin Liu claimed this problem is convex in \(\lambda_{0j}\),
    However, on the above example, the quality of the result depends on the
    bounds of the search. Is this problem actually convex?

    By default, the bounds are \([\min(x_i), \max(x_i)]\), which can be
    extremely large. However, we need to remove the scaling factor, so should
    we instead search over \([\min(x_i / R_i), \max(x_i / R_i)]\)? The motivation for the
    proposed alternative is to only look over plausible values of
    \(\lambda_i\).

    #+BEGIN_SRC ipython :async t
      grid = np.geomspace(1e-3, xj.max(), 100)
      llik = np.array([np.array(
        ashr.ash(
          pd.Series(np.zeros(xj.shape)),
          1,
          lik=ashr.lik_pois(y=xj, scale=size_factor, link='identity'),
          mode=lam0,
          outputlevel='loglik').rx2('loglik')) for lam0 in grid]).ravel()
    #+END_SRC

    #+RESULTS:
    :RESULTS:
    # Out[12]:
    :END:

    #+BEGIN_SRC ipython
      res0 = ashr.ash(
        pd.Series(np.zeros(xj.shape)),
        1,
        lik=ashr.lik_pois(y=xj, scale=size_factor, link='identity'),
        mode='estimate')
      res1 = ashr.ash(
        pd.Series(np.zeros(x.shape[0])),
        1,
        lik=ashr.lik_pois(y=xj, scale=size_factor, link='identity'),
        mode=pd.Series([lam.min(), lam.max()]))
    #+END_SRC

    #+RESULTS:
    :RESULTS:
    # Out[17]:
    :END:

    #+BEGIN_SRC ipython :ipyfile figure/deconvolution.org/mode-est.png
      plt.clf()
      plt.gcf().set_size_inches(3, 3)
      plt.xscale('log')
      plt.plot(grid, np.array(llik).ravel(), lw=1, c='k')
      plt.axvline(x=np.array(res0.rx2('fitted_g').rx2('a'))[0], c='k', lw=1, ls=':', label='Default')
      plt.axvline(x=np.array(res1.rx2('fitted_g').rx2('a'))[0], c='r', lw=1, ls=':', label='Restricted')
      plt.legend(frameon=False, loc='center left', bbox_to_anchor=(1, .5))
      plt.xlabel('Mode $\lambda_0$')
      _ = plt.ylabel('Marginal likelihood')
    #+END_SRC

    #+RESULTS:
    :RESULTS:
    # Out[22]:
    [[file:figure/deconvolution.org/mode-est.png]]
    :END:

    It appears the problem is actually non-convex. Surprisingly, it appears
    non-convex even for a case where the data are not bimodal. For bimodal
    data, we might expect that choice of the mode would change the weight
    on/near zero and result in a non-convex objective.

    According to the documentation, the search can fail for poor choice of
    initial query, which depends entirely on the initial interval. In this
    example, the initial interval does not contain the mode, and therefore the
    search finds the correct local optimum within the interval, but fails to
    find the global optimum.

    This result does not necessarily mean that our proposed alternative, to
    search over \([\min(x_i / R_i), \max(x_i / R_i)]\) will work, because
    Poisson noise could mean the true \(\lambda_i > x_i / R_i\) for some sample
    \(i\). 

    Should we search further to be reasonably certain we haven't missed the
    mode? Intuitively, the largest \(\hat\lambda_i\) value we do observe should
    be "overestimated"; if it were not, then we should expect higher density of
    \(g\) around it, and values larger than it in the observed data.

    *Second,* we have to solve an ~ash~ subproblem for each query
    \(\lambda_0\), which becomes extremely expensive for large data sets. We
    can speed up the procedure by downsampling the data for mode
    estimation. How much worse is the fitted model?

    #+BEGIN_SRC ipython
      def score_mode_estimation(data, seed=0, p=0.1):
        temp = data.sample(random_state=seed, frac=p)
        res0 = ashr.ash(
          pd.Series(np.zeros(temp.shape[0])),
          1,
          lik=ashr.lik_pois(y=temp['x'], scale=temp['scale'], link='identity'),
          mode=pd.Series([temp['lam'].min(), temp['lam'].max()]))
        lam0 = np.array(res0.rx2('fitted_g').rx2('a'))[0]
        res = ashr.ash(
          pd.Series(np.zeros(data.shape[0])),
          1,
          lik=ashr.lik_pois(y=data['x'], scale=data['scale'], link='identity'),
          mode=lam0)
        return lam0, np.array(res.rx2('loglik'))[0]

      def evaluate_mode_estimation(data, num_trials):
        result = []
        for p in (0.1, 0.25, 0.5):
          for trial in range(num_trials):
            lam0, llik = score_mode_estimation(data, seed=trial, p=p)
            result.append([p, trial, lam0, llik])
        result = pd.DataFrame(result, columns=['p', 'trial', 'lam0', 'llik'])
        return result
    #+END_SRC

    #+RESULTS:
    :RESULTS:
    # Out[58]:
    :END:

    #+BEGIN_SRC ipython :async t
      mode_estimation_result = evaluate_mode_estimation(pd.DataFrame({'x': xj, 'scale': size_factor, 'lam': lam}), num_trials=10)
    #+END_SRC

    #+RESULTS:
    :RESULTS:
    # Out[59]:
    :END:

    #+BEGIN_SRC ipython :ipyfile figure/deconvolution.org/downsampling-mode-estimation.png
      plt.clf()
      plt.gcf().set_size_inches(3, 3)
      plt.scatter(mode_estimation_result['p'], mode_estimation_result['llik'], s=4, c='k')
      plt.axhline(y=np.array(res1.rx2('loglik'))[0], c='k', lw=1, ls=':')
      plt.xlabel('Fraction of original data')
      plt.ylabel('Training log likelihood')
    #+END_SRC

    #+RESULTS:
    :RESULTS:
    # Out[66]:
    : Text(0, 0.5, 'Training log likelihood')
    [[file:figure/deconvolution.org/downsampling-mode-estimation.png]]
    :END:

    Downsampling is likely to result in a *much* worse model fit, so we should
    not pursue that strategy to speed up the model estimation.

*** Run the benchmark
    :PROPERTIES:
    :CUSTOM_ID: homogeneous-results
    :END:

   Use iPSCs derived from Yoruba LCLs
   ([[https://www.biorxiv.org/content/early/2018/09/23/424192][Sarkar et
   al. 2018]]). The data were sequenced on the Fluidigm C1 platform to much
   greater depth, allowing many more genes to be detected (especially lower
   expressed genes), potentially including genes with bimodal steady state gene
   expression. Because many more genes are detected, we need to randomly
   subsample genes for CPU-based methods to complete in a reasonable budget.

   #+NAME: read-ipsc
   #+BEGIN_SRC ipython :tangle /project2/mstephens/aksarkar/projects/singlecell-modes/code/benchmark.py
     def ipsc(prefix='/project2/mstephens/aksarkar/projects/singlecell-qtl/data', return_df=False, query=None, n=None, seed=0):
       annotations = pd.read_csv(f'{prefix}/scqtl-annotation.txt', sep='\t')
       keep_samples = pd.read_csv(f'{prefix}/quality-single-cells.txt', sep='\t', index_col=0, header=None)
       keep_genes = pd.read_csv(f'{prefix}/genes-pass-filter.txt', sep='\t', index_col=0, header=None)
       if query is not None:
         keep_genes = keep_genes[keep_genes.index.isin(query)]
       if n is not None:
         keep_genes = keep_genes.sample(n=n, random_state=seed)
       annotations = annotations.loc[keep_samples.values.ravel()]
       result = []
       for chunk in pd.read_csv(f'{prefix}/scqtl-counts.txt.gz', sep='\t', index_col=0, chunksize=100):
          x = (chunk
               .loc[:,keep_samples.values.ravel()]
               .filter(items=keep_genes[keep_genes.values.ravel()].index, axis='index'))
          if not x.empty:
            result.append(x)
       result = pd.concat(result)
       # Return samples x genes
       if return_df:
         return result.T
       else:
         return result.values.T
   #+END_SRC

   #+RESULTS: read-ipsc
   :RESULTS:
   # Out[47]:
   :END:

   Run the benchmark.

   #+NAME: run-homogeneous-benchmark-cpu
   #+BEGIN_SRC ipython :eval never
     import os
     cpu_methods = ['unimix', 'descend', 'npmle', 'saturated']
     task = int(os.environ['SLURM_ARRAY_TASK_ID'])
     with mp.Pool(maxtasksperchild=10) as pool:
       x = ipsc(n=100, return_df=True)
       res = evaluate_generalization(x.values, pool=pool, test_size=0.1, random_state=0, methods=cpu_methods[task:task + 1])
       res.index = x.columns
       res.to_csv(f'/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/ipsc-{cpu_methods[task]}.txt.gz', compression='gzip', sep='\t')
   #+END_SRC

   #+BEGIN_SRC sh :noweb eval :dir /scratch/midway2/aksarkar/modes/
     sbatch --partition=broadwl --mem=32G -a 0-3 -n1 -c28 --exclusive --time=6:00:00 --job-name=benchmark --output=benchmark-cpu.out
     #!/bin/bash
     module load cuda/9.0
     source activate scmodes
     python -i /project2/mstephens/aksarkar/projects/singlecell-modes/code/benchmark.py <<EOF
     <<run-homogeneous-benchmark-cpu>>
     EOF
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 59234912

   #+NAME: run-homogeneous-benchmark-gpu
   #+BEGIN_SRC ipython :eval never
     x = ipsc(return_df=True)
     res = evaluate_generalization(x.values, pool=None, test_size=0.1, random_state=0, methods=['nb', 'zinb'])
     res.index = x.columns
     res.to_csv(f'/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/ipsc-gpu.txt.gz', compression='gzip', sep='\t')
   #+END_SRC

   #+BEGIN_SRC sh :noweb eval :dir /scratch/midway2/aksarkar/modes/
     sbatch --partition=gpu2 --gres=gpu:1 --mem=16G --time=12:00:00 --job-name=benchmark --output=benchmark-gpu.out
     #!/bin/bash
     module load cuda/9.0
     source activate scmodes
     python -i /project2/mstephens/aksarkar/projects/singlecell-modes/code/benchmark.py <<EOF
     <<run-homogeneous-benchmark-gpu>>
     EOF
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 59246091

   Read the results.

   #+BEGIN_SRC ipython
     benchmark = {}
     for data in ('cd8_cytotoxic_t_cells', 'cd19_b_cells'):
       benchmark[data] = pd.read_csv(f'/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/{data}.txt.gz', sep='\t', index_col=0)
     benchmark['ipsc'] = (
       pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/ipsc-gpu.txt.gz', index_col=0, sep='\t')
       .merge(pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/ipsc-unimix.txt.gz', index_col=0, sep='\t'), on='gene')
       .merge(pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/ipsc-descend.txt.gz', sep='\t'), on='gene')
       .merge(pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/ipsc-npmle.txt.gz', index_col=0, sep='\t'), on='gene')
       .merge(pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/ipsc-saturated.txt.gz', index_col=0, sep='\t'), on='gene')
       .set_index('gene'))
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[106]:
   :END:

   Plot the generalization performance of each method.

   #+BEGIN_SRC ipython :ipyfile figure/deconvolution.org/homogeneous-benchmark.png
     fig, ax = plt.subplots(1, 3)
     fig.set_size_inches(7, 3)
     for i, k in enumerate(sorted(benchmark.keys())):
       ax[i].boxplot((benchmark[f'{k}'].values - benchmark[f'{k}']['nb'].values.reshape(-1, 1))[:,1:-1],
                     widths=0.25, medianprops={'color': 'k'}, flierprops={'marker': '.', 'markersize': 4})
       ax[i].set_xticklabels(benchmark[f'{k}'].columns[1:-1], rotation=90)
       ax[i].set_xlabel('Method')
       ax[i].set_title(k)
     ax[0].set_ylabel('Diff val set log lik from NB')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[19]:
   [[file:figure/deconvolution.org/homogeneous-benchmark.png]]
   :END:

   The results suggest that NB (assuming Gamma-distributed latent gene
   expression) is adequate to accurately estimate \(g\) from the data.

   Of note, ~ash~ is >10\(\times\) slower than ~descend~, and ~descend~ is
   faster on small data sets than GPU-based methods. 

*** Investigate ZINB vs. NB in high-depth data
    :PROPERTIES:
    :CUSTOM_ID: bimodal-example-ipsc
    :END:

   Try to find a case in the full iPSC data where ZINB is required, by
   performing likelihood ratio tests against NB. Use Bonferroni correction to
   control FWER.

   #+BEGIN_SRC ipython
     ipsc_res = pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/ipsc-gpu.txt.gz', index_col=0, sep='\t')
     lrt = st.chi2(1).sf(-2 * (ipsc_res['nb'] - ipsc_res['zinb']))
     ipsc_res[lrt < .05 / lrt.shape[0]]
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[4]:
   #+BEGIN_EXAMPLE
     nb         zinb
     gene
     ENSG00000112530 -2017.717750 -1987.894352
     ENSG00000129824 -1803.651932 -1789.106410
   #+END_EXAMPLE
   :END:

   #+BEGIN_SRC ipython :async t
     ipsc_counts = ipsc(query=list(ipsc_res[lrt < .05 / lrt.shape[0]].index), return_df=True)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[7]:
   :END:

   #+BEGIN_SRC ipython
     gene_info = pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-genes.txt.gz', sep='\t', index_col=0)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[66]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/deconvolution.org/ipsc-zinb.png
     plt.clf()
     fig, ax = plt.subplots(1, 2)
     fig.set_size_inches(6, 3)
     for a, k in zip(ax, ipsc_counts):
       a.hist(ipsc_counts[k], bins=ipsc_counts[k].max() + 1, color='k')
       a.set_title(gene_info.loc[k, 'name'])
       a.set_xlabel('Number of molecules')
       a.set_ylabel('Number of cells')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[54]:
   [[file:figure/deconvolution.org/ipsc-zinb.png]]
   :END:

   /PACRG/ ([[https://www.omim.org/entry/608427][Parkin coregulated gene]])
   shares a promoter with /PKRG/ (/PARK2/;
   [[https://www.omim.org/entry/602544?search=park2&highlight=park2][Parkin]]). PARKIN
   is involved in a [[https://www.pnas.org/content/115/2/E180][signaling
   pathway for mitochondria damage]], and mitochrondrial damage appears to be
   relevant to neurodegenerative disease etiology
   ([[https://www.ncbi.nlm.nih.gov/pubmed/29414602][Alzheimer's disease]] and
   [[https://www.pnas.org/content/115/2/E180][Parkinson's disease]]).
   Mitochrondrial damage could explain the bimodal distribution of /PACRG/ in
   the iPSC data: the reprogramming protocol could induce oxidative stress on
   some of the cells, resulting in damage.

   /RPS4Y1/ ([[https://www.omim.org/entry/470000][Ribosomal protein S4,
   Y-linked]]) is on the Y chromosome, and therefore we should expect it to
   only be expressed in males. The homologous gene on the X chromosome
   ([[https://www.omim.org/entry/312760][/RPS4X/]]) has a different coding
   sequence. Sex-linkage should fully explain why this gene exhibits bimodal
   gene expression.

   Look at the full benchmark results for NB vs. ZINB (only the GPU
   implementation is suitable for this).

   #+BEGIN_SRC ipython :ipyfile figure/deconvolution.org/homogeneous-nb-zinb-benchmark.png
     plt.clf()
     plt.gcf().set_size_inches(3, 3)
     plt.plot([ipsc_res['nb'].min(), ipsc_res['nb'].max()], [ipsc_res['nb'].min(), ipsc_res['nb'].max()], c='r', ls='--', lw=1)
     plt.scatter(ipsc_res['nb'], ipsc_res['zinb'], c='k', s=2, alpha=.25)
     plt.xlabel('NB validation log lik.')
     plt.ylabel('ZINB validation log lik.')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[49]:
   : Text(0, 0.5, 'ZINB validation log lik.')
   [[file:figure/deconvolution.org/homogeneous-nb-zinb-benchmark.png]]
   :END:

*** Investigate genes with support for more complex models

    In B cells, several genes are better fit by DESCEND than other methods.

    #+BEGIN_SRC ipython
      b_cells = read_10x('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/b_cells/filtered_matrices_mex/hg19/', return_df=True)
    #+END_SRC

    #+RESULTS:
    :RESULTS:
    # Out[110]:
    :END:

    #+BEGIN_SRC ipython :ipyfile figure/deconvolution.org/b-cell-descend-examples.png
      query = benchmark['cd19_b_cells'][benchmark['cd19_b_cells']['descend'] > benchmark['cd19_b_cells']['nb']].index
      plt.clf()
      fig, ax = plt.subplots(6, 6, sharey=True, sharex=True)
      fig.set_size_inches(9, 9)
      for a, k in zip(ax.ravel(), query):
        a.hist(b_cells.iloc[:,k], bins=b_cells.iloc[:,k].max(), color='k')
        a.set_title(gene_info.loc[b_cells.columns[k], 'name'] if b_cells.columns[k] in gene_info.index else b_cells.columns[k])
      for y in range(ax.shape[0]):
        ax[y][0].set_ylabel('Number of cells')
      for x in range(ax.shape[1]):
        ax[-1][x].set_xlabel('Num mols')
      fig.tight_layout()
    #+END_SRC

    #+RESULTS:
    :RESULTS:
    # Out[112]:
    [[file:figure/deconvolution.org/b-cell-descend-examples.png]]
    :END:

    In iPSC, several genes appear to be better fit by mixture of uniforms than
    other methods.

    #+BEGIN_SRC ipython :async t
      ipsc_counts = ipsc(n=100, return_df=True)
    #+END_SRC

    #+BEGIN_SRC ipython :ipyfile figure/deconvolution.org/ipsc-unimix-examples.png
      query = benchmark['ipsc'][benchmark['ipsc']['unimix'] > benchmark['ipsc']['nb']].index
      plt.clf()
      fig, ax = plt.subplots(2, 3)
      fig.set_size_inches(6, 4)
      for a, k in zip(ax.ravel(), query):
        a.hist(ipsc_counts.loc[:,k], bins=ipsc_counts.loc[:,k].max(), color='k')
        a.set_title(gene_info.loc[k, 'name'])
      for y in range(ax.shape[0]):
        ax[y][0].set_ylabel('Number of cells')
      for x in range(ax.shape[1]):
        ax[-1][x].set_xlabel('Num mols')
      fig.tight_layout()
    #+END_SRC

    #+RESULTS:
    :RESULTS:
    # Out[114]:
    [[file:figure/deconvolution.org/ipsc-unimix-examples.png]]
    :END:

    In iPSC, several genes appear to be better fit by NPMLE than other methods.

    #+BEGIN_SRC ipython :ipyfile figure/deconvolution.org/ipsc-npmle-examples.png
      llik_diff = benchmark['ipsc']['npmle'] - benchmark['ipsc']['nb']
      query = benchmark['ipsc'].loc[llik_diff[llik_diff > 0].sort_values(ascending=False).head(n=12).index].index
      plt.clf()
      fig, ax = plt.subplots(3, 4)
      fig.set_size_inches(6, 4)
      for a, k in zip(ax.ravel(), query):
        a.hist(ipsc_counts.loc[:,k], bins=ipsc_counts.loc[:,k].max(), color='k')
        a.set_title(gene_info.loc[k, 'name'])
      for y in range(ax.shape[0]):
        ax[y][0].set_ylabel('Number of cells')
      for x in range(ax.shape[1]):
        ax[-1][x].set_xlabel('Num mols')
      fig.tight_layout()
    #+END_SRC

    #+RESULTS:
    :RESULTS:
    # Out[113]:
    [[file:figure/deconvolution.org/ipsc-npmle-examples.png]]
    :END:

    Overall, these examples suggest flexibility is needed for accurately
    modeling the tails of the distribution.

** Synthetic cell mixtures
*** T cell-B cell mixture

   Create a synthetic heterogeneous population of cells by combining sorted
   CD8+ T cells and CD19+ B cells from
   [[https://www.nature.com/articles/ncomms14049][Zheng et al. 2017]].

   #+BEGIN_SRC ipython :tangle /project2/mstephens/aksarkar/projects/singlecell-modes/code/benchmark.py
     def cd8_cd19_mix():
       cd8 = cd8_cytotoxic_t_cells(return_df=True)
       cd19 = cd19_b_cells(return_df=True)
       x = pd.concat([cd8, cd19], axis='index', join='inner')
       y = np.zeros(x.shape[0]).astype(int)
       y[:cd8.shape[0]] = 1
       return x, y
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[79]:
   :END:

   #+NAME: run-synthetic-mix-benchmark
   #+BEGIN_SRC ipython :eval never
     x, y = cd8_cd19_mix()
     res = evaluate_generalization(x.values, stratify=y, train_size=0.5, test_size=0.1, random_state=0)
     res.to_csv(f'/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/cd8-cd19-mix.txt.gz', compression='gzip', sep='\t')
   #+END_SRC

   #+RESULTS: run-synthetic-mix-benchmark
   :RESULTS:
   # Out[82]:
   :END:

   #+BEGIN_SRC sh :noweb eval :dir /scratch/midway2/aksarkar/modes/
     sbatch --partition=gpu2 --gres=gpu:1 --mem=16G --time=6:00:00 --job-name=benchmark --output=benchmark.out
     #!/bin/bash
     module load cuda/9.0
     source activate scmodes
     python -i /project2/mstephens/aksarkar/projects/singlecell-modes/code/benchmark.py <<EOF
     <<run-synthetic-mix-benchmark>>
     EOF
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 58958403

*** Naive/activated T cell mixture

   Create a synthetic mixture of cells by combining sorted
   CD8+ cytotoxic T cells and CD8+/CD45RA+ naive cytotoxic T cells
   [[https://www.nature.com/articles/ncomms14049][Zheng et al. 2017]].

   #+BEGIN_SRC ipython :tangle /project2/mstephens/aksarkar/projects/singlecell-modes/code/benchmark.py
     def cyto_naive_t_mix():
       cyto = read_10x(prefix='/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/cytotoxic_t/filtered_matrices_mex/hg19', return_df=True)
       naive = read_10x(prefix='/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/naive_cytotoxic/filtered_matrices_mex/hg19', return_df=True)
       x = pd.concat([cyto, naive], axis='index', join='inner')
       y = np.zeros(x.shape[0]).astype(int)
       y[:cyto.shape[0]] = 1
       return x, y
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[80]:
   :END:

   #+BEGIN_SRC sh :noweb eval :dir /scratch/midway2/aksarkar/modes/
     sbatch --partition=gpu2 --gres=gpu:1 --mem=16G --time=6:00:00 --job-name=benchmark --output=benchmark-gpu.out
     #!/bin/bash
     source activate scmodes
     python -i /project2/mstephens/aksarkar/projects/singlecell-modes/code/benchmark.py <<EOF
     x, y = cyto_naive_t_mix()
     res = evaluate_generalization(x.values, pool=None, stratify=y, train_size=0.5, test_size=0.1, random_state=0, methods=['nb', 'zinb'])
     res.index = x.columns
     res.to_csv(f'/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/cyto-naive-t-mix-gpu.txt.gz', compression='gzip', sep='\t')
     EOF
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 59268870

   #+BEGIN_SRC sh
     sbatch --partition=broadwl -a 0-3 -n1 -c28 --exclusive --mem=16G --time=6:00:00 --job-name=benchmark --output=benchmark-cpu.out
     #!/bin/bash
     source activate scmodes
     python -i /project2/mstephens/aksarkar/projects/singlecell-modes/code/benchmark.py <<EOF
     import os
     cpu_methods = ['unimix', 'descend', 'npmle', 'saturated']
     task = int(os.environ['SLURM_ARRAY_TASK_ID'])
     with mp.Pool(maxtasksperchild=10) as pool:
       x, y = cyto_naive_t_mix()
       res = evaluate_generalization(x.values, pool=pool, stratify=y, train_size=0.5, test_size=0.1, random_state=0, methods=cpu_methods[task:task + 1])
       res.index = x.columns
       res.to_csv(f'/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/cyto-naive-t-mix-{cpu_methods[task]}.txt.gz', compression='gzip', sep='\t')
     EOF
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 59268928

   Read the results.

   #+BEGIN_SRC ipython
     benchmark = {}
     benchmark['cd8-cd19-mix'] = pd.read_csv(f'/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/cd8-cd19-mix.txt.gz', sep='\t', index_col=0)
     benchmark['cyto-naive-t-mix'] = (
       pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/cyto-naive-t-mix-gpu.txt.gz', sep='\t')
       .merge(pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/cyto-naive-t-mix-unimix.txt.gz', sep='\t'), on='0')
       .merge(pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/cyto-naive-t-mix-descend.txt.gz', sep='\t'), on='0')
       .merge(pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/cyto-naive-t-mix-npmle.txt.gz', sep='\t'), on='0')
       .merge(pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/cyto-naive-t-mix-saturated.txt.gz', sep='\t'), on='0')
       .set_index('0'))
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[78]:
   :END:

   Plot the results.

   #+BEGIN_SRC ipython :ipyfile figure/deconvolution.org/synthetic-mix-benchmark.png
     fig, ax = plt.subplots(1, 2, sharey=True)
     fig.set_size_inches(5, 3)
     for a, k in zip(ax, sorted(benchmark.keys())):
       a.boxplot((benchmark[f'{k}'].values - benchmark[f'{k}']['nb'].values.reshape(-1, 1))[:,1:-1],
                 widths=0.25, medianprops={'color': 'k'}, flierprops={'marker': '.', 'markersize': 4})
       a.set_xticklabels(benchmark[f'{k}'].columns[1:-1], rotation=90)
       a.set_xlabel('Method')
       a.set_title(k)
     ax[0].set_ylabel('Diff val set log lik from NB')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[16]:
   [[file:figure/deconvolution.org/synthetic-mix-benchmark.png]]
   :END:

*** Investigate genes favoring more complex models

    Read the data.

    #+BEGIN_SRC ipython :async t
      mix1, y1 = cd8_cd19_mix()
      mix2, y2 = cyto_naive_t_mix()
    #+END_SRC

    #+RESULTS:
    :RESULTS:
    # Out[81]:
    :END:

    Look at genes where NPMLE has higher validation set log likelihood than NB.

    #+BEGIN_SRC ipython
      llik_diff = benchmark['cd8-cd19-mix']['npmle'] - benchmark['cd8-cd19-mix']['nb']
      query = benchmark['cd8-cd19-mix'].loc[llik_diff[llik_diff > 0].sort_values(ascending=False).head(n=4).index].index
    #+END_SRC

    #+RESULTS:
    :RESULTS:
    # Out[82]:
    :END:

    #+BEGIN_SRC ipython :ipyfile figure/deconvolution.org/cd8-cd19-mix-npmle-examples.png
      plt.clf()
      fig, ax = plt.subplots(1, 4)
      fig.set_size_inches(8, 2)
      for a, k in zip(ax.ravel(), query):
        a.hist(mix1.iloc[:,k], bins=mix1.iloc[:,k].max(), color='k')
        a.set_title(gene_info.loc[mix1.columns[k], 'name'] if mix1.columns[k] in gene_info.index else mix1.columns[k])
        a.set_xlabel('Number of molecules')
      ax[0].set_ylabel('Number of cells')
      fig.tight_layout()
    #+END_SRC

    #+RESULTS:
    :RESULTS:
    # Out[104]:
    [[file:figure/deconvolution.org/cd8-cd19-mix-npmle-examples.png]]
    :END:

    /CD74/ appears to be the first good example of a gene (so far) showing
    bimodal gene expression as predicted by a simple kinetic model (Munsky et
    al. 2013, Kim and Marioni 2013).

** Heterogeneous cell populations

   #+BEGIN_SRC ipython :tangle /project2/mstephens/aksarkar/projects/singlecell-modes/code/benchmark.py
     def pbmcs_68k(**kwargs):
       return read_10x('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/68k_pbmcs/filtered_matrices_mex/hg19/', **kwargs)

     def cortex():
       counts = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/zeisel-2015/GSE60361_C1-3005-Expression.txt.gz', index_col=0)
       # Follow scVI here
       return counts.loc[counts.var(axis=1).sort_values(ascending=False).head(n=500).index].values.T
   #+END_SRC

