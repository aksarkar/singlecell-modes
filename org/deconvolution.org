#+TITLE: Comparison of expression deconvolution approaches
#+SETUPFILE: setup.org

* Introduction

  Suppose we have observations \(x_i \sim f(\theta_i), i = 1, \ldots, n\), and
  \(\theta_i \sim g(\cdot)\). /Distribution deconvolution/ is the problem of
  estimating \(g \in \mathcal{G}\) from \(x_1, \ldots, x_n\), assuming \(f\) is known
  ([[https://academic.oup.com/biomet/article/103/1/1/2390141][Efron
  2016]]).

  Recent work suggests that scRNA-seq data follows this generative model
  ([[http://dx.doi.org/10.1073/pnas.1721085115][Wang et al. 2018]]). Here, we
  investigate the trade-off between model complexity/flexibility and
  generalization for different choices of \(\mathcal{G}\) in real data.

* Setup

  #+BEGIN_SRC emacs-lisp :exports none
    (add-to-list 'python-shell-completion-native-disabled-interpreters "jupyter")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(mem="8G",partition="mstephens",venv="scmodes") :dir /scratch/midway2/aksarkar/modes :exports none

  #+RESULTS:
  : Submitted batch job 58986993

  #+BEGIN_SRC ipython :tangle /project2/mstephens/aksarkar/projects/singlecell-modes/code/benchmark.py
    import gc
    import gzip
    import numpy as np
    import pandas as pd
    import scipy.io
    import scipy.stats as st
    import scipy.special as sp
    import sklearn.model_selection as skms

    import rpy2.robjects.packages
    import rpy2.robjects.pandas2ri
    import rpy2.robjects.numpy2ri

    rpy2.robjects.pandas2ri.activate()
    rpy2.robjects.numpy2ri.activate()

    ashr = rpy2.robjects.packages.importr('ashr')
    descend = rpy2.robjects.packages.importr('descend')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[1]:
  :END:

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])
    import colorcet
    import matplotlib.pyplot as plt
    plt.rcParams['figure.facecolor'] = 'w'
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[10]:
  :END:

* Methods
** Distribution deconvolution

   The general form of distribution deconvolution for scRNA-seq is:

   \[ x_{ij} \sim \mathrm{Poisson}(\exp(\mathbf{z}_i' \mathbf{b}_j) \lambda_{ij}) \]

   \[ \lambda_{ij} \sim g_j(\cdot) \]

   where:

   - \(x_{ij}\) is the count of molecules of gene \(j\) in cell \(i\)
   - \(\mathbf{z}_i\) is a \(q\)-vector of covariates for cell \(i\)
   - \(\mathbf{b}_j\) is a \(q\)-vector of confounding effects on gene \(j\)
   - \(\lambda_{ij}\) is proportional to the relative abundance of gene \(j\)
     in cell \(i\)

   The primary inference goal is to recover \(g_j\). A secondary goal could be
   to recover \(\lambda_{ij}\). We can trade off flexibility and complexity of
   \(g_j\) for ease of implementation and speed.

   1. *Point mass:* \(g_j = \delta_\mu\). We mention it for completeness.
   2. *Gamma:* \(g_j = \mathrm{Gamma}(\cdot)\). This leads to the negative
      binomial marginal likelihood, and can be motivated by the empirical
      observation that the counts are overdispersed.
   3. *Point-Gamma:* \(g_j = \pi_j \delta_0(\cdot) + (1 - \pi_j)
      \mathrm{Gamma}(\cdot)\). This leads to the zero-inflated negative
      binomial marginal likelihood, which is still analytic and therefore
      computationally favorable. The inclusion of the point mass can be
      motivated by theory suggesting a biological mechanism for bimodal gene
      expression ([[http://dx.doi.org/10.1126/science.1216379][Munsky et
      al. 2013]], [[http://dx.doi.org/10.1186/gb-2013-14-1-r7][Kim and Marioni
      2013]]).
   4. *Unimodal mixture of uniforms:* \(g_j = \pi_0 \delta_0(\cdot) + \sum_k
      \pi_k \mathrm{Uniform}(\cdot; \lambda_0, a_{jk})\), where \(\lambda_0\)
      is the mode ([[http://dx.doi.org/10.1093/biostatistics/kxw041][Stephens
      2016]]). The inclusion of the point mass technically makes this
      distribution bimodal.
   5. *Exponential family:* \(g_j = \exp(\mathbf{Q}\alpha - \phi(\alpha))\),
      where \(\mathbf{Q}\) is a [[https://en.wikipedia.org/wiki/B-spline][B
      spline spline basis matrix]] for a
      [[https://en.wikipedia.org/wiki/Cubic_Hermite_spline][natural cubic
      spline]]
      ([[https://www.rdocumentation.org/packages/splines/topics/ns][~ns~
      function]];
      [[https://academic.oup.com/biomet/article/103/1/1/2390141][Efron
      2016]]). The key idea of the method is use spline regression to find the
      sufficient statistic and natural parameters which maximizes the penalized
      likelihood of the observed data. The method has been extended to include
      a point mass on zero ([[http://dx.doi.org/10.1073/pnas.1721085115][Wang
      et al. 2018]]).
   6. *Nonparametric:* \(g_j = \sum_k \pi_k \mathrm{Uniform}(\cdot; ak, a(k +
      1))\), where \(a\) is a fixed step size
      ([[https://projecteuclid.org/euclid.aoms/1177728066][Kiefer and Wolfowitz
      1956]],
      [[https://amstat.tandfonline.com/doi/abs/10.1080/01621459.2013.869224][Koenker
      and Mizera 2014]]).

   In order to evaluate methods on their ability to estimate \(g_j\), we hold
   out a validation set, and compute the validation set log likelihood. To set
   a common baseline, we compare against the saturated model \(\hat\lambda_{ij}
   = x_{ij}\).

   #+NAME: score-impl
   #+BEGIN_SRC ipython :tangle /project2/mstephens/aksarkar/projects/singlecell-modes/code/benchmark.py
     def nb_llik(x, mean, inv_disp):
       return (x * np.log(mean / inv_disp) -
               x * np.log(1 + mean / inv_disp) -
               inv_disp * np.log(1 + mean / inv_disp) +
               sp.gammaln(x + inv_disp) -
               sp.gammaln(inv_disp) -
               sp.gammaln(x + 1))

     def score_nb(x_train, x_test):
       import scqtl
       onehot = np.ones((x_train.shape[0], 1))
       size_factor = x_train.sum(axis=1).reshape(-1, 1)
       design = np.zeros((x_train.shape[0], 1))
       log_mu, log_phi, *_ = scqtl.tf.fit(
         umi=x_train.astype(np.float32),
         onehot=onehot.astype(np.float32),
         design=design.astype(np.float32),
         size_factor=size_factor.astype(np.float32),
         learning_rate=1e-3,
         max_epochs=30000)
       return nb_llik(x_test, x_test.sum(axis=1, keepdims=True) * np.exp(log_mu), np.exp(-log_phi)).sum(axis=0)

     def softplus(x):
       return np.where(x > 30, x, np.log(1 + np.exp(x)))

     def zinb_llik(x, mean, inv_disp, logodds):
       case_zero = -softplus(-logodds) + softplus(nb_llik(x, mean, inv_disp) - logodds)
       case_non_zero = -softplus(logodds) + nb_llik(x, mean, inv_disp)
       return np.where(x < 1, case_zero, case_non_zero)

     def score_zinb(x_train, x_test):
       import scqtl
       onehot = np.ones((x_train.shape[0], 1))
       size_factor = x_train.sum(axis=1).reshape(-1, 1)
       init = scqtl.tf.fit(
         umi=x_train.astype(np.float32),
         onehot=onehot.astype(np.float32),
         size_factor=size_factor.astype(np.float32),
         learning_rate=1e-3,
         max_epochs=30000)
       log_mu, log_phi, logodds, *_ = scqtl.tf.fit(
         umi=x_train.astype(np.float32),
         onehot=onehot.astype(np.float32),
         size_factor=size_factor.astype(np.float32),
         learning_rate=1e-3,
         max_epochs=30000,
         warm_start=init[:3])
       return zinb_llik(x_test, x_test.sum(axis=1, keepdims=True) * np.exp(log_mu), np.exp(-log_phi), logodds).sum(axis=0)

     def score_unimix(x_train, x_test):
       result = []
       train_size_factor = pd.Series(x_train.sum(axis=1))
       test_size_factor = pd.Series(x_test.sum(axis=1))
       # np iterates over rows
       for train, test in zip(x_train.T, x_test.T):
         lam = train / train_size_factor
         res0 = ashr.ash_workhorse(
           # these are ignored by ash
           pd.Series(np.zeros(train.shape)),
           1,
           # numpy2ri doesn't DTRT, so we need to use pandas
           lik=ashr.lik_pois(y=pd.Series(train), scale=train_size_factor, link='identity'),
           mode=pd.Series([lam.min(), lam.max()]))
         res = ashr.ash_workhorse(
           pd.Series(np.zeros(test.shape)),
           1,
           lik=ashr.lik_pois(y=pd.Series(test), scale=test_size_factor, link='identity'),
           fixg=True,
           g=res0.rx2('fitted_g'))
         result.append(np.array(res.rx2('loglik')))
         # R objects don't get GC'ed by python correctly
         del res, res0
         gc.collect()
       return np.array(result).ravel()

     def score_descend(x_train, x_test):
       result = []
       # numpy2ri doesn't DTRT, so we need to use pandas
       train_size_factor = pd.Series(x_train.sum(axis=1))
       test_size_factor = x_test.sum(axis=1).reshape(-1, 1)
       # np iterates over rows
       for train, test in zip(x_train.T, x_test.T):
         res = descend.deconvSingle(pd.Series(train), scaling_consts=train_size_factor, verbose=False)
         # DESCEND returns NA on errors
         if tuple(res.rclass) != ('DESCEND',):
           result.append(-np.inf)
           continue
         g = np.array(res.slots['distribution'])[:,:2]
         # Don't marginalize over lambda = 0 for x > 0, because p(x > 0 | lambda =
         # 0) = 0
         case_nonzero = (st.poisson(mu=test_size_factor * g[1:,0])
                         .logpmf(test.reshape(-1, 1))
                         .dot(g[1:,1]))
         case_zero = (st.poisson(mu=test_size_factor * g[:,0])
                      .logpmf(test.reshape(-1, 1))
                      .dot(g[:,1]))
         llik = np.where(test > 0, case_nonzero, case_zero).sum()
         result.append(llik)
         # R objects don't get GC'ed by python correctly
         del res
         gc.collect()
       return np.array(result).ravel()

     def score_npmle(x_train, x_test, K=100):
       result = []
       train_size_factor = pd.Series(x_train.sum(axis=1))
       test_size_factor = pd.Series(x_test.sum(axis=1))
       # np iterates over rows
       for train, test in zip(x_train.T, x_test.T):
         lam = train / train_size_factor
         grid = np.linspace(lam.min(), 2 * lam.max(), K + 1)
         res0 = ashr.ash_workhorse(
           # these are ignored by ash
           pd.Series(np.zeros(train.shape)),
           1,
           # numpy2ri doesn't DTRT, so we need to use pandas
           lik=ashr.lik_pois(y=pd.Series(train), scale=train_size_factor, link='identity'),
           g=ashr.unimix(pd.Series(np.ones(K) / K), pd.Series(grid[:-1]), pd.Series(grid[1:])))
         res = ashr.ash_workhorse(
           pd.Series(np.zeros(test.shape)),
           1,
           lik=ashr.lik_pois(y=pd.Series(test), scale=test_size_factor, link='identity'),
           fixg=True,
           g=res0.rx2('fitted_g'))
         result.append(np.array(res.rx2('loglik')))
         # R objects don't get GC'ed by python correctly
         del res, res0
         gc.collect()
       return np.array(result).ravel()

     def score_saturated(x_train, x_test):
       return st.poisson(mu=x_test).logpmf(x_test).sum(axis=0)

     def evaluate_generalization(x, **kwargs):
       result = {}
       train, val = skms.train_test_split(x, **kwargs)
       methods = ['nb', 'zinb', 'unimix', 'descend', 'npmle', 'saturated']
       for m in methods:
         # Hack: get functions by name
         result[m] = globals()[f'score_{m}'](train, val)
       return pd.DataFrame.from_dict(result, orient='columns')
   #+END_SRC

   #+RESULTS: score-impl
   :RESULTS:
   # Out[12]:
   :END:

* Results
** Homogeneous cell populations

   Use sorted cells from [[https://www.nature.com/articles/ncomms14049][Zheng
   et al. 2017]].

   #+NAME: read-zheng
   #+BEGIN_SRC ipython :tangle /project2/mstephens/aksarkar/projects/singlecell-modes/code/benchmark.py
     def read_10x(prefix, min_detect=0.25, return_df=False):
       counts = scipy.io.mmread(f'{prefix}/matrix.mtx.gz').tocsr()
       keep = ((counts > 0).mean(axis=1) >= min_detect).A.ravel()
       counts = counts[keep].T.A.astype(np.int)
       if return_df:
         genes = pd.read_csv(f'{prefix}/genes.tsv.gz', sep='\t', header=None)
         return pd.DataFrame(counts, columns=genes.loc[keep, 0])
       else:
         return counts

     def cd8_cytotoxic_t_cells(**kwargs):
       return read_10x('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/cd8+_cytotoxic_t_cells/filtered_matrices_mex/hg19', **kwargs)

     def cd19_b_cells(**kwargs):
       return read_10x('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/cd19+_b_cells/filtered_matrices_mex/hg19/', **kwargs)
   #+END_SRC

   #+RESULTS: read-zheng
   :RESULTS:
   # Out[74]:
   :END:

   Look at some examples.

   #+BEGIN_SRC ipython
     x = cd8_cytotoxic_t_cells()
     xj = pd.Series(x[:,x.mean(axis=0).argmax()])
     size_factor = pd.Series(x.sum(axis=1))
     lam = xj / size_factor
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[4]:
   :END:

   Fit Poisson ash.

   #+BEGIN_SRC ipython
     unimix_res = ashr.ash_workhorse(
       pd.Series(np.zeros(x.shape[0])),
       1,
       lik=ashr.lik_pois(y=xj, scale=size_factor, link='identity'),
       mode=pd.Series([lam.min(), lam.max()]))
     unimix_cdf = ashr.cdf_ash(unimix_res, np.linspace(lam.min(), lam.max(), 1000))
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[7]:
   :END:

   Fit NPMLE.

   #+BEGIN_SRC ipython
     K = 100
     grid = np.linspace(lam.min(), lam.max(), K + 1)
     npmle_res = ashr.ash_workhorse(
       pd.Series(np.zeros(x.shape[0])),
       1,
       lik=ashr.lik_pois(y=xj, scale=size_factor, link='identity'),
       g=ashr.unimix(pd.Series(np.ones(K) / K), pd.Series(grid[:-1]), pd.Series(grid[1:])))
     npmle_cdf = ashr.cdf_ash(npmle_res, np.linspace(lam.min(), lam.max(), 1000))
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[12]:
   :END:

   Fit DESCEND.

   #+BEGIN_SRC ipython
     descend_res = descend.deconvSingle(xj, scaling_consts=size_factor, verbose=False)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[14]:
   :END:

   Fit NB/ZINB.

   #+BEGIN_SRC ipython :eval never :noweb tangle :tangle /project2/mstephens/aksarkar/projects/singlecell-modes/code/fit-nb.py
     import numpy as np
     import pandas as pd
     import scipy.io
     import scqtl
     <<read-zheng>>
     x = cd8_cytotoxic_t_cells()
     size_factor = x.sum(axis=1).reshape(-1, 1)
     onehot = np.ones((x.shape[0], 1))
     design = np.zeros((x.shape[0], 1))
     init = scqtl.tf.fit(
       umi=x.astype(np.float32),
       onehot=onehot.astype(np.float32),
       design=design.astype(np.float32),
       size_factor=size_factor.astype(np.float32),
       learning_rate=1e-3,
       max_epochs=30000,
       verbose=True)
     pd.DataFrame(init[0]).to_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-nb-log-mu.txt.gz', compression='gzip', sep='\t')
     pd.DataFrame(init[1]).to_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-nb-log-phi.txt.gz', compression='gzip', sep='\t')
     log_mu, log_phi, logodds, nb_llik, zinb_llik = scqtl.tf.fit(
       umi=x.astype(np.float32),
       onehot=onehot.astype(np.float32),
       design=design.astype(np.float32),
       size_factor=size_factor.astype(np.float32),
       learning_rate=1e-3,
       max_epochs=30000,
       warm_start=init[:3],
       verbose=True)
     pd.DataFrame(log_mu).to_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-zinb-log-mu.txt.gz', compression='gzip', sep='\t')
     pd.DataFrame(log_phi).to_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-zinb-log-phi.txt.gz', compression='gzip', sep='\t')
     pd.DataFrame(logodds).to_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-zinb-logodds.txt.gz', compression='gzip', sep='\t')
   #+END_SRC

   #+BEGIN_SRC sh :dir /scratch/midway2/aksarkar/modes/
     sbatch --partition=gpu2 --gres=gpu:1 --mem=16G --time=60:00 --job-name=fit-nb
     #!/bin/bash
     source activate scmodes
     python /project2/mstephens/aksarkar/projects/singlecell-modes/code/fit-nb.py
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 58803475

   #+BEGIN_SRC ipython
     j = str(x.mean(axis=0).argmax())
     nb_log_mu = pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-nb-log-mu.txt.gz', sep='\t')
     nb_log_phi = pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-nb-log-phi.txt.gz', sep='\t')
     # Gamma (Use MATLAB and MATHEMATICA (b=theta=scale, a=alpha=shape) definition)
     # https://github.com/scipy/scipy/blob/v1.2.1/scipy/stats/_continuous_distns.py#L2479     
     gamma_cdf = st.gamma(a=np.exp(-nb_log_phi[j]), scale=np.exp(nb_log_mu[j] + nb_log_phi[j])).cdf(np.linspace(lam.min(), lam.max(), 1000))
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[15]:
   :END:

   #+BEGIN_SRC ipython
     zinb_log_mu = pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-zinb-log-mu.txt.gz', sep='\t')
     zinb_log_phi = pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-zinb-log-phi.txt.gz', sep='\t')
     zinb_logodds = pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-zinb-log-phi.txt.gz', sep='\t')
     point_gamma_cdf = st.gamma(a=np.exp(-zinb_log_phi[j]), scale=np.exp(zinb_log_mu[j] + zinb_log_phi[j])).cdf(np.linspace(lam.min(), lam.max(), 1000))
     point_gamma_cdf *= sp.expit(-zinb_logodds[j].values)
     point_gamma_cdf += sp.expit(zinb_logodds[j].values)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[16]:
   :END:

   Plot the observed counts and deconvolved distributions.

   #+BEGIN_SRC ipython :ipyfile figure/deconvolution.org/ash-example.png
     cm = plt.get_cmap('Dark2').colors
     fig, ax = plt.subplots(2, 1)
     ax[0].hist(xj, bins=100, color='k')
     ax[0].set_xlabel('Molecule count')
     ax[0].set_ylabel('Number of cells')

     ax[1].plot(np.linspace(lam.min(), lam.max(), 1000), gamma_cdf, color=cm[0], lw=1, label='Gamma')
     ax[1].plot(np.linspace(lam.min(), lam.max(), 1000), point_gamma_cdf, color=cm[1], lw=1, label='Point-Gamma')
     ax[1].plot(np.array(unimix_cdf.rx2('x')),
                np.array(unimix_cdf.rx2('y')).ravel(), c=cm[2], lw=1, label='Unif mix')
     F = np.cumsum(np.array(descend_res.slots['density.points'])[:,1])
     ax[1].plot(np.array(descend_res.slots['density.points'])[:,0],
                F / F.max(), c=cm[3], lw=1, label='DESCEND')
     ax[1].plot(np.array(npmle_cdf.rx2('x')),
                np.array(npmle_cdf.rx2('y')).ravel(), c=cm[4], lw=1, label='NPMLE')
     ax[1].set_xlabel('Latent gene expression')
     ax[1].set_ylabel('CDF')

     ax[1].legend(frameon=False)

     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[20]:
   [[file:figure/deconvolution.org/ash-example.png]]
   :END:

   Use iPSCs derived from Yoruba LCLs
   ([[https://www.biorxiv.org/content/early/2018/09/23/424192][Sarkar et
   al. 2018]]). The data were sequenced on the Fluidigm C1 platform to much
   greater depth, allowing many more genes to be detected (especially lower
   expressed genes), potentially including genes with bimodal steady state gene
   expression.

   #+BEGIN_SRC ipython :tangle /project2/mstephens/aksarkar/projects/singlecell-modes/code/benchmark.py
     def ipsc(return_df=False):
       annotations = pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-annotation.txt', sep='\t')
       keep_samples = pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt', sep='\t', index_col=0, header=None)
       keep_genes = pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/genes-pass-filter.txt', sep='\t', index_col=0, header=None)
       annotations = annotations.loc[keep_samples.values.ravel()]
       result = []
       for chunk in pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-counts.txt.gz', sep='\t', index_col=0, chunksize=100):
          x = (chunk
               .loc[:,keep_samples.values.ravel()]
               .loc[:,(annotations['chip_id'] == 'NA18507').values]
               .filter(items=keep_genes[keep_genes.values.ravel()].index, axis='index'))
          if not x.empty:
            result.append(x)
       result = pd.concat(result)
       # Return samples x genes
       if return_df:
         return result.T
       else:
         return result.values.T
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[23]:
   :END:

   Run the benchmark.

   #+NAME: run-homogeneous-benchmark
   #+BEGIN_SRC ipython :eval never
     for data in ('ipsc',):
       x = globals()[data]()
       res = evaluate_generalization(x, test_size=0.1, random_state=0)
       res.to_csv(f'/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/{data}.txt.gz', compression='gzip', sep='\t')
   #+END_SRC

   #+BEGIN_SRC sh :noweb eval :dir /scratch/midway2/aksarkar/modes/
     sbatch --partition=gpu2 --gres=gpu:1 --mem=16G --time=5:00:00 --job-name=benchmark --output=benchmark.out
     #!/bin/bash
     module load cuda/9.0
     source activate scmodes
     python -i /project2/mstephens/aksarkar/projects/singlecell-modes/code/benchmark.py <<EOF
     <<run-homogeneous-benchmark>>
     EOF
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 58987904

   Read the results.

   #+BEGIN_SRC ipython
     benchmark = {}
     for data in ('cd8_cytotoxic_t_cells', 'cd19_b_cells'):
       benchmark[data] = pd.read_csv(f'/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/{data}.txt.gz', sep='\t', index_col=0)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[19]:
   :END:

   Plot the generalization performance of each method.

   #+BEGIN_SRC ipython :ipyfile figure/deconvolution.org/homogeneous-benchmark-deviance.png
     fig, ax = plt.subplots(1, 2, sharey=True)
     fig.set_size_inches(5, 3)
     for i, k in enumerate(sorted(benchmark.keys())):
       ax[i].boxplot((benchmark[f'{k}']['saturated'].values.reshape(-1, 1) - benchmark[f'{k}'].values)[:,:-1],
                     widths=0.25, medianprops={'color': 'k'}, flierprops={'marker': '.', 'markersize': 4})
       ax[i].set_xticklabels(benchmark[f'{k}'].columns[:-1], rotation=90)
       ax[i].set_xlabel('Method')
       ax[i].set_title(k)
     ax[0].set_ylabel('Validation set deviance')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[50]:
   [[file:figure/deconvolution.org/homogeneous-benchmark-deviance.png]]
   :END:

** Synthetic cell mixtures

   Create a synthetic heterogeneous population of cells by combining sorted
   CD8+ T cells and CD19+ B cells from
   [[https://www.nature.com/articles/ncomms14049][Zheng et al. 2017]].

   #+NAME: run-synthetic-mix-benchmark
   #+BEGIN_SRC ipython :eval never
     cd8 = cd8_cytotoxic_t_cells(return_df=True)
     cd19 = cd19_b_cells(return_df=True)
     x = pd.concat([cd8, cd19], axis='index', join='inner')
     y = np.zeros(x.shape[0]).astype(int)
     y[:cd8.shape[0]] = 1
     res = evaluate_generalization(x.values, stratify=y, train_size=0.5, test_size=0.1, random_state=0)
     res.to_csv(f'/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/cd8-cd19-mix.txt.gz', compression='gzip', sep='\t')
   #+END_SRC

   #+RESULTS: run-synthetic-mix-benchmark
   :RESULTS:
   # Out[82]:
   :END:

   #+BEGIN_SRC sh :noweb eval :dir /scratch/midway2/aksarkar/modes/
     sbatch --partition=gpu2 --gres=gpu:1 --mem=16G --time=6:00:00 --job-name=benchmark --output=benchmark.out
     #!/bin/bash
     module load cuda/9.0
     source activate scmodes
     python -i /project2/mstephens/aksarkar/projects/singlecell-modes/code/benchmark.py <<EOF
     <<run-synthetic-mix-benchmark>>
     EOF
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 58958403

   Read the results.

   #+BEGIN_SRC ipython
     benchmark = {}
     for data in ('cd8-cd19-mix',):
       benchmark[data] = pd.read_csv(f'/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/{data}.txt.gz', sep='\t', index_col=0)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[4]:
   :END:

   Plot the results.

   #+BEGIN_SRC ipython :ipyfile figure/deconvolution.org/synthetic-benchmark-deviance.png
     plt.gcf().set_size_inches(3, 3)
     for i, k in enumerate(sorted(benchmark.keys())):
       plt.gca().boxplot((benchmark[f'{k}']['saturated'].values.reshape(-1, 1) - benchmark[f'{k}'].values)[:,:-1],
                     widths=0.25, medianprops={'color': 'k'}, flierprops={'marker': '.', 'markersize': 4})
       plt.gca().set_xticklabels(benchmark[f'{k}'].columns[:-1], rotation=90)
       plt.gca().set_xlabel('Method')
       plt.gca().set_title(k)
     plt.gca().set_ylabel('Validation set deviance')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[12]:
   [[file:figure/deconvolution.org/synthetic-benchmark-deviance.png]]
   :END:
   

** Heterogeneous cell populations

   #+BEGIN_SRC ipython :tangle /project2/mstephens/aksarkar/projects/singlecell-modes/code/benchmark.py
     def pbmcs_68k(**kwargs):
       return read_10x('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/68k_pbmcs/filtered_matrices_mex/hg19/', **kwargs)

     def cortex():
       counts = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/zeisel-2015/GSE60361_C1-3005-Expression.txt.gz', index_col=0)
       # Follow scVI here
       return counts.loc[counts.var(axis=1).sort_values(ascending=False).head(n=500).index].values.T
   #+END_SRC
