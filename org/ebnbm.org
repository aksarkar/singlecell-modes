#+TITLE: Negative binomial measurement model
#+SETUPFILE: setup.org

* Introduction

  We, and Svensson 2020, found some evidence for overdispersion in control
  scRNA-seq data. Here, we estimate to what extent that overdispersion could be
  explained by an overdispersed measurement model, using the key fact that the
  measurement overdispersion is described by a single parameter common across
  all genes. We specifically consider combining an NB measurement model with a
  Gamma expression model \(
  \DeclareMathOperator\Pois{Poisson}
  \DeclareMathOperator\Gam{Gamma}
  \DeclareMathOperator\NB{NB}
  \newcommand\const{\mathrm{const}}
  \newcommand\lnb{l_{\mathrm{NB}}}
  \newcommand\E[1]{\left\langle #1 \right\rangle}
  \)

  \begin{align}
    x_{ij} \mid s_i, \lambda_{ij}, u_{ij} &\sim \NB(s_i \lambda_{ij}, \theta)\\
    \lambda_{ij} \mid a_j, b_j &\sim \Gam(a_j, b_j),
  \end{align}

  where the NB distribution is parameterized by mean and dispersion. 

* Setup
  :PROPERTIES:
  :CUSTOM_ID: setup
  :END:

  #+BEGIN_SRC emacs-lisp :exports none
    (add-to-list 'python-shell-completion-native-disabled-interpreters "jupyter")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
    (org-babel-lob-ingest "llr.org")
  #+END_SRC

  #+RESULTS:
  : 6

  #+CALL: ipython3(venv="scmodes",partition="gpu2",opts="--gres=gpu:1",memory="8G") :exports none :dir /scratch/midway2/aksarkar/modes

  #+BEGIN_SRC ipython
    import numpy as np
    import pandas as pd
    import scipy.special as sp
    import scipy.stats as st
    import scmodes
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[1]:
  :END:

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[2]:
  :END:

  #+BEGIN_SRC ipython
    import colorcet
    import matplotlib
    import matplotlib.pyplot as plt
    plt.rcParams['figure.facecolor'] = 'w'
    plt.rcParams['font.family'] = 'Nimbus Sans'
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[3]:
  :END:

* Results
** VBEM algorithm for Gamma expression model

   To estimate \(a_1, \ldots, a_p, b_1, \ldots, b_p, \theta\) from observed
   data, we use a VBEM algorithm. First, introduce latent variables \(u_{ij}\)

   \begin{align}
     x_{ij} \mid s_i, \lambda_{ij}, u_{ij} &\sim \Pois(s_i \lambda_{ij} u_{ij})\\
     u_{ij} \mid \theta &\sim \Gam(\theta^{-1}, \theta^{-1})\\
     \lambda_{ij} \mid a_j, b_j &\sim \Gam(a_j, b_j),
   \end{align}

   where the Gamma distribution is parameterized by shape and rate. It is
   straightforward to show that marginalizing over \(u_{ij}\) yields the
   original NB-Gamma compound model of interest. The log joint

   \begin{multline}
     \ln p(x_{ij} \mid \lambda_{ij}, u_{ij}, a_j, b_j, \theta) = x_{ij} \ln (s_i \lambda_{ij} u_{ij}) - s_i \lambda_{ij} u_{ij} - \ln\Gamma(x_{ij} + 1)\\
     + (a_j - 1) \ln \lambda_{ij} - b_j \lambda_{ij} + a_j \ln b_j - \ln\Gamma(a_j) + (\theta^{-1} - 1) \ln u_{ij} - \theta^{-1} u_{ij} + \theta^{-1}\ln(\theta^{-1}) - \ln\Gamma(\theta^{-1}),
   \end{multline}

   and the posteriors

   \begin{align}
     \ln p(\lambda_{ij} \mid x_{ij}, u_{ij}, a_j, b_j) &= (x_{ij} + a_j - 1) \ln \lambda_{ij} - (s_i u_{ij} + b_j) \lambda_{ij} + \const\\
     &= \Gam(x_{ij} + a_j, s_i u_{ij} + b_j)\\
     \ln p(u_{ij} \mid x_{ij}, \lambda_{ij}, a_j, b_j) &= (x_{ij} + \theta^{-1} - 1) \ln \lambda_{ij} - (s_i \lambda_{ij} + b_j) u_{ij} + \const\\
     &= \Gam(x_{ij} + \theta^{-1}, s_i \lambda_{ij} + b_j).
   \end{align}

   However, the required expectations for an EM algorithm that directly
   maximizes the likelihood are non-analytic. To side-step this problem,
   introduce a variational approximation

   \begin{align}
     q &= \prod_{i,j} q(\lambda_{ij}) q(u_{ij})\\
     q^*(\lambda_{ij}) &\propto \exp((x_{ij} + a_j - 1) \ln \lambda_{ij} - (s_i \E{u_{ij}} + b_j) \lambda_{ij})\\
     &= \Gam(x_{ij} + a_j, s_i \E{u_{ij}} + b_j)\\
     &\triangleq \Gam(\alpha_{ij}, \beta_{ij})\\
     q^*(u_{ij}) &\propto \exp((x_{ij} + \theta^{-1} - 1) \ln u_{ij} - (s_i \E{\lambda_{ij}} + b_j) u_{ij})\\
     &= \Gam(x_{ij} + \theta^{-1}, s_i \E{\lambda_{ij}} + \theta^{-1})\\
     &\triangleq \Gam(\gamma_{ij}, \delta_{ij}).
   \end{align}

   The evidence lower bound

   \begin{multline}
     \ell = \sum_{i, j} \left[ (x_{ij} + a_j - \alpha_{ij}) \E{\ln \lambda_{ij}} - (b_j - \beta_{ij}) \E{\lambda_{ij}} + (x_{ij} + \theta^{-1} - \gamma_{ij}) \E{\ln u_{ij}} - (\theta^{-1} - \delta_{ij}) \E{u_{ij}} - s_i \E{\lambda_{ij}} \E{u_{ij}}\right.\\
       + \left. a_j \ln b_j + \theta^{-1}\ln(\theta^{-1}) - \alpha_{ij} \ln \beta_{ij} - \gamma_{ij} \ln \delta_{ij} - \ln\Gamma(a_j) - \ln\Gamma(\theta^{-1}) + \ln\Gamma(\alpha_{ij}) + \ln\Gamma(\gamma_{ij})\right] + \const,
   \end{multline}

   where

   \begin{align}
     \E{\lambda_{ij}} &= \alpha_{ij} / \beta_{ij}\\
     \E{\ln\lambda_{ij}} &= \psi(\alpha_{ij}) - \ln(\beta_{ij})\\
     \E{u_{ij}} &= \gamma_{ij} / \delta_{ij}\\
     \E{\ln u_{ij}} &= \psi(\gamma_{ij}) - \ln(\delta_{ij}),
   \end{align}

   and \(\psi\) denotes the digamma function. Then, we have analytic M step update

   \begin{align}
     \frac{\partial \ell}{\partial b_j} &= \sum_{i} \frac{a_j}{b_j} - \E{\lambda_{ij}} = 0\\
     b_j &:= \frac{n a_j}{\sum_i \E{\lambda_{ij}}}
   \end{align}

   and Newton-Raphson partial M step updates

   \begin{align}
     \eta &\triangleq \theta^{-1}\\
     \frac{\partial \ell}{\partial \eta} &= \sum_{i, j} 1 + \E{\ln u_{ij}} - \E{u_{ij}} - \psi(\eta)\\
     \frac{\partial^2 \ell}{\partial \eta^2} &= -n p \psi^{(1)}(\eta)\\
     \frac{\partial \ell}{\partial a_j} &= \sum_i \E{\ln\lambda_{ij}} + \ln b_j - \psi(a_j)\\
     \frac{\partial^2 \ell}{\partial a_j^2} &=  -n \psi^{(1)}(a_j),
   \end{align}

   where \(\psi^{(1)}\) denotes the trigamma function.

** Simulation

   Simulate data from the model.

   #+BEGIN_SRC ipython
     def simulate_nb_gamma(n, p, theta, seed=0):
       np.random.seed(seed)
       log_mean = np.random.uniform(low=-12, high=-8, size=(1, p))
       log_disp = np.random.uniform(low=-6, high=0, size=(1, p))
       s = 1e5 * np.ones((n, 1))
       lam = st.gamma(a=np.exp(-log_disp), scale=np.exp(log_mean + log_disp)).rvs(size=(n, p))
       u = st.gamma(a=1 / theta, scale=theta).rvs(size=(n, p))
       x = st.poisson(s * lam * u).rvs()
       return x, s, lam, u, log_mean, -log_disp, theta
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[4]:
   :END:

   Fit the model to a simulated example, fixing the hyperparameters to the
   ground truth values. (This only updates the variational approximation.)

   #+BEGIN_SRC ipython :async t
     x, s, lam, u, log_mean, log_inv_disp, theta = simulate_nb_gamma(n=1000, p=5, theta=0.2, seed=1)
     log_mean_hat, log_inv_disp_hat, log_meas_disp_hat, alpha, beta, gamma, delta, elbo = scmodes.ebnbm.ebnbm_gamma(
       x,
       s,
       init=np.hstack([np.exp(log_inv_disp).ravel(), np.exp(-log_mean + log_inv_disp).ravel(), theta]),
       tol=1e-3,
       extrapolate=False,
       fix_g=True,
       fix_theta=True)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[5]:
   :END:

   Make sure we didn't mess up the parameterization.

   #+BEGIN_SRC ipython :ipyfile figure/ebnbm.org/sim-ex.png
     cm = plt.get_cmap('Dark2')
     plt.clf()
     fig, ax = plt.subplots(1, 5, sharey=True)
     fig.set_size_inches(8, 2)

     for i, a in enumerate(ax):
       y = np.arange(x[:,i].max() + 1)
       # Poisson measurement => NB observation
       pmf = dict()
       pmf['Poisson'] = st.nbinom(n=np.exp(log_inv_disp[:,i]), p=1 / (1 + (s * np.exp(log_mean[:,i] - log_inv_disp[:,i])))).pmf(y).mean(axis=0)
       # NB measurement => Monte Carlo integral
       n_samples = 1000
       Ghat = st.gamma(a=np.exp(log_inv_disp_hat[:,i]), scale=np.exp(log_mean_hat[:,i] - log_inv_disp_hat[:,i]))
       temp = Ghat.rvs(size=(n_samples, y.shape[0], 1))
       pmf[rf'NB ($\hat\theta$ = {np.exp(log_meas_disp_hat):.2g})'] = st.nbinom(n=np.exp(-log_meas_disp_hat), p=1 / (1 + s[0] * temp * np.exp(log_meas_disp_hat))).pmf(y.reshape(-1, 1)).mean(axis=0)

       ax[i].hist(x[:,i], bins=y, color='0.7', density=True)
       for j, k in enumerate(pmf):
         ax[i].plot(y + .5, pmf[k], c=cm(j), lw=1, label=k)
       ax[i].set_title(f'Gene {i+1}')
       ax[i].set_xlim(0, x[:,i].max())

     ax[0].set_ylabel('Density')
     ax[-1].legend(title='Measurement model', frameon=False, bbox_to_anchor=(1, .5), loc='center left')
     a = fig.add_subplot(111, frameon=False, xticks=[], yticks=[])
     a.set_xlabel('Number of molecules', labelpad=16)
     fig.tight_layout(pad=0.5)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[45]:
   [[file:figure/ebnbm.org/sim-ex.png]]
   :END:

   Compare the true posterior against the variational approximation.

   #+BEGIN_SRC ipython :ipyfile figure/ebnbm.org/sim-ex2.png
     plt.clf()
     fig, ax = plt.subplots(1, 2)
     fig.set_size_inches(5, 2.5)
     for a in ax:
       a.set_aspect('equal', adjustable='datalim')

     ax[0].set_xscale('log')
     ax[0].set_yscale('log')
     ax[0].scatter(((x + np.exp(log_inv_disp)) / (s * u + np.exp(log_inv_disp - log_mean))).ravel(), (alpha / beta).ravel(), s=1, c='k', alpha=0.1)
     lim = ax[0].get_xlim()
     ax[0].plot(lim, lim, lw=1, ls=':', c='r')
     ax[0].set_xlabel(r'$\mathrm{E}[\lambda \mid x, u]$')
     ax[0].set_ylabel(r'$\mathrm{E}_q[\lambda]$')

     ax[1].scatter(((x + theta) / (s * lam + theta)), (gamma / delta).ravel(), s=1, c='k', alpha=0.1)
     lim = ax[1].get_xlim()
     ax[1].plot(lim, lim, lw=1, ls=':', c='r')
     ax[1].set_xlabel(r'$\mathrm{E}[u \mid x, \lambda]$')
     ax[1].set_ylabel(r'$\mathrm{E}_q[u]$')

     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[48]:
   [[file:figure/ebnbm.org/sim-ex2.png]]
   :END:

   Now, fit the model initialized at the ground truth hyperparameters, fixing
   \(\theta\).

   #+BEGIN_SRC ipython :async t
     log_mean_hat, log_inv_disp_hat, log_meas_disp_hat, alpha, beta, gamma, delta, elbo = scmodes.ebnbm.ebnbm_gamma(
       x,
       s,
       init=np.hstack([np.exp(log_inv_disp).ravel(), np.exp(-log_mean + log_inv_disp).ravel(), theta]),
       tol=1e-3, 
       max_iters=20000,
       extrapolate=True,
       fix_g=False,
       fix_theta=True)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[55]:
   :END:

   Plot the fitted observation models against the observed data.

   #+BEGIN_SRC ipython :ipyfile figure/ebnbm.org/sim-ex3.png
     cm = plt.get_cmap('Dark2')
     plt.clf()
     fig, ax = plt.subplots(1, 5, sharey=True)
     fig.set_size_inches(8, 2)

     for i, a in enumerate(ax):
       y = np.arange(x[:,i].max() + 1)
       # Poisson measurement => NB observation
       pmf = dict()
       pmf['Poisson'] = st.nbinom(n=np.exp(log_inv_disp[:,i]), p=1 / (1 + (s * np.exp(log_mean[:,i] - log_inv_disp[:,i])))).pmf(y).mean(axis=0)
       # NB measurement => Monte Carlo integral
       n_samples = 1000
       Ghat = st.gamma(a=np.exp(log_inv_disp_hat[:,i]), scale=np.exp(log_mean_hat[:,i] - log_inv_disp_hat[:,i]))
       temp = Ghat.rvs(size=(n_samples, y.shape[0], 1))
       pmf[rf'NB ($\hat\theta$ = {np.exp(log_meas_disp_hat):.2g})'] = st.nbinom(n=np.exp(-log_meas_disp_hat), p=1 / (1 + s[0] * temp * np.exp(log_meas_disp_hat))).pmf(y.reshape(-1, 1)).mean(axis=0)

       ax[i].hist(x[:,i], bins=y, color='0.7', density=True)
       for j, k in enumerate(pmf):
         ax[i].plot(y + .5, pmf[k], c=cm(j), lw=1, label=k)
       ax[i].set_title(f'Gene {i+1}')
       ax[i].set_xlim(0, x[:,i].max())

     ax[0].set_ylabel('Density')
     ax[-1].legend(title='Measurement model', frameon=False, bbox_to_anchor=(1, .5), loc='center left')
     a = fig.add_subplot(111, frameon=False, xticks=[], yticks=[])
     a.set_xlabel('Number of molecules', labelpad=16)
     fig.tight_layout(pad=0.5)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[52]:
   [[file:figure/ebnbm.org/sim-ex3.png]]
   :END:

   Compare the estimated expression models against the ground truth.

   #+BEGIN_SRC ipython :ipyfile figure/ebnbm.org/sim-ex4.png
     plt.clf()
     fig, ax = plt.subplots(1, 2)
     fig.set_size_inches(5, 2.5)
     for a in ax:
       a.set_aspect('equal', 'datalim')
     ax[0].scatter(log_mean, log_mean_hat, s=4, c='k')
     lim = ax[0].get_xlim()
     ax[0].plot(lim, lim, lw=1, ls=':', c='r')
     ax[0].set_xlabel('$\ln(\mu)$')
     ax[0].set_ylabel('$\ln(\hat\mu)$')
     ax[1].scatter(log_inv_disp, log_inv_disp_hat, s=4, c='k')
     lim = ax[1].get_xlim()
     ax[1].plot(lim, lim, lw=1, ls=':', c='r')
     ax[1].set_xlabel('$\ln(\phi)$')
     ax[1].set_ylabel('$\ln(\hat\phi)$')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[57]:
   [[file:figure/ebnbm.org/sim-ex4.png]]
   :END:

   Now fit the model, fixing \(\theta\) to the ground truth, and initializing
   the expression models at the MLE of a Gamma expression model assuming a
   Poisson measurement model.

   #+BEGIN_SRC ipython :async t
     par = np.array([scmodes.ebpm.ebpm_gamma(x[:,j], s.ravel()) for j in range(x.shape[1])])
     init = np.hstack([np.exp(par[:,1]), np.exp(par[:,1] - par[:,0]), theta])
     log_mean_hat, log_inv_disp_hat, log_meas_disp_hat, alpha, beta, gamma, delta, elbo = scmodes.ebnbm.ebnbm_gamma(
       x,
       s,
       init=init,
       tol=1e-3, 
       max_iters=20_000,
       extrapolate=True,
       fix_g=False,
       fix_theta=True)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[61]:
   :END:

   Compare the fitted expression models against the ground truth.

   #+BEGIN_SRC ipython :ipyfile figure/ebnbm.org/sim-ex5.png
     plt.clf()
     fig, ax = plt.subplots(1, 2)
     fig.set_size_inches(5, 2.5)
     for a in ax:
       a.set_aspect('equal', 'datalim')
     ax[0].scatter(log_mean, par[:,0], s=16, c='r', marker='x', label='Initialization')
     ax[0].scatter(log_mean, log_mean_hat, s=16, c='k', marker='+', label='Estimate')
     lim = ax[0].get_xlim()
     ax[0].plot(lim, lim, lw=1, ls=':', c='r')
     ax[0].legend(handletextpad=0, frameon=False)
     ax[0].set_xlabel('$\ln(\mu)$')
     ax[0].set_ylabel('$\ln(\hat\mu)$')

     ax[1].scatter(log_inv_disp, par[:,1], s=16, c='r', marker='x', label='Initialization')
     ax[1].scatter(log_inv_disp, log_inv_disp_hat, s=16, c='k', marker='+', label='Estimate')
     lim = ax[1].get_ylim()
     ax[1].plot(lim, lim, lw=1, ls=':', c='r')
     ax[1].set_xlabel('$\ln(\phi)$')
     ax[1].set_ylabel('$\ln(\hat\phi)$')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[71]:
   [[file:figure/ebnbm.org/sim-ex5.png]]
   :END:

   Compute the ELBO as a function of \(\theta\).

   #+BEGIN_SRC ipython :async t
     grid = np.logspace(-2, 1, 10)
     par = np.array([scmodes.ebpm.ebpm_gamma(x[:,j], s.ravel()) for j in range(x.shape[1])])
     fits = []
     for theta in grid:
       print(f'fitting theta={theta:.3g}')
       fit = scmodes.ebnbm.ebnbm_gamma(
         x,
         s,
         init=np.hstack([np.exp(par[:,1]), np.exp(par[:,1] - par[:,0]), theta]),
         tol=1e-4,
         max_iters=100_000,
         extrapolate=True,
         fix_g=False,
         fix_theta=True)
       # Warm start
       par = np.vstack(fit[:2]).T
       fits.append(fit)
     elbo = np.array([f[-1] for f in fits])
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[6]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/ebnbm.org/sim-elbo.png
     plt.clf()
     plt.gcf().set_size_inches(2.5, 2.5)
     plt.xscale('log')
     plt.plot(grid, elbo, lw=1, c='k')
     plt.axvline(x=0.2, lw=1, ls=':', c='r')
     plt.xlabel(r'Measurement dispersion $\theta$')
     plt.ylabel('ELBO')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[7]:
   [[file:figure/ebnbm.org/sim-elbo.png]]
   :END:

   Look at what's happening for small \(\theta\), by comparing the estimated
   expression models.

   #+BEGIN_SRC ipython :ipyfile figure/ebnbm.org/sim-ex6.png
     cm = colorcet.cm['bmy']
     plt.clf()
     fig, ax = plt.subplots(1, 2)
     fig.set_size_inches(6, 2.5)
     for a in ax:
       a.set_aspect('equal', 'datalim')

     for theta, fit in zip(grid, fits):
       c = (np.log(theta) - np.log(1e-2)) / (np.log(10) - np.log(1e-2))
       ax[0].scatter(log_mean, fit[0], s=4, c=np.array(cm(c)).reshape(1, -1))
       ax[1].scatter(log_inv_disp, fit[1], s=16, c=np.array(cm(c)).reshape(1, -1))

     lim = ax[0].get_xlim()
     ax[0].plot(lim, lim, lw=1, ls=':', c='r')
     ax[0].set_xlabel('$\ln(\mu)$')
     ax[0].set_ylabel('$\ln(\hat\mu)$')

     lim = ax[1].get_ylim()
     ax[1].plot(lim, lim, lw=1, ls=':', c='r')
     ax[1].set_xlabel('$\ln(\phi)$')
     ax[1].set_ylabel('$\ln(\hat\phi)$')

     cb = plt.colorbar(matplotlib.cm.ScalarMappable(matplotlib.colors.LogNorm(vmin=1e-2, vmax=10), cmap=cm),
                       fraction=0.05, shrink=0.5)
     cb.set_label(r'Dispersion $\theta$')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[8]:
   [[file:figure/ebnbm.org/sim-ex6.png]]
   :END:

   Compare the estimated observation models.

   #+BEGIN_SRC ipython :ipyfile figure/ebnbm.org/sim-ex7.png
     cm = plt.get_cmap('Dark2')
     plt.clf()
     fig, ax = plt.subplots(1, 5, sharey=True)
     fig.set_size_inches(8, 2)

     for i, a in enumerate(ax):
       y = np.arange(x[:,i].max() + 1)
       # Poisson measurement => NB observation
       pmf = dict()
       pmf['Poisson'] = st.nbinom(n=np.exp(log_inv_disp[:,i]), p=1 / (1 + (s * np.exp(log_mean[:,i] - log_inv_disp[:,i])))).pmf(y).mean(axis=0)

       # NB measurement => Monte Carlo integral
       n_samples = 1000
       for j in (2, 4):
         Ghat = st.gamma(a=np.exp(fits[j][1][:,i]), scale=np.exp(fits[j][0][:,i] - fits[j][1][:,i]))
         temp = Ghat.rvs(size=(n_samples, y.shape[0], 1))
         pmf[rf'NB ($\theta$ = {grid[j]:.2g})'] = st.nbinom(n=1 / grid[j], p=1 / (1 + s[0] * temp * grid[j])).pmf(y.reshape(-1, 1)).mean(axis=0)

       ax[i].hist(x[:,i], bins=y, color='0.7', density=True)
       for j, k in enumerate(pmf):
         ax[i].plot(y + .5, pmf[k], c=cm(j), lw=1, label=k)
       ax[i].set_title(f'Gene {i+1}')
       ax[i].set_xlim(0, x[:,i].max())

     ax[0].set_ylabel('Density')
     ax[-1].legend(title='Measurement model', frameon=False, bbox_to_anchor=(1, .5), loc='center left')
     a = fig.add_subplot(111, frameon=False, xticks=[], yticks=[])
     a.set_xlabel('Number of molecules', labelpad=24)
     fig.tight_layout(pad=0.5)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[9]:
   [[file:figure/ebnbm.org/sim-ex7.png]]
   :END:

** Application to control data

   Fit the model to each control data set.

   #+BEGIN_SRC sh :noweb eval :dir /scratch/midway2/aksarkar/modes/
     sbatch --partition=gpu2 --gres=gpu:1 --mem=4G -a 0
     #!/bin/bash
     source activate scmodes
     python <<EOF
     <<imports>>
     import anndata
     import pickle
     import os
     <<data>>
     tasks = [d for d in data if d in control]
     d = tasks[int(os.environ['SLURM_ARRAY_TASK_ID'])]
     x = data[d]()
     s = x.X.sum(axis=1).A
     grid = np.logspace(-2, 1, 10)
     par = np.array([scmodes.ebpm.ebpm_gamma(x.X[:,j].A.ravel(), s.ravel()) for j in range(x.shape[1])])
     fits = dict()
     for theta in grid:
       print(f'fitting theta={theta:.3g}')
       fits[theta] = scmodes.ebnbm.ebnbm_gamma(
         x.X.A,
         s,
         init=np.hstack([np.exp(par[:,1]), np.exp(par[:,1] - par[:,0]), theta]),
         tol=1e-4,
         max_iters=100_000,
         extrapolate=True,
         fix_g=False,
         fix_theta=True)
       # Warm start
       par = np.vstack(fits[theta][:2]).T
     with open(f'/scratch/midway2/aksarkar/modes/ebnbm-{d}.pkl', 'wb') as f:
       pickle.dump(fit, f)
     EOF
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 4287557

   Read the fitted models.

   #+BEGIN_SRC ipython
     import pickle
     d = 'dropseq'
     with open(f'/scratch/midway2/aksarkar/modes/ebnbm-{d}.pkl', 'rb') as f:
       fits = pickle.load(f)
   #+END_SRC
