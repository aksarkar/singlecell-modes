#+TITLE: Gamma deconvolution
#+SETUPFILE: setup.org

* Introduction

  Deconvolution assuming Gamma or point-Gamma latent gene expression
  distribution does worse than expected. Investigate why by comparing different
  implementations on an example problem.

* Setup
  :PROPERTIES:
  :CUSTOM_ID: setup
  :END:

  #+BEGIN_SRC emacs-lisp :exports none
    (add-to-list 'python-shell-completion-native-disabled-interpreters "jupyter")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(venv="scmodes",partition="mstephens",mem="16G") :dir /scratch/midway2/aksarkar/modes

  #+RESULTS:
  : Submitted batch job 60296162

  #+BEGIN_SRC ipython
    import numpy as np
    import pandas as pd
    import rpy2.robjects
    import rpy2.robjects.packages
    import rpy2.robjects.pandas2ri
    import scipy.special as sp
    import scipy.stats as st
    import scmodes
    import scqtl
    import scqtl.simple

    rpy2.robjects.pandas2ri.activate()
    pscl = rpy2.robjects.packages.importr('pscl')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[1]:
  :END:

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[2]:
  :END:

  #+BEGIN_SRC ipython
    import matplotlib.pyplot as plt
    plt.rcParams['figure.facecolor'] = 'w'
    plt.rcParams['font.family'] = 'Nimbus Sans L'
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[3]:
  :END:

* Results
** Simulation

   Simulate a simple example.

   #+BEGIN_SRC ipython
     np.random.seed(4)
     N = 1000
     s = 1e5 * np.ones(N)
     mu = 1e-3
     # Inverse dispersion = 3
     u = st.gamma(a=3, scale=1/3).rvs(size=N)
     z = (np.random.uniform(size=N) < 0.95).astype(float)
     x = np.random.poisson(lam=s * mu * u * z)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[6]:
   :END:

   Report the logit-transformed size of the point mass on zero.

   #+BEGIN_SRC ipython
     sp.logit(0.05)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[72]:
   : -2.9444389791664403
   :END:

** Simple CPU implementation

   We implemented Gamma deconvolution using the Nelder-Mead algorithm in the
   [[https://github.com/aksarkar/scqtl/][scqtl]] package.

   #+BEGIN_SRC ipython
     res = scqtl.simple.fit_zinb(x, s)
     pd.Series(res, index=['mean', 'inv_disp', 'logodds', 'llik'])
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[86]:
   #+BEGIN_EXAMPLE
     mean           0.000999
     inv_disp       3.097253
     logodds       -2.924002
     llik       -5284.263660
     dtype: float64
   #+END_EXAMPLE
   :END:

** Political Science Computation Laboratory implementation

   The R package ~pscl~ contains a function ~zeroinfl~ for fitting
   zero-inflated generalized linear models. We can write simple deconvolution
   problems in this form.

   #+BEGIN_SRC ipython
     def pscl_zinb(x, s):
       f = rpy2.robjects.Formula('x ~ offset(log(s)) + 1 | 1')
       f.environment['x'] = x
       f.environment['s'] = s
       res = pscl.zeroinfl(f, dist='negbin')
       mean = np.exp(np.array(res.rx2('coefficients').rx2('count')))
       inv_disp = np.array(res.rx2('theta'))
       logodds = np.array(res.rx2('coefficients').rx2('zero'))
       llik = np.array(res.rx2('loglik'))
       return pd.Series(np.concatenate([mean, inv_disp, logodds, llik]), index=['mean', 'inv_disp', 'logodds', 'llik'])
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[31]:
   :END:

   #+BEGIN_SRC ipython
     pscl_zinb(x, s)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[8]:
   #+BEGIN_EXAMPLE
     mean           0.000999
     inv_disp       3.097312
     logodds       -2.923963
     llik       -5284.263659
     dtype: float64
   #+END_EXAMPLE
   :END:

   Looking at the implementation, there appear to be three major differences:

   1. [[https://github.com/cran/pscl/blob/master/R/zeroinfl.R#L410][Optimization
      using BFGS]]
   2. [[https://github.com/cran/pscl/blob/master/R/zeroinfl.R#L83][Analytic
      computation of the gradient]]
   3. [[https://github.com/cran/pscl/blob/master/R/zeroinfl.R#L243][Initialization
      from Poisson solution]]

** Comparison of CPU/GPU implementation on real data

   We previously fit point-Gamma distributions to the latent gene expression of
   sorted CD8+ T cells from Zheng et al. 2017.

   Read the GPU-fitted results.

   #+BEGIN_SRC ipython
     gpu_log_mu = pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-example/zheng-cd8-zinb-log-mu.txt.gz', sep='\t', index_col=0)
     gpu_log_phi = pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-example/zheng-cd8-zinb-log-phi.txt.gz', sep='\t', index_col=0)
     gpu_logodds = pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-example/zheng-cd8-zinb-logodds.txt.gz', sep='\t', index_col=0)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[5]:
   :END:

   Compute the log likelihood of the GPU solution.

   #+BEGIN_SRC ipython
     x = scmodes.dataset.read_10x('/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/cytotoxic_t/filtered_matrices_mex/hg19')
     s = x.sum(axis=1)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[16]:
   :END:

   #+BEGIN_SRC ipython
     gpu_nb_llik = np.array([st.nbinom(n=np.exp(-log_phi), p=1/(1 + s * np.exp(log_mu + log_phi))).logpmf(x_j) for log_mu, log_phi, x_j in zip(gpu_log_mu.values.T, gpu_log_phi.values.T, x.T)])
     case_zero = -np.log1p(np.exp(-gpu_logodds.values.T)) + np.log1p(np.exp(gpu_nb_llik - gpu_logodds.values.T))
     case_nonzero = -np.log1p(np.exp(gpu_logodds.values.T)) + gpu_nb_llik
     gpu_zinb_llik = np.where(x.T < 1, case_zero, case_nonzero)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[18]:
   :END:

   Write out the GPU results.

   #+BEGIN_SRC ipython
     gpu_res = pd.DataFrame({'mean': np.exp(gpu_log_mu.values.ravel()),
                             'inv_disp': np.exp(-gpu_log_phi.values.ravel()),
                             'logodds': gpu_logodds.values.ravel(),
                             'llik': gpu_zinb_llik.sum(axis=1)})
     gpu_res.to_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/gamma-deconv/gpu.txt.gz', sep='\t', compression='gzip')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[27]:
   :END:

   Run CPU methods.

   #+BEGIN_SRC ipython
     def fit_all(x, s, method):
       res = []
       for j in range(x.shape[1]):
         try:
           res.append(method(x[:,j], s))
         except:
           # Failed to converge
           res.append(np.array([np.nan, np.nan, np.nan, np.nan]))
       return pd.DataFrame(res, columns=['mean', 'inv_disp', 'logodds', 'llik'])
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[28]:
   :END:

   #+BEGIN_SRC ipython :async t
     scqtl_res = fit_all(x, s, scqtl.simple.fit_zinb)
     scqtl_res.to_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/gamma-deconv/scqtl.txt.gz', sep='\t', compression='gzip')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[33]:
   :END:

   #+BEGIN_SRC ipython :async t
     pscl_res = fit_all(x, s, pscl_zinb)
     pscl_res.to_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/gamma-deconv/pscl.txt.gz', sep='\t', compression='gzip')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[32]:
   :END:

   Read the results.

   #+BEGIN_SRC ipython
     gpu_res = pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/gamma-deconv/gpu.txt.gz', sep='\t')
     pscl_res = pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/gamma-deconv/pscl.txt.gz', sep='\t')
     scqtl_res = pd.read_csv('/project2/mstephens/aksarkar/projects/singlecell-modes/data/gamma-deconv/scqtl.txt.gz', sep='\t')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[17]:
   :END:

   Plot the difference in (training) log likelihood.

   #+BEGIN_SRC ipython
     T = pd.DataFrame({'gpu': gpu_res['llik'],
                       'cpu': scqtl_res['llik'],
                       'pscl': pscl_res['llik']}).dropna()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[18]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/gamma.org/training-llik-diff.png
     plt.clf()
     plt.gcf().set_size_inches(3, 3)
     plt.boxplot((T.values - T['gpu'].values.reshape(-1, 1))[:,1:] / x.shape[0],
                 medianprops={'color': 'k'}, flierprops={'marker': '.', 'markersize': 4})
     plt.xticks(range(1, 3), T.columns[1:])
     plt.xlabel('Method')
     plt.ylabel('Per obs diff train llik from GPU')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[58]:
   [[file:figure/gamma.org/training-llik-diff.png]]
   :END:

   Look at cases where CPU does much better than GPU.

   #+BEGIN_SRC ipython
     llik_diff = T['cpu'] - T['gpu']
     query = llik_diff[llik_diff / x.shape[0] > 0.5].index
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[19]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/gamma.org/zig-examples.png
     plt.clf()
     fig, ax = plt.subplots(1, 2)
     fig.set_size_inches(4, 2)
     for a, k in zip(ax, query):
       a.hist(x[:,k], color='k', bins=np.arange(x[:,k].max() + 1))
       a.set_xlabel('Num mols')
       a.set_ylabel('Num cells')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[84]:
   [[file:figure/gamma.org/zig-examples.png]]
   :END:

   Inspect the GPU result.
   
   #+BEGIN_SRC ipython
     gpu_res.iloc[query]
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[86]:
   #+BEGIN_EXAMPLE
     mean     inv_disp    logodds          llik
     25   0.000358  1164.383452 -12.345718  -9030.897929
     377  0.000647   990.383954 -13.082376 -12151.227973
   #+END_EXAMPLE
   :END:

   Inspect the CPU result.

   #+BEGIN_SRC ipython
     scqtl_res.iloc[query]
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[88]:
   #+BEGIN_EXAMPLE
     mean      inv_disp   logodds         llik
     25   0.000356  2.852713e+14 -8.394238   610.494460
     377  0.000645  2.852848e+14 -8.395699  1006.905231
   #+END_EXAMPLE
   :END:

   The results suggest the log likelihood computation had a numerical problem.
   Check against a Poisson model.

   #+BEGIN_SRC ipython
     pd.DataFrame([scqtl.simple.fit_pois(xj, s) for xj in x[:,query].T], columns=['mean', 'llik'])
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[98]:
   #+BEGIN_EXAMPLE
     mean          llik
     0  0.000358  -9030.866338
     1  0.000647 -12151.180012
   #+END_EXAMPLE
   :END:

   Check our implementation of ZINB log likelihood.

   #+BEGIN_SRC ipython
     scmodes.benchmark.deconvolution.zinb_llik(x, s.reshape(-1, 1) * gpu_res['mean'].values, gpu_res['inv_disp'].values, gpu_res['logodds'].values).sum(axis=0)[query]
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[33]:
   : array([ -9030.89792855, -12151.22797265])
   :END:
