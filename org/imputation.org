#+TITLE: Imputation of count matrices
#+SETUPFILE: setup.org

* Introduction
  :PROPERTIES:
  :CUSTOM_ID: introduction
  :END:

  The key idea of our approach to modeling scRNA-seq is to separate sampling
  variation and expression variation. This approach leads to the following
  multi-gene model for scRNA-seq data: \(
  \newcommand\const{\mathrm{const}}
  \newcommand\E[1]{\left\langle #1 \right\rangle}
  \newcommand\vx{\mathbf{x}}
  \newcommand\vw{\mathbf{w}}
  \newcommand\vz{\mathbf{z}}
  \newcommand\mx{\mathbf{X}}
  \newcommand\mU{\mathbf{U}}
  \newcommand\mw{\mathbf{W}}
  \newcommand\mz{\mathbf{Z}}
  \newcommand\ml{\mathbf{L}}
  \newcommand\mf{\mathbf{F}}
  \)

  \begin{align*}
    x_{ij} &\sim \operatorname{Poisson}(\lambda_{ij})\\
    \lambda_{ij} &= h^{-1}((\ml\mf')_{ij})
  \end{align*}

  where \(i = 1, \ldots, n\), \(j = 1, \ldots, p\), \(\ml\) is an \(n \times
  K\) matrix, and \(\mf\) is a \(p \times K\) matrix. (Here, we absorb the size
  factor into \(\ml\).) We [[file:lra.org][previously used Poisson thinning of
  real data sets to benchmark methods]] which fit this model. The key idea of
  this approach is that binomial sampling results in two data matrices which
  have the same \(\lambda_{ij}\), and this works even without knowing the
  ground truth \(\lambda_{ij}\). However, it is natural to further assume that
  expression variation itself can be partitioned into /structured/ and
  /unstructured/ variation, e.g.

  \begin{align*}
    x_{ij} &\sim \operatorname{Poisson}(\lambda_{ij})\\
    \lambda_{ij} &= \mu_{ij} u_{ij}\\
    \mu_{ij} &= h^{-1}((\ml\mf')_{ij})\\
    u_{ij} &\sim \operatorname{Gamma}(1/\phi, 1/\phi)
  \end{align*}

  *Remark* This formulation suggests against making a point-Gamma assumption on
  \(\lambda_{ij}\), because the implicit assumption on multiplicative random
  effect \(u_{ij}\) is that it does not have mean 1.

  We cannot use Poisson thinning to evaluate methods on fitting this model,
  because the evaluation requires the ground truth \(\mu_{ij}\). Here, we study
  the problem of masking data entries in an scRNA-seq count matrix and imputing
  them by estimating \(\mu_{ij}\). Critically, this masking /cannot/ be
  achieved simply by setting some entries to zero, because zero is a valid
  non-missing observation in scRNA-seq data. Instead, we require methods which
  can handle missing data, e.g., having associated weights. The key idea which
  enables fast methods for our Poisson model of interest is that the likelihood
  factorizes, so we can maximize the incomplete data log likelihood (or a lower
  bound) /without/ marginalizing over unobserved data.

* Setup
  :PROPERTIES:
  :CUSTOM_ID: setup
  :END:

  #+BEGIN_SRC emacs-lisp :exports none
    (add-to-list 'python-shell-completion-native-disabled-interpreters "jupyter")
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(venv="scmodes",partition="mstephens",memory="16G") :exports none :dir /scratch/midway2/aksarkar/modes

  #+RESULTS:
  : Submitted batch job 64230390

  #+NAME: imports
  #+BEGIN_SRC ipython
    import numpy as np
    import pandas as pd
    import scipy.stats as st
    import scmodes
  #+END_SRC

  #+RESULTS: imports
  :RESULTS:
  # Out[1]:
  :END:

  #+BEGIN_SRC ipython
    %matplotlib inline
    %config InlineBackend.figure_formats = set(['retina'])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[2]:
  :END:

  #+BEGIN_SRC ipython
    import matplotlib.pyplot as plt
    plt.rcParams['figure.facecolor'] = 'w'
    plt.rcParams['font.family'] = 'Nimbus Sans'
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[3]:
  :END:

* Methods
  :PROPERTIES:
  :CUSTOM_ID: methods
  :END:
** Weighted non-negative matrix factorization
   :PROPERTIES:
   :CUSTOM_ID: wnmf
   :END:

   Weighted non-negative matrix factorization (WNMF;
   [[https://epubs.siam.org/doi/abs/10.1137/1.9781611972764.58][Zhang et
   al. 2006]], [[https://www.cs.umd.edu/~bhargav/nips2010.pdf][Khanagal et
   al. 2010]]) is an extension of NMF (Lee & Seung 2001) to handle incomplete
   data associated with weights \(w_{ij} \in \{0, 1\}\). In our model of
   interest, the classical multiplicative updates are in fact EM updates of an
   augmented model (Cemgil 2009)

   \begin{align*}
     x_{ij} &= \sum_k z_{ijk}\\
     z_{ijk} &\sim \operatorname{Poisson}(l_{ik} f_{jk})
   \end{align*}

   and we can derive how the updates should change for incomplete data. The log
   posterior

   \begin{align*}
     \ln p(\mz \mid \mx, \mw, \ml, \mf) &= \ln p(\mx, \mz \mid \ml, \mf, \mw) - \ln p(\mx \mid \ml, \mf, \mw)\\
     &= \sum_{i,j,k} w_{ij} \left[z_{ijk} \ln (l_{ik}f_{jk}) - l_{ik} f_{jk} + \ln\Gamma(z_{ijk} + 1)\right] - \sum_{i,j,k} w_{ij} \left[x_{ij} \ln\left(\sum_k l_{ik}f_{jk}\right) - \sum_k l_{ik} f_{jk} + \ln\Gamma(x_{ij} + 1)\right]\\
     &= \sum_{i, j, k} \left[w_{ij} z_{ijk} \ln\left(\frac{l_{ik}{f_{jk}}}{\sum_t l_{it} f_{jt}}\right)\right] + \const\\
     &= \sum_{i, j} w_{ij} \operatorname{Multinomial}(\cdot; x_{ij}, \frac{l_{i1} f_{j1}}{\sum_t l_{it} f_{jt}}, \ldots, \frac{l_{iK} f_{jK}}{\sum_t l_{it} f_{jt}})
   \end{align*}

   Therefore

   \[ \E{z_{ijk}} = x_{ij} \frac{l_{ik} f_{jk}}{\sum_t l_{it} f_{jt}} \]

   when \(w_{ij} = 1\), and is missing otherwise. The expected log joint

   \[ \E{\ln p(\mx, \mz \mid \ml, \mf, \mw)} = \sum_{i, j, k} w_{ij} [\E{z_{ijk}} \ln(l_{ij} f_{jk}) - l_{ik} f_{jk}] + \const \]

   yielding M step updates

   \begin{align*}
     l_{ik} &:= \frac{\sum_j w_{ij} \E{z_{ijk}}}{\sum_j w_{ij} f_{jk}}\\
     f_{jk} &:= \frac{\sum_i w_{ij} \E{z_{ijk}}}{\sum_i w_{ij} l_{ik}}
   \end{align*}

   Plugging in \(\E{z_{ijk}}\) yields the classical multiplicative updates,
   modified by introducing the weights in the numerator and denominator.

** Weighted Negative Binomial Matrix Factorization
   :PROPERTIES:
   :CUSTOM_ID: wnbmf
   :END:

   Negative Binomial Matrix Factorization (NBMF; Gouvert et al 2018) is the
   (augmented) model

   \begin{align*}
     x_{ij} &= \sum_{k=1}^K z_{ijk}\\
     z_{ijk} &\sim \operatorname{Poisson}(l_{ik} f_{jk} u_{ij})\\
     u_{ij} &\sim \operatorname{Gamma}(1/\phi_{ij}, 1/\phi_{ij})
   \end{align*}

   where the Gamma distribution is parameterized by a shape and a rate. (The
   mean of the Gamma distribution is 1, and its variance is \(\phi_{ij}\).)
   Gouvert et al. 2018 only consider the case \(\phi_{ij} = \phi\); however,
   other natural choices are \(\phi_{ij} = \phi_j\) and \(\phi_{ij} = \phi_i
   \phi_j\). To derive an EM algorithm, first note that

   \[ p(\mz \mid \mx, \mU, \ml, \mf) 
   = p(\mz \mid \mx, \ml, \mf)
   = \sum_{i, j} \operatorname{Multinomial}(\cdot; x_{ij}, \frac{l_{i1} f_{j1}}{\mu_{ij}}, \ldots, \frac{l_{iK} f_{jK}}{\mu_{ij}}) \]

   where \(\mu_{ij} \triangleq \sum_t l_{it} f_{jt}\). Further,

   \begin{align*}
     p(u_{ij} \mid \mx, \mz, \ml, \mf) &\propto p(\mx, \mz, u_{ij} \mid \ml, \mf)\\
     &\propto \prod_k u_{ij}^{z_{ijk}} \exp(-l_{ik} f_{jk} u_{ij}) u_{ij}^{1/\phi_{ij} - 1} \exp(-u_{ij} / \phi_{ij})\\
     &\propto u_{ij}^{x_{ij} + 1 / \phi_{ij} - 1} \exp\left(-u_{ij}\left(\mu_{ij} + 1 / \phi_{ij}\right)\right)\\
     &= \operatorname{Gamma}(x_{ij} + 1 / \phi_{ij}, \mu_{ij} + 1 / \phi_{ij})
   \end{align*}

   The expected log joint

   \begin{multline*}
     \ell \triangleq \E{\ln p(\mx, \mz, \mU \mid \ml, \mf)} = \sum_{i, j, k} [\E{z_{ijk}}(\E{\ln u_{ij}} + \ln (l_{ik} f_{jk})) - l_{ik} f_{jk} \E{u_{ij}} - \ln\Gamma(x_{ij} + 1)]\\
     + \sum_{i, j} [(1 / \phi_{ij}) \ln(1 / \phi_{ij}) + (1 / \phi_{ij} - 1) \E{\ln u_{ij}} - \E{u_{ij}} / \phi_{ij} - \ln\Gamma(1 / \phi_{ij})]
   \end{multline*}

   yielding analytic M step updates

   \begin{align*}
     l_{ik} &= \frac{\sum_j \E{z_{ijk}}}{\sum_j f_{jk} \E{u_{ij}}}\\
     f_{jk} &= \frac{\sum_i \E{z_{ijk}}}{\sum_i l_{ik} \E{u_{ij}}}
   \end{align*}

   Using properties of the Gamma distribution,

   \begin{align*}
     \E{u_{ij}} &= \frac{x_{ij} + 1/\phi_{ij}}{\mu_{ij} + 1/\phi_{ij}}\\
     \E{\ln u_{ij}} &= \psi(x_{ij} + 1 / \phi_{ij}) - \ln(\mu_{ij} + 1 / \phi_{ij})
   \end{align*}

   Plugging in \(\E{z_{ijk}}\), \(\E{u_{ij}}\) yields multiplicative updates of
   a form similar to WNMF. Following the arguments for WNMF, if we introduce
   weights \(w_{ij} \in \{0, 1\}\), they will enter the M step updates in the
   numerator and denominator.

   Gouvert et al. 2018 treat \(\phi_{ij}\) as fixed, rather than estimating it
   from data. For the easy case \(\phi_{ij} = \phi\), numerical optimization of
   the expected log joint with respect to \(\theta = 1 / \phi\) is
   straightforward (using e.g. Newton's method):

   \begin{align*}
     \frac{\partial \ell}{\partial \theta} &= \sum_{i, j} \Bigg[1 + \ln \theta + \E{\ln u_{ij}} - \E{u_{ij}} - \psi(\theta)\Bigg]\\
     \frac{\partial^2 \ell}{\partial \theta^2} &= \sum_{i, j} \Bigg[\frac{1}{\theta} - \psi_1(\theta)\Bigg]
   \end{align*}

   where \(\psi(\cdot)\) denotes the digamma function and \(\psi_1(\cdot)\)
   denotes the trigamma function. It is clear that weights \(w_{ij} \in \{0,
   1\}\) enter inside the sums in these expressions.

** Weighted GLM-PCA
   :PROPERTIES:
   :CUSTOM_ID: wglmpca
   :END:

   GLM-PCA ([[https://arxiv.org/abs/1907.02647][Townes 2019]]) uses
   [[https://en.wikipedia.org/wiki/Scoring_algorithm][Fisher scoring]] to fit
   the model assuming \(h = \log\). The weighted version of the problem is

   \begin{align*}
     \ell \triangleq \ln p(\mx \mid \ml, \mf, \mw) &= \sum_{i, j} w_{ij} \left[x_{ij} \sum_k l_{ik} f_{jk} - \exp\left(\sum_t l_{it} f_{jt}\right) \right] + \const\\
     \frac{\partial \ell}{\partial l_{ik}} &= \sum_j w_{ij} \left[x_{ij} f_{jk} - \exp\left(\sum_t l_{it} f_{jt}\right) f_{jk}\right]\\
     \frac{\partial \ell}{\partial f_{jk}} &= \sum_i w_{ij} \left[x_{ij} l_{ik} - \exp\left(\sum_t l_{it} f_{jt}\right) l_{ik}\right]\\
     \mathcal{I}(l_{ik}) &= \sum_j w_{ij} \exp\left(\sum_t l_{it} f_{jt}\right) f_{jk}^2\\
     \mathcal{I}(f_{jk}) &= \sum_i w_{ij} \exp\left(\sum_t l_{it} f_{jt}\right) l_{ik}^2
   \end{align*}

** Incomplete data VAE
   :PROPERTIES:
   :CUSTOM_ID: vae
   :END:

   Variational autoencoders fit a generative model parameterized by a neural
   network (Kingma and Welling 2014, Rezende and Mohammed 2014). Our model of
   interest is

   \begin{align*}
     x_{ij} \mid \lambda_{ij} &\sim \operatorname{Poisson}(\lambda_{ij})\\
     \lambda_{ij} \mid \vz_i, u_{ij} &= \mu(\vz_i)_j\, u_{ij}\\
     u_{ij} &\sim p(u_{ij})\\
     \vz_i &\sim \mathcal{N}(\boldsymbol{0}, \mathbf{I}_K)
   \end{align*}

   where \(\mu(\cdot)\) is a \(p\)-dimensional output of a fully connected
   feed-forward neural network. To approximate the intractable posterior
   \(p(\vz_i \mid \vx_i)\), we use a variational approximation

   \[ q(\vz_i \mid \vx_i) = \mathcal{N}(m(\vz_i), \operatorname{diag}(S(\vz_i))) \]

   where \(m(\cdot), S(\cdot)\) are \(K\)-dimensional outputs of a FF
   network. Fitting VAEs with incomplete data has only recently been studied
   ([[https://arxiv.org/abs/1807.03653][Nazabal et al. 2018]],
   [[https://arxiv.org/abs/1812.02633][Mattei and Frellsen 2018]]). The key
   idea is that if the coordinates of \(\vx_i\) are separable (meaning the
   likelihood factorizes) and coordinates are missing at random, then
   maximizing the log likelihood (or a lower bound to the log likelihood) of
   only the observed coordinates is a statistically sound procedure. In this
   case, the lower bound is

   \[ \ell \triangleq \sum_{i, j} \E{w_{ij} \ln\left(\int_0^\infty \operatorname{Poisson}(x_{ij}; \mu(\vz_i)_j\, u_{ij})\; dp(u_{ij})\right)} - \sum_{i} \E{\ln\left(\frac{q(\vz_i \mid f(\vx_i))}{p(\vz_i)}\right)}\]

   where \(f\) is some imputation function (e.g., fill in missing values with
   zero) and expectations are taken with respect to \(q\). For easy choices of
   \(p(u_{ij})\), the integrals inside the expectation are analytic. Mattei and
   Frellsen 2018 suggest filling missing data with 0 in the encoder network
   works, and that a tighter bound can be achieved using importance sampling
   ([[https://arxiv.org/abs/1509.00519][Burda et al. 2016]])

   \[ \ell_{\text{IWAE}} \triangleq \sum_i \E{\ln\left(\frac{1}{S} \sum_{s=1}^S \frac{p(\vx_i, \vz_i^{(s)})}{q(\vz_i^{(s)} \mid \vx_i)}\right)} \]

** Datasets
   :PROPERTIES:
   :CUSTOM_ID: datasets
   :END:

   #+NAME: data
   #+BEGIN_SRC ipython
     def _read_10x(k, min_detect=0.01, n_cells=1000, seed=1):
       return scmodes.dataset.read_10x(f'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/{k}/filtered_matrices_mex/hg19/', min_detect=0.01, return_df=True).sample(n=n_cells, axis=0, random_state=seed)

     def _mix_10x(k1, k2, min_detect=0.01, n_cells=1000, seed=1):
       x1 = scmodes.dataset.read_10x(f'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/{k1}/filtered_matrices_mex/hg19/', return_df=True, min_detect=0)
       x2 = scmodes.dataset.read_10x(f'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/{k2}/filtered_matrices_mex/hg19/', return_df=True, min_detect=0)
       return scmodes.dataset.synthetic_mix(x1, x2, min_detect=min_detect)[0].sample(n=n_cells, axis=0, random_state=seed)

     def _cd8_cd19_mix(**kwargs):
       return _mix_10x('cytotoxic_t', 'b_cells', **kwargs)

     def _cyto_naive_mix(**kwargs):
       return _mix_10x('cytotoxic_t', 'naive_t', **kwargs)

     data = {
       'cytotoxic_t': lambda: _read_10x('cytotoxic_t'),
       'b_cells': lambda: _read_10x('b_cells'),
       'ipsc': lambda: scmodes.dataset.ipsc('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/', return_df=True).sample(n=1000, axis=0, random_state=1),
       'cytotoxic_t-b_cells': _cd8_cd19_mix,
       'cytotoxic_t-naive_t': _cyto_naive_mix,
       'pbmcs_68k': lambda: _read_10x('fresh_68k_pbmc_donor_a'),
     }
   #+END_SRC

   #+RESULTS: data
   :RESULTS:
   # Out[5]:
   :END:

   Report the data dimensions.

   #+BEGIN_SRC ipython :async t
     pd.DataFrame([data[k]().shape for k in data],
                  columns=['num_cells', 'num_genes'],
                  index=data.keys())
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[11]:
   #+BEGIN_EXAMPLE
     num_cells  num_genes
     cytotoxic_t               1000       6530
     b_cells                   1000       6417
     ipsc                      1000       9957
     cytotoxic_t-b_cells       1000       6647
     cytotoxic_t-naive_t       1000       6246
     pbmcs_68k                 1000       6502
   #+END_EXAMPLE
   :END:

   Report the proportion of zeros.

   #+BEGIN_SRC ipython :async t
     pd.Series({k: (data[k]() == 0).values.mean() for k in data})
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[6]:
   #+BEGIN_EXAMPLE
     cytotoxic_t            0.914572
     b_cells                0.921772
     ipsc                   0.291596
     cytotoxic_t-b_cells    0.920908
     cytotoxic_t-naive_t    0.917528
     pbmcs_68k              0.920052
     dtype: float64
   #+END_EXAMPLE
   :END:

* Results
  :PROPERTIES:
  :CUSTOM_ID: results
  :END:
** Simulated Poisson example
   :PROPERTIES:
   :CUSTOM_ID: simulation
   :END:

   Simulate some data where \([\lambda_{ij}]\) is rank 3.

   #+BEGIN_SRC ipython
     np.random.seed(0)
     n = 500
     p = 1000
     k = 3
     l = np.random.lognormal(sigma=.5, size=(n, k))
     f = np.random.lognormal(sigma=.5, size=(p, k))
     lam = l.dot(f.T)
     x = np.random.poisson(lam=lam)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[71]:
   :END:

   Get the oracle rank of \([\ln \lambda_{ij}]\).

   #+BEGIN_SRC ipython
     d = np.linalg.svd(np.log(lam))[1]
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[54]:
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/imputation.org/sim-spectrum.png
     plt.clf()
     plt.gcf().set_size_inches(5, 3)
     plt.plot(np.arange(n), d, lw=1, c='k')
     plt.xlabel('Singular value')
     plt.ylabel('Magnitude')
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[53]:
   : Text(0, 0.5, 'Magnitude')
   [[file:figure/imputation.org/sim-spectrum.png]]
   :END:

   #+BEGIN_SRC ipython
     np.where(d <= 1e-4)[0].min()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[58]:
   : 29
   :END:

   Mask 10% of entries at random. For each choice of rank, evaluate the
   imputation loss of different methods.

   #+BEGIN_SRC ipython :async t
     imputation_res = []
     for rank in range(1, 11):
       for method in ('oracle', 'ebpm_point', 'wnmf', 'wglmpca'):
         try:
           loss = getattr(scmodes.benchmark, f'imputation_score_{method}')(x, rank=rank, frac=0.1, seed=0)
         except RuntimeError:
           # WGLMPCA fails often
           loss = np.nan
         imputation_res.append([method, rank, loss])
     imputation_res = pd.DataFrame(imputation_res, columns=['method', 'rank', 'loss'])
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[47]:
   :END:

   Plot the results.

   #+BEGIN_SRC ipython :ipyfile figure/imputation.org/sim.png
     cm = plt.get_cmap('Dark2')
     plt.clf()
     plt.gcf().set_size_inches(4, 3)
     for i, (k, g) in enumerate(imputation_res.groupby('method')):
       plt.plot(g['rank'], g['loss'], lw=1, marker=None, c=cm(i), label=k.upper())
     plt.axvline(x=3, lw=1, ls=':', c='k')
     plt.legend(frameon=False, loc='center left', bbox_to_anchor=(1, .5))
     plt.xticks(np.arange(1, 11), np.arange(1, 11))
     plt.xlabel('Assumed rank')
     plt.ylabel('Poisson loss')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[100]:
   [[file:figure/imputation.org/sim.png]]
   :END:

   Zoom in on WNMF and WGLMPCA.

   #+BEGIN_SRC ipython :ipyfile figure/imputation.org/sim-inset.png
     cm = plt.get_cmap('Dark2')
     plt.clf()
     plt.gcf().set_size_inches(4, 3)
     for i, (k, g) in enumerate(imputation_res.groupby('method')):
       if k in ('wnmf', 'wglmpca'):
         plt.plot(g['rank'], g['loss'], lw=1, marker=None, c=cm(i), label=k.upper())
     plt.axvline(x=3, lw=1, ls=':', c='k')
     plt.legend(frameon=False, loc='center left', bbox_to_anchor=(1, .5))
     plt.xticks(np.arange(1, 11), np.arange(1, 11))
     plt.xlabel('Assumed rank')
     plt.ylabel('Poisson loss')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[101]:
   [[file:figure/imputation.org/sim-inset.png]]
   :END:

   Mask the same 10% of the entries, and look at the predicted values.

   #+BEGIN_SRC ipython :async t
     import scmodes.benchmark.imputation

     w = scmodes.benchmark.imputation._mask_entries(x, frac=0.1, seed=0)
     wnmf_res = scmodes.lra.nmf(x, w=w, rank=3)
     # Hack
     wglmpca_res = scmodes.lra.glmpca(.1 * x, w=w, rank=3, max_iters=5000)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[83]:
   :END:

   #+BEGIN_SRC ipython
     wnmf_pred = wnmf_res[0].dot(wnmf_res[1].T)[~w].ravel()
     wglmpca_pred = np.exp(np.log(10) + wglmpca_res[0].dot(wglmpca_res[1].T))[~w].ravel()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[84]:
   :END:

   #+BEGIN_SRC ipython
     pd.Series({
       'wnmf': st.poisson(mu=wnmf_pred).logpmf(x[~w].ravel()).sum(),
       'wglmpca': st.poisson(mu=wglmpca_pred).logpmf(x[~w].ravel()).sum()})
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[93]:
   #+BEGIN_EXAMPLE
     wnmf      -100876.82653
     wglmpca   -101614.49192
     dtype: float64
   #+END_EXAMPLE
   :END:

   #+BEGIN_SRC ipython :ipyfile figure/imputation.org/sim-pred.png
     plt.clf()
     plt.gcf().set_size_inches(3, 3)
     plt.scatter(wnmf_pred, wglmpca_pred, s=1, c='k', alpha=0.1)
     plt.plot([0, 25], [0, 25], lw=1, ls=':', c='r')
     plt.xlabel('WNMF predicted value')
     plt.ylabel('WGLMPCA predicted value')
     plt.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[87]:
   [[file:figure/imputation.org/sim-pred.png]]
   :END:

** Imputation benchmark

   #+BEGIN_SRC sh :noweb eval :dir /scratch/midway2/aksarkar/modes/
     sbatch --partition=broadwl -n1 -c28 --exclusive --time=12:00:00 --job-name=imputation-benchmark -a 0-5
     #!/bin/bash
     source activate scmodes
     python <<EOF
     <<imports>>
     import os
     import sys
     <<data>>
     tasks = [(k, m) for k in data for m in ('ebpm_point',)]
     d, m = tasks[int(os.environ['SLURM_ARRAY_TASK_ID'])]
     x = data[d]()
     res = scmodes.benchmark.evaluate_imputation(x.values, methods=[m], n_trials=10, max_retries=3)
     res.to_csv(f'/scratch/midway2/aksarkar/modes/imputation/{d}-{m}.txt.gz', compression='gzip', sep='\t')
     EOF
   #+END_SRC

   #+RESULTS:
   : Submitted batch job 64111638

   Move the results to permanent storage.

   #+BEGIN_SRC sh
     rsync -au /scratch/midway2/aksarkar/modes/imputation/ /project2/mstephens/aksarkar/projects/singlecell-modes/data/imputation/
   #+END_SRC

   #+RESULTS:

   Read the results.

   #+BEGIN_SRC ipython
     methods = ['oracle', 'ebpm_point', 'wnmf', 'wglmpca']
     imputation_res = (pd.concat({(d, m): pd.read_csv(f'/project2/mstephens/aksarkar/projects/singlecell-modes/data/imputation/{d}-{m}.txt.gz', sep='\t', index_col=0)
                                 for d in data for m in methods})
                       .reset_index(level=0)
                       .reset_index(drop=True)
                       .rename({'level_0': 'dataset'}, axis=1))
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[6]:
   :END:

   Plot the results.

   #+BEGIN_SRC ipython :ipyfile figure/imputation.org/pois-loss.png
     labels = ['Oracle', 'EBPM-Point', 'WNMF', 'WGLMPCA']
     plt.clf()
     fig, ax = plt.subplots(1, len(data), sharey=True)
     fig.set_size_inches(8, 2.5)
     for a, d, t in zip(ax, data, ['T cells', 'B cells', 'iPSC', 'T cell/B cell', 'Cytotoxic/naive T', 'PBMC']):
       g = imputation_res[imputation_res['dataset'] == d]
       a.set_yscale('log')
       for x, (m, k) in enumerate(zip(methods, labels)):
         y = g.loc[g['method'] == m]
         a.scatter(x + np.random.normal(size=y.shape[0], scale=0.1), y['loss'], s=2, c='k', zorder=4)
       a.set_title(t)
       a.grid(c='0.8', lw=1, axis='x')
       a.set_xticks(np.arange(4))
       a.set_xticklabels(labels, rotation=90)
       a.set_xlabel('Method')
     ax[0].set_ylabel('Poisson loss')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[66]:
   [[file:figure/imputation.org/pois-loss.png]]
   :END:

   Plot the improvement over mean imputation.

   #+BEGIN_SRC ipython :ipyfile figure/imputation.org/pois-loss-improvement.png
     plt.clf()
     fig, ax = plt.subplots(2, 3, sharex=True)
     fig.set_size_inches(5, 4)
     for a, d, t in zip(ax.ravel(), data, ['T cells', 'B cells', 'iPSC', 'T cell/B cell', 'Cytotoxic/naive T', 'PBMC']):
       g = imputation_res[imputation_res['dataset'] == d]
       base = g.loc[g['method'] == 'ebpm_point']
       for x, m in enumerate(methods[2:]):
         y = g.loc[g['method'] == m]
         a.scatter(x + np.random.normal(size=y.shape[0], scale=0.1),
                   1e5 * (base['loss'].values - y['loss'].values), s=2, c='k', zorder=4)
       a.axhline(y=0, c='r', lw=1, ls=':')
       a.set_title(t)
       a.grid(c='0.8', lw=1, axis='x')
     for a in ax[-1]:
       a.set_xticks(np.arange(2))
       a.set_xticklabels([m.upper() for m in methods[2:]], rotation=90)
       a.set_xlabel('Method')
     a = fig.add_subplot(111, frameon=False)
     a.tick_params(labelcolor='none', top='off', bottom='off', left='off', right='off')
     a.set_ylabel('Improvement in Poisson loss\nover mean imputation')
     fig.tight_layout()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[67]:
   [[file:figure/imputation.org/pois-loss-improvement.png]]
   :END:
