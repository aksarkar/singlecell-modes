<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2019-04-06 Sat 16:45 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Comparison of expression deconvolution approaches</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Abhishek Sarkar">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link href="css/bootstrap.min.css" rel="stylesheet" />
<link rel="stylesheet" type="text/css" href="css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="css/supp.css"/>
<style type="text/css">body {width: 60em; margin:auto} pre.src {overflow:auto}</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Comparison of expression deconvolution approaches</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgdd973c4">Introduction</a></li>
<li><a href="#org8316ba8">Setup</a></li>
<li><a href="#orga35b51b">Methods</a>
<ul>
<li><a href="#orga21875c">Distribution deconvolution</a></li>
</ul>
</li>
<li><a href="#org030a59b">Results</a>
<ul>
<li><a href="#org6ebc2bf">Homogeneous cell populations</a></li>
<li><a href="#orgf65a0fe">Synthetic cell mixtures</a></li>
<li><a href="#orgae913a7">Heterogeneous cell populations</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgdd973c4" class="outline-2">
<h2 id="orgdd973c4">Introduction</h2>
<div class="outline-text-2" id="text-orgdd973c4">
<p>
Suppose we have observations \(x_i \sim f(\theta_i), i = 1, \ldots, n\), and
\(\theta_i \sim g(\cdot)\). <i>Distribution deconvolution</i> is the problem of
estimating \(g \in \mathcal{G}\) from \(x_1, \ldots, x_n\), assuming \(f\) is known
(<a href="https://academic.oup.com/biomet/article/103/1/1/2390141">Efron
2016</a>).
</p>

<p>
Recent work suggests that scRNA-seq data follows this generative model
(<a href="http://dx.doi.org/10.1073/pnas.1721085115">Wang et al. 2018</a>). Here, we
investigate the trade-off between model complexity/flexibility and
generalization for different choices of \(\mathcal{G}\) in real data.
</p>
</div>
</div>

<div id="outline-container-org8316ba8" class="outline-2">
<h2 id="org8316ba8">Setup</h2>
<div class="outline-text-2" id="text-org8316ba8">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> functools <span class="org-keyword">as</span> ft
<span class="org-keyword">import</span> gc
<span class="org-keyword">import</span> gzip
<span class="org-keyword">import</span> multiprocessing <span class="org-keyword">as</span> mp
<span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> pandas <span class="org-keyword">as</span> pd
<span class="org-keyword">import</span> scipy.io
<span class="org-keyword">import</span> scipy.stats <span class="org-keyword">as</span> st
<span class="org-keyword">import</span> scipy.special <span class="org-keyword">as</span> sp
<span class="org-keyword">import</span> sklearn.model_selection <span class="org-keyword">as</span> skms

<span class="org-keyword">import</span> rpy2.robjects.packages
<span class="org-keyword">import</span> rpy2.robjects.pandas2ri
<span class="org-keyword">import</span> rpy2.robjects.numpy2ri

rpy2.robjects.pandas2ri.activate()
rpy2.robjects.numpy2ri.activate()

<span class="org-variable-name">ashr</span> = rpy2.robjects.packages.importr(<span class="org-string">'ashr'</span>)
<span class="org-variable-name">descend</span> = rpy2.robjects.packages.importr(<span class="org-string">'descend'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">%matplotlib inline
%config <span class="org-variable-name">InlineBackend.figure_formats</span> = <span class="org-builtin">set</span>([<span class="org-string">'retina'</span>])
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> colorcet
<span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'figure.facecolor'</span>] = <span class="org-string">'w'</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-orga35b51b" class="outline-2">
<h2 id="orga35b51b">Methods</h2>
<div class="outline-text-2" id="text-orga35b51b">
</div>
<div id="outline-container-orga21875c" class="outline-3">
<h3 id="orga21875c">Distribution deconvolution</h3>
<div class="outline-text-3" id="text-orga21875c">
<p>
The general form of distribution deconvolution for scRNA-seq is:
</p>

<p>
\[ x_{ij} \sim \mathrm{Poisson}(\exp(\mathbf{z}_i' \mathbf{b}_j) \lambda_{ij}) \]
</p>

<p>
\[ \lambda_{ij} \sim g_j(\cdot) \]
</p>

<p>
where:
</p>

<ul class="org-ul">
<li>\(x_{ij}\) is the count of molecules of gene \(j\) in cell \(i\)</li>
<li>\(\mathbf{z}_i\) is a \(q\)-vector of covariates for cell \(i\)</li>
<li>\(\mathbf{b}_j\) is a \(q\)-vector of confounding effects on gene \(j\)</li>
<li>\(\lambda_{ij}\) is proportional to the relative abundance of gene \(j\)
in cell \(i\)</li>
</ul>

<p>
The primary inference goal is to recover \(g_j\). A secondary goal could be
to recover \(\lambda_{ij}\). We can trade off flexibility and complexity of
\(g_j\) for ease of implementation and speed.
</p>

<ol class="org-ol">
<li><b>Point mass:</b> \(g_j = \delta_\mu\). We mention it for completeness.</li>
<li><b>Gamma:</b> \(g_j = \mathrm{Gamma}(\cdot)\). This leads to the negative
binomial marginal likelihood, and can be motivated by the empirical
observation that the counts are overdispersed.</li>
<li><b>Point-Gamma:</b> \(g_j = \pi_j \delta_0(\cdot) + (1 - \pi_j)
      \mathrm{Gamma}(\cdot)\). This leads to the zero-inflated negative
binomial marginal likelihood, which is still analytic and therefore
computationally favorable. The inclusion of the point mass can be
motivated by theory suggesting a biological mechanism for bimodal gene
expression (<a href="http://dx.doi.org/10.1126/science.1216379">Munsky et
al. 2013</a>, <a href="http://dx.doi.org/10.1186/gb-2013-14-1-r7">Kim and Marioni
2013</a>).</li>
<li><b>Unimodal mixture of uniforms:</b> \(g_j = \pi_0 \delta_0(\cdot) + \sum_k
      \pi_k \mathrm{Uniform}(\cdot; \lambda_0, a_{jk})\), where \(\lambda_0\)
is the mode (<a href="http://dx.doi.org/10.1093/biostatistics/kxw041">Stephens
2016</a>). The inclusion of the point mass technically makes this
distribution bimodal.</li>
<li><b>Exponential family:</b> \(g_j = \exp(\mathbf{Q}\alpha - \phi(\alpha))\),
where \(\mathbf{Q}\) is a <a href="https://en.wikipedia.org/wiki/B-spline">B
spline spline basis matrix</a> for a
<a href="https://en.wikipedia.org/wiki/Cubic_Hermite_spline">natural cubic
spline</a>
(<a href="https://www.rdocumentation.org/packages/splines/topics/ns"><code>ns</code>
function</a>;
<a href="https://academic.oup.com/biomet/article/103/1/1/2390141">Efron
2016</a>). The key idea of the method is use spline regression to find the
sufficient statistic and natural parameters which maximizes the penalized
likelihood of the observed data. The method has been extended to include
a point mass on zero (<a href="http://dx.doi.org/10.1073/pnas.1721085115">Wang
et al. 2018</a>).</li>
<li><b>Nonparametric:</b> \(g_j = \sum_k \pi_k \mathrm{Uniform}(\cdot; ak, a(k +
      1))\), where \(a\) is a fixed step size
(<a href="https://projecteuclid.org/euclid.aoms/1177728066">Kiefer and Wolfowitz
1956</a>,
<a href="https://amstat.tandfonline.com/doi/abs/10.1080/01621459.2013.869224">Koenker
and Mizera 2014</a>).</li>
</ol>

<p>
In order to evaluate methods on their ability to estimate \(g_j\), we hold
out a validation set, and compute the validation set log likelihood. To set
a common baseline, we compare against the saturated model \(\hat\lambda_{ij}
   = x_{ij}\).
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="orgf2fd497"><span class="org-keyword">def</span> <span class="org-function-name">nb_llik</span>(x, mean, inv_disp):
  <span class="org-keyword">return</span> (x * np.log(mean / inv_disp) -
          x * np.log(1 + mean / inv_disp) -
          inv_disp * np.log(1 + mean / inv_disp) +
          sp.gammaln(x + inv_disp) -
          sp.gammaln(inv_disp) -
          sp.gammaln(x + 1))

<span class="org-keyword">def</span> <span class="org-function-name">score_nb</span>(x_train, x_test, **kwargs):
  <span class="org-keyword">import</span> scqtl
  <span class="org-variable-name">onehot</span> = np.ones((x_train.shape[0], 1))
  <span class="org-variable-name">size_factor</span> = x_train.<span class="org-builtin">sum</span>(axis=1).reshape(-1, 1)
  <span class="org-variable-name">design</span> = np.zeros((x_train.shape[0], 1))
  log_mu, log_phi, *<span class="org-variable-name">_</span> = scqtl.tf.fit(
    umi=x_train.astype(np.float32),
    onehot=onehot.astype(np.float32),
    design=design.astype(np.float32),
    size_factor=size_factor.astype(np.float32),
    learning_rate=1e-3,
    max_epochs=30000)
  <span class="org-keyword">return</span> nb_llik(x_test, x_test.<span class="org-builtin">sum</span>(axis=1, keepdims=<span class="org-constant">True</span>) * np.exp(log_mu), np.exp(-log_phi)).<span class="org-builtin">sum</span>(axis=0)

<span class="org-keyword">def</span> <span class="org-function-name">softplus</span>(x):
  <span class="org-keyword">return</span> np.where(x &gt; 30, x, np.log(1 + np.exp(x)))

<span class="org-keyword">def</span> <span class="org-function-name">zinb_llik</span>(x, mean, inv_disp, logodds):
  <span class="org-variable-name">case_zero</span> = -softplus(-logodds) + softplus(nb_llik(x, mean, inv_disp) - logodds)
  <span class="org-variable-name">case_non_zero</span> = -softplus(logodds) + nb_llik(x, mean, inv_disp)
  <span class="org-keyword">return</span> np.where(x &lt; 1, case_zero, case_non_zero)

<span class="org-keyword">def</span> <span class="org-function-name">score_zinb</span>(x_train, x_test, **kwargs):
  <span class="org-keyword">import</span> scqtl
  <span class="org-variable-name">onehot</span> = np.ones((x_train.shape[0], 1))
  <span class="org-variable-name">size_factor</span> = x_train.<span class="org-builtin">sum</span>(axis=1).reshape(-1, 1)
  <span class="org-variable-name">init</span> = scqtl.tf.fit(
    umi=x_train.astype(np.float32),
    onehot=onehot.astype(np.float32),
    size_factor=size_factor.astype(np.float32),
    learning_rate=1e-3,
    max_epochs=30000)
  log_mu, log_phi, logodds, *<span class="org-variable-name">_</span> = scqtl.tf.fit(
    umi=x_train.astype(np.float32),
    onehot=onehot.astype(np.float32),
    size_factor=size_factor.astype(np.float32),
    learning_rate=1e-3,
    max_epochs=30000,
    warm_start=init[:3])
  <span class="org-keyword">return</span> zinb_llik(x_test, x_test.<span class="org-builtin">sum</span>(axis=1, keepdims=<span class="org-constant">True</span>) * np.exp(log_mu), np.exp(-log_phi), logodds).<span class="org-builtin">sum</span>(axis=0)

<span class="org-keyword">def</span> <span class="org-function-name">_score_unimix</span>(train, test, train_size_factor, test_size_factor):
  <span class="org-variable-name">lam</span> = train / train_size_factor
  <span class="org-keyword">try</span>:
    <span class="org-variable-name">res0</span> = ashr.ash_workhorse(
      <span class="org-comment-delimiter"># </span><span class="org-comment">these are ignored by ash</span>
      pd.Series(np.zeros(train.shape)),
      1,
      <span class="org-comment-delimiter"># </span><span class="org-comment">numpy2ri doesn't DTRT, so we need to use pandas</span>
      lik=ashr.lik_pois(y=pd.Series(train), scale=train_size_factor, link=<span class="org-string">'identity'</span>),
      mode=pd.Series([lam.<span class="org-builtin">min</span>(), lam.<span class="org-builtin">max</span>()]))
    <span class="org-variable-name">res</span> = ashr.ash_workhorse(
      pd.Series(np.zeros(test.shape)),
      1,
      lik=ashr.lik_pois(y=pd.Series(test), scale=test_size_factor, link=<span class="org-string">'identity'</span>),
      fixg=<span class="org-constant">True</span>,
      g=res0.rx2(<span class="org-string">'fitted_g'</span>))
    <span class="org-variable-name">ret</span> = np.array(res.rx2(<span class="org-string">'loglik'</span>))
  <span class="org-keyword">except</span>:
    <span class="org-variable-name">ret</span> = -np.inf
  <span class="org-keyword">return</span> ret

<span class="org-keyword">def</span> <span class="org-function-name">score_unimix</span>(x_train, x_test, pool, **kwargs):
  <span class="org-variable-name">result</span> = []
  <span class="org-variable-name">train_size_factor</span> = pd.Series(x_train.<span class="org-builtin">sum</span>(axis=1))
  <span class="org-variable-name">test_size_factor</span> = pd.Series(x_test.<span class="org-builtin">sum</span>(axis=1))
  <span class="org-variable-name">f</span> = ft.partial(_score_unimix, train_size_factor=train_size_factor,
                 test_size_factor=test_size_factor)
  <span class="org-comment-delimiter"># </span><span class="org-comment">np iterates over rows</span>
  <span class="org-variable-name">result</span> = pool.starmap(f, <span class="org-builtin">zip</span>(x_train.T, x_test.T))
  <span class="org-keyword">return</span> np.array(result).ravel()

<span class="org-keyword">def</span> <span class="org-function-name">_score_descend</span>(train, test, train_size_factor, test_size_factor):
  <span class="org-variable-name">res</span> = descend.deconvSingle(pd.Series(train), scaling_consts=train_size_factor, verbose=<span class="org-constant">False</span>)
  <span class="org-comment-delimiter"># </span><span class="org-comment">DESCEND returns NA on errors</span>
  <span class="org-keyword">if</span> <span class="org-builtin">tuple</span>(res.rclass) != (<span class="org-string">'DESCEND'</span>,):
    <span class="org-keyword">return</span> -np.inf
  <span class="org-variable-name">g</span> = np.array(res.slots[<span class="org-string">'distribution'</span>])[:,:2]
  <span class="org-comment-delimiter"># </span><span class="org-comment">Don't marginalize over lambda = 0 for x &gt; 0, because p(x &gt; 0 | lambda =</span>
  <span class="org-comment-delimiter"># </span><span class="org-comment">0) = 0</span>
  <span class="org-variable-name">case_nonzero</span> = (st.poisson(mu=test_size_factor * g[1:,0])
                  .logpmf(test.reshape(-1, 1))
                  .dot(g[1:,1]))
  <span class="org-variable-name">case_zero</span> = (st.poisson(mu=test_size_factor * g[:,0])
               .logpmf(test.reshape(-1, 1))
               .dot(g[:,1]))
  <span class="org-variable-name">llik</span> = np.where(test &gt; 0, case_nonzero, case_zero).<span class="org-builtin">sum</span>()
  <span class="org-keyword">return</span> llik

<span class="org-keyword">def</span> <span class="org-function-name">score_descend</span>(x_train, x_test, pool, **kwargs):
  <span class="org-variable-name">result</span> = []
  <span class="org-comment-delimiter"># </span><span class="org-comment">numpy2ri doesn't DTRT, so we need to use pandas</span>
  <span class="org-variable-name">train_size_factor</span> = pd.Series(x_train.<span class="org-builtin">sum</span>(axis=1))
  <span class="org-variable-name">test_size_factor</span> = x_test.<span class="org-builtin">sum</span>(axis=1).reshape(-1, 1)
  <span class="org-variable-name">f</span> = ft.partial(_score_descend, train_size_factor=train_size_factor,
                 test_size_factor=test_size_factor)
  <span class="org-variable-name">result</span> = pool.starmap(f, <span class="org-builtin">zip</span>(x_train.T, x_test.T))
  <span class="org-keyword">return</span> np.array(result).ravel()

<span class="org-keyword">def</span> <span class="org-function-name">_score_npmle</span>(train, test, train_size_factor, test_size_factor, K):
  <span class="org-variable-name">lam</span> = train / train_size_factor
  <span class="org-variable-name">grid</span> = np.linspace(lam.<span class="org-builtin">min</span>(), 2 * lam.<span class="org-builtin">max</span>(), K + 1)
  <span class="org-keyword">try</span>:
    <span class="org-variable-name">res0</span> = ashr.ash_workhorse(
      <span class="org-comment-delimiter"># </span><span class="org-comment">these are ignored by ash</span>
      pd.Series(np.zeros(train.shape)),
      1,
      <span class="org-comment-delimiter"># </span><span class="org-comment">numpy2ri doesn't DTRT, so we need to use pandas</span>
      lik=ashr.lik_pois(y=pd.Series(train), scale=train_size_factor, link=<span class="org-string">'identity'</span>),
      g=ashr.unimix(pd.Series(np.ones(K) / K), pd.Series(grid[:-1]), pd.Series(grid[1:])))
    <span class="org-variable-name">res</span> = ashr.ash_workhorse(
      pd.Series(np.zeros(test.shape)),
      1,
      lik=ashr.lik_pois(y=pd.Series(test), scale=test_size_factor, link=<span class="org-string">'identity'</span>),
      fixg=<span class="org-constant">True</span>,
      g=res0.rx2(<span class="org-string">'fitted_g'</span>))
    <span class="org-variable-name">ret</span> = res.rx2(<span class="org-string">'loglik'</span>)
  <span class="org-keyword">except</span>:
    <span class="org-variable-name">ret</span> = -np.inf
  <span class="org-keyword">return</span> ret

<span class="org-keyword">def</span> <span class="org-function-name">score_npmle</span>(x_train, x_test, pool, K=100, **kwargs):
  <span class="org-variable-name">result</span> = []
  <span class="org-variable-name">train_size_factor</span> = pd.Series(x_train.<span class="org-builtin">sum</span>(axis=1))
  <span class="org-variable-name">test_size_factor</span> = pd.Series(x_test.<span class="org-builtin">sum</span>(axis=1))
  <span class="org-variable-name">f</span> = ft.partial(_score_npmle, train_size_factor=train_size_factor,
                 test_size_factor=test_size_factor, K=K)
  <span class="org-variable-name">result</span> = pool.starmap(f, <span class="org-builtin">zip</span>(x_train.T, x_test.T))
  <span class="org-keyword">return</span> np.array(result).ravel()

<span class="org-keyword">def</span> <span class="org-function-name">score_saturated</span>(x_train, x_test, **kwargs):
  <span class="org-keyword">return</span> st.poisson(mu=x_test).logpmf(x_test).<span class="org-builtin">sum</span>(axis=0)

<span class="org-keyword">def</span> <span class="org-function-name">evaluate_generalization</span>(x, pool, methods=<span class="org-constant">None</span>, **kwargs):
  <span class="org-variable-name">result</span> = {}
  <span class="org-variable-name">train</span>, <span class="org-variable-name">val</span> = skms.train_test_split(x, **kwargs)
  <span class="org-keyword">if</span> methods <span class="org-keyword">is</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">methods</span> = [<span class="org-string">'nb'</span>, <span class="org-string">'zinb'</span>, <span class="org-string">'unimix'</span>, <span class="org-string">'descend'</span>, <span class="org-string">'npmle'</span>, <span class="org-string">'saturated'</span>]
  <span class="org-keyword">for</span> m <span class="org-keyword">in</span> methods:
    <span class="org-comment-delimiter"># </span><span class="org-comment">Hack: get functions by name</span>
    <span class="org-variable-name">result</span>[m] = <span class="org-builtin">globals</span>()[f<span class="org-string">'score_{m}'</span>](train, val, pool=pool)
  <span class="org-keyword">return</span> pd.DataFrame.from_dict(result, orient=<span class="org-string">'columns'</span>)
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org030a59b" class="outline-2">
<h2 id="org030a59b">Results</h2>
<div class="outline-text-2" id="text-org030a59b">
</div>
<div id="outline-container-org6ebc2bf" class="outline-3">
<h3 id="org6ebc2bf">Homogeneous cell populations</h3>
<div class="outline-text-3" id="text-org6ebc2bf">
</div>
<div id="outline-container-org0366370" class="outline-4">
<h4 id="examples"><a id="org0366370"></a>Examples</h4>
<div class="outline-text-4" id="text-examples">
<p>
Use sorted cells from <a href="https://www.nature.com/articles/ncomms14049">Zheng
et al. 2017</a>.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="orgd873068"><span class="org-keyword">def</span> <span class="org-function-name">read_10x</span>(prefix, min_detect=0.25, return_df=<span class="org-constant">False</span>):
  <span class="org-variable-name">counts</span> = scipy.io.mmread(f<span class="org-string">'{prefix}/matrix.mtx.gz'</span>).tocsr()
  <span class="org-variable-name">keep</span> = ((counts &gt; 0).mean(axis=1) &gt;= min_detect).A.ravel()
  <span class="org-variable-name">counts</span> = counts[keep].T.A.astype(np.<span class="org-builtin">int</span>)
  <span class="org-keyword">if</span> return_df:
    <span class="org-variable-name">genes</span> = pd.read_csv(f<span class="org-string">'{prefix}/genes.tsv.gz'</span>, sep=<span class="org-string">'\t'</span>, header=<span class="org-constant">None</span>)
    <span class="org-keyword">return</span> pd.DataFrame(counts, columns=genes.loc[keep, 0])
  <span class="org-keyword">else</span>:
    <span class="org-keyword">return</span> counts

<span class="org-keyword">def</span> <span class="org-function-name">cd8_cytotoxic_t_cells</span>(**kwargs):
  <span class="org-keyword">return</span> read_10x(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/cytotoxic_t/filtered_matrices_mex/hg19'</span>, **kwargs)

<span class="org-keyword">def</span> <span class="org-function-name">cd19_b_cells</span>(**kwargs):
  <span class="org-keyword">return</span> read_10x(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/b_cells/filtered_matrices_mex/hg19/'</span>, **kwargs)
</pre>
</div>

<p>
Look at some examples.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">x</span> = cd8_cytotoxic_t_cells()
<span class="org-variable-name">xj</span> = pd.Series(x[:,x.mean(axis=0).argmax()])
<span class="org-variable-name">size_factor</span> = pd.Series(x.<span class="org-builtin">sum</span>(axis=1))
<span class="org-variable-name">lam</span> = xj / size_factor
</pre>
</div>

<p>
Fit Poisson ash.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">unimix_res</span> = ashr.ash_workhorse(
  pd.Series(np.zeros(x.shape[0])),
  1,
  lik=ashr.lik_pois(y=xj, scale=size_factor, link=<span class="org-string">'identity'</span>),
  mode=pd.Series([lam.<span class="org-builtin">min</span>(), lam.<span class="org-builtin">max</span>()]))
<span class="org-variable-name">unimix_cdf</span> = ashr.cdf_ash(unimix_res, np.linspace(lam.<span class="org-builtin">min</span>(), lam.<span class="org-builtin">max</span>(), 1000))
</pre>
</div>

<p>
Fit NPMLE.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">K</span> = 100
<span class="org-variable-name">grid</span> = np.linspace(lam.<span class="org-builtin">min</span>(), lam.<span class="org-builtin">max</span>(), K + 1)
<span class="org-variable-name">npmle_res</span> = ashr.ash_workhorse(
  pd.Series(np.zeros(x.shape[0])),
  1,
  lik=ashr.lik_pois(y=xj, scale=size_factor, link=<span class="org-string">'identity'</span>),
  g=ashr.unimix(pd.Series(np.ones(K) / K), pd.Series(grid[:-1]), pd.Series(grid[1:])))
<span class="org-variable-name">npmle_cdf</span> = ashr.cdf_ash(npmle_res, np.linspace(lam.<span class="org-builtin">min</span>(), lam.<span class="org-builtin">max</span>(), 1000))
</pre>
</div>

<p>
Fit DESCEND.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">descend_res</span> = descend.deconvSingle(xj, scaling_consts=size_factor, verbose=<span class="org-constant">False</span>)
</pre>
</div>

<p>
Fit NB/ZINB.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> pandas <span class="org-keyword">as</span> pd
<span class="org-keyword">import</span> scipy.io
<span class="org-keyword">import</span> scqtl
&lt;&lt;read-zheng&gt;&gt;
<span class="org-variable-name">x</span> = cd8_cytotoxic_t_cells()
<span class="org-variable-name">size_factor</span> = x.<span class="org-builtin">sum</span>(axis=1).reshape(-1, 1)
<span class="org-variable-name">onehot</span> = np.ones((x.shape[0], 1))
<span class="org-variable-name">design</span> = np.zeros((x.shape[0], 1))
<span class="org-variable-name">init</span> = scqtl.tf.fit(
  umi=x.astype(np.float32),
  onehot=onehot.astype(np.float32),
  design=design.astype(np.float32),
  size_factor=size_factor.astype(np.float32),
  learning_rate=1e-3,
  max_epochs=30000,
  verbose=<span class="org-constant">True</span>)
pd.DataFrame(init[0]).to_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-nb-log-mu.txt.gz'</span>, compression=<span class="org-string">'gzip'</span>, sep=<span class="org-string">'\t'</span>)
pd.DataFrame(init[1]).to_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-nb-log-phi.txt.gz'</span>, compression=<span class="org-string">'gzip'</span>, sep=<span class="org-string">'\t'</span>)
<span class="org-variable-name">log_mu</span>, <span class="org-variable-name">log_phi</span>, <span class="org-variable-name">logodds</span>, <span class="org-variable-name">nb_llik</span>, <span class="org-variable-name">zinb_llik</span> = scqtl.tf.fit(
  umi=x.astype(np.float32),
  onehot=onehot.astype(np.float32),
  design=design.astype(np.float32),
  size_factor=size_factor.astype(np.float32),
  learning_rate=1e-3,
  max_epochs=30000,
  warm_start=init[:3],
  verbose=<span class="org-constant">True</span>)
pd.DataFrame(log_mu).to_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-zinb-log-mu.txt.gz'</span>, compression=<span class="org-string">'gzip'</span>, sep=<span class="org-string">'\t'</span>)
pd.DataFrame(log_phi).to_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-zinb-log-phi.txt.gz'</span>, compression=<span class="org-string">'gzip'</span>, sep=<span class="org-string">'\t'</span>)
pd.DataFrame(logodds).to_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-zinb-logodds.txt.gz'</span>, compression=<span class="org-string">'gzip'</span>, sep=<span class="org-string">'\t'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-sh">sbatch --partition=gpu2 --gres=gpu:1 --mem=16G --time=60:00 --job-name=fit-nb
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/bash</span>
<span class="org-builtin">source</span> activate scmodes
python /project2/mstephens/aksarkar/projects/singlecell-modes/code/fit-nb.py
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">j</span> = <span class="org-builtin">str</span>(x.mean(axis=0).argmax())
<span class="org-variable-name">nb_log_mu</span> = pd.read_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-nb-log-mu.txt.gz'</span>, sep=<span class="org-string">'\t'</span>)
<span class="org-variable-name">nb_log_phi</span> = pd.read_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-nb-log-phi.txt.gz'</span>, sep=<span class="org-string">'\t'</span>)
<span class="org-comment-delimiter"># </span><span class="org-comment">Gamma (Use MATLAB and MATHEMATICA (b=theta=scale, a=alpha=shape) definition)</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">https://github.com/scipy/scipy/blob/v1.2.1/scipy/stats/_continuous_distns.py#L2479     </span>
<span class="org-variable-name">gamma_cdf</span> = st.gamma(a=np.exp(-nb_log_phi[j]), scale=np.exp(nb_log_mu[j] + nb_log_phi[j])).cdf(np.linspace(lam.<span class="org-builtin">min</span>(), lam.<span class="org-builtin">max</span>(), 1000))
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">zinb_log_mu</span> = pd.read_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-zinb-log-mu.txt.gz'</span>, sep=<span class="org-string">'\t'</span>)
<span class="org-variable-name">zinb_log_phi</span> = pd.read_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-zinb-log-phi.txt.gz'</span>, sep=<span class="org-string">'\t'</span>)
<span class="org-variable-name">zinb_logodds</span> = pd.read_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-modes/data/zheng-cd8-zinb-log-phi.txt.gz'</span>, sep=<span class="org-string">'\t'</span>)
<span class="org-variable-name">point_gamma_cdf</span> = st.gamma(a=np.exp(-zinb_log_phi[j]), scale=np.exp(zinb_log_mu[j] + zinb_log_phi[j])).cdf(np.linspace(lam.<span class="org-builtin">min</span>(), lam.<span class="org-builtin">max</span>(), 1000))
<span class="org-variable-name">point_gamma_cdf</span> *= sp.expit(-zinb_logodds[j].values)
<span class="org-variable-name">point_gamma_cdf</span> += sp.expit(zinb_logodds[j].values)
</pre>
</div>

<p>
Plot the observed counts and deconvolved distributions.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>).colors
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(2, 1)
ax[0].hist(xj, bins=100, color=<span class="org-string">'k'</span>)
ax[0].set_xlabel(<span class="org-string">'Molecule count'</span>)
ax[0].set_ylabel(<span class="org-string">'Number of cells'</span>)

ax[1].plot(np.linspace(lam.<span class="org-builtin">min</span>(), lam.<span class="org-builtin">max</span>(), 1000), gamma_cdf, color=cm[0], lw=1, label=<span class="org-string">'Gamma'</span>)
ax[1].plot(np.linspace(lam.<span class="org-builtin">min</span>(), lam.<span class="org-builtin">max</span>(), 1000), point_gamma_cdf, color=cm[1], lw=1, label=<span class="org-string">'Point-Gamma'</span>)
ax[1].plot(np.array(unimix_cdf.rx2(<span class="org-string">'x'</span>)),
           np.array(unimix_cdf.rx2(<span class="org-string">'y'</span>)).ravel(), c=cm[2], lw=1, label=<span class="org-string">'Unif mix'</span>)
<span class="org-variable-name">F</span> = np.cumsum(np.array(descend_res.slots[<span class="org-string">'density.points'</span>])[:,1])
ax[1].plot(np.array(descend_res.slots[<span class="org-string">'density.points'</span>])[:,0],
           F / F.<span class="org-builtin">max</span>(), c=cm[3], lw=1, label=<span class="org-string">'DESCEND'</span>)
ax[1].plot(np.array(npmle_cdf.rx2(<span class="org-string">'x'</span>)),
           np.array(npmle_cdf.rx2(<span class="org-string">'y'</span>)).ravel(), c=cm[4], lw=1, label=<span class="org-string">'NPMLE'</span>)
ax[1].set_xlabel(<span class="org-string">'Latent gene expression'</span>)
ax[1].set_ylabel(<span class="org-string">'CDF'</span>)

ax[1].legend(frameon=<span class="org-constant">False</span>)

fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/deconvolution.org/ash-example.png" alt="ash-example.png">
</p>
</div>
</div>
</div>

<div id="outline-container-org1e18387" class="outline-4">
<h4 id="mode-estimation"><a id="org1e18387"></a>Speed up ash mode estimation</h4>
<div class="outline-text-4" id="text-mode-estimation">
<p>
To estimate the mode \(\lambda_{0j}\) for gene \(j\), we find:
</p>

<p>
\[ \lambda_{0j}^* = \arg\max_{\lambda_{0j}} \sum_i \int f(x_i \mid \lambda_i) g_j(\lambda_i \mid \pi, \lambda_{0j})\ d\lambda_i \]
</p>

<p>
using
<a href="https://www.rdocumentation.org/packages/stats/versions/3.5.3/topics/optimize">golden
section search</a>. Investigate two issues:
</p>

<p>
<b>First,</b> Mengyin Liu claimed this problem is convex in \(\lambda_{0j}\),
However, on the above example, the quality of the result depends on the
bounds of the search. Is this problem actually convex?
</p>

<p>
By default, the bounds are \([\min(x_i), \max(x_i)]\), which can be
extremely large. However, we need to remove the scaling factor, so should
we instead search over \([\min(x_i / R_i), \max(x_i / R_i)]\)? The motivation for the
proposed alternative is to only look over plausible values of
\(\lambda_i\).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">grid</span> = np.geomspace(1e-3, xj.<span class="org-builtin">max</span>(), 100)
<span class="org-variable-name">llik</span> = np.array([np.array(
  ashr.ash(
    pd.Series(np.zeros(xj.shape)),
    1,
    lik=ashr.lik_pois(y=xj, scale=size_factor, link=<span class="org-string">'identity'</span>),
    mode=lam0,
    outputlevel=<span class="org-string">'loglik'</span>).rx2(<span class="org-string">'loglik'</span>)) <span class="org-keyword">for</span> lam0 <span class="org-keyword">in</span> grid]).ravel()
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">res0</span> = ashr.ash(
  pd.Series(np.zeros(xj.shape)),
  1,
  lik=ashr.lik_pois(y=xj, scale=size_factor, link=<span class="org-string">'identity'</span>),
  mode=<span class="org-string">'estimate'</span>)
<span class="org-variable-name">res1</span> = ashr.ash(
  pd.Series(np.zeros(x.shape[0])),
  1,
  lik=ashr.lik_pois(y=xj, scale=size_factor, link=<span class="org-string">'identity'</span>),
  mode=pd.Series([lam.<span class="org-builtin">min</span>(), lam.<span class="org-builtin">max</span>()]))
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
plt.gcf().set_size_inches(3, 3)
plt.xscale(<span class="org-string">'log'</span>)
plt.plot(grid, np.array(llik).ravel(), lw=1, c=<span class="org-string">'k'</span>)
plt.axvline(x=np.array(res0.rx2(<span class="org-string">'fitted_g'</span>).rx2(<span class="org-string">'a'</span>))[0], c=<span class="org-string">'k'</span>, lw=1, ls=<span class="org-string">':'</span>, label=<span class="org-string">'Default'</span>)
plt.axvline(x=np.array(res1.rx2(<span class="org-string">'fitted_g'</span>).rx2(<span class="org-string">'a'</span>))[0], c=<span class="org-string">'r'</span>, lw=1, ls=<span class="org-string">':'</span>, label=<span class="org-string">'Restricted'</span>)
plt.legend(frameon=<span class="org-constant">False</span>, loc=<span class="org-string">'center left'</span>, bbox_to_anchor=(1, .5))
plt.xlabel(<span class="org-string">'Mode $\lambda_0$'</span>)
<span class="org-variable-name">_</span> = plt.ylabel(<span class="org-string">'Marginal likelihood'</span>)
</pre>
</div>


<div class="figure">
<p><img src="figure/deconvolution.org/mode-est.png" alt="mode-est.png">
</p>
</div>

<p>
It appears the problem is actually non-convex. Surprisingly, it appears
non-convex even for a case where the data are not bimodal. For bimodal
data, we might expect that choice of the mode would change the weight
on/near zero and result in a non-convex objective.
</p>

<p>
According to the documentation, the search can fail for poor choice of
initial query, which depends entirely on the initial interval. In this
example, the initial interval does not contain the mode, and therefore the
search finds the correct local optimum within the interval, but fails to
find the global optimum.
</p>

<p>
This result does not necessarily mean that our proposed alternative, to
search over \([\min(x_i / R_i), \max(x_i / R_i)]\) will work, because
Poisson noise could mean the true \(\lambda_i > x_i / R_i\) for some sample
\(i\). 
</p>

<p>
Should we search further to be reasonably certain we haven't missed the
mode? Intuitively, the largest \(\hat\lambda_i\) value we do observe should
be "overestimated"; if it were not, then we should expect higher density of
\(g\) around it, and values larger than it in the observed data.
</p>

<p>
<b>Second,</b> we have to solve an <code>ash</code> subproblem for each query
\(\lambda_0\), which becomes extremely expensive for large data sets. We
can speed up the procedure by downsampling the data for mode
estimation. How much worse is the fitted model?
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">score_mode_estimation</span>(data, seed=0, p=0.1):
  <span class="org-variable-name">temp</span> = data.sample(random_state=seed, frac=p)
  <span class="org-variable-name">res0</span> = ashr.ash(
    pd.Series(np.zeros(temp.shape[0])),
    1,
    lik=ashr.lik_pois(y=temp[<span class="org-string">'x'</span>], scale=temp[<span class="org-string">'scale'</span>], link=<span class="org-string">'identity'</span>),
    mode=pd.Series([temp[<span class="org-string">'lam'</span>].<span class="org-builtin">min</span>(), temp[<span class="org-string">'lam'</span>].<span class="org-builtin">max</span>()]))
  <span class="org-variable-name">lam0</span> = np.array(res0.rx2(<span class="org-string">'fitted_g'</span>).rx2(<span class="org-string">'a'</span>))[0]
  <span class="org-variable-name">res</span> = ashr.ash(
    pd.Series(np.zeros(data.shape[0])),
    1,
    lik=ashr.lik_pois(y=data[<span class="org-string">'x'</span>], scale=data[<span class="org-string">'scale'</span>], link=<span class="org-string">'identity'</span>),
    mode=lam0)
  <span class="org-keyword">return</span> lam0, np.array(res.rx2(<span class="org-string">'loglik'</span>))[0]

<span class="org-keyword">def</span> <span class="org-function-name">evaluate_mode_estimation</span>(data, num_trials):
  <span class="org-variable-name">result</span> = []
  <span class="org-keyword">for</span> p <span class="org-keyword">in</span> (0.1, 0.25, 0.5):
    <span class="org-keyword">for</span> trial <span class="org-keyword">in</span> <span class="org-builtin">range</span>(num_trials):
      <span class="org-variable-name">lam0</span>, <span class="org-variable-name">llik</span> = score_mode_estimation(data, seed=trial, p=p)
      result.append([p, trial, lam0, llik])
  <span class="org-variable-name">result</span> = pd.DataFrame(result, columns=[<span class="org-string">'p'</span>, <span class="org-string">'trial'</span>, <span class="org-string">'lam0'</span>, <span class="org-string">'llik'</span>])
  <span class="org-keyword">return</span> result
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">mode_estimation_result</span> = evaluate_mode_estimation(pd.DataFrame({<span class="org-string">'x'</span>: xj, <span class="org-string">'scale'</span>: size_factor, <span class="org-string">'lam'</span>: lam}), num_trials=10)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
plt.gcf().set_size_inches(3, 3)
plt.scatter(mode_estimation_result[<span class="org-string">'p'</span>], mode_estimation_result[<span class="org-string">'llik'</span>], s=4, c=<span class="org-string">'k'</span>)
plt.axhline(y=np.array(res1.rx2(<span class="org-string">'loglik'</span>))[0], c=<span class="org-string">'k'</span>, lw=1, ls=<span class="org-string">':'</span>)
plt.xlabel(<span class="org-string">'Fraction of original data'</span>)
plt.ylabel(<span class="org-string">'Training log likelihood'</span>)
</pre>
</div>

<pre class="example">
Text(0, 0.5, 'Training log likelihood')

</pre>

<div class="figure">
<p><img src="figure/deconvolution.org/downsampling-mode-estimation.png" alt="downsampling-mode-estimation.png">
</p>
</div>

<p>
Downsampling is likely to result in a <b>much</b> worse model fit, so we should
not pursue that strategy to speed up the model estimation.
</p>
</div>
</div>

<div id="outline-container-org2ccef52" class="outline-4">
<h4 id="homogeneous-results"><a id="org2ccef52"></a>Run the benchmark</h4>
<div class="outline-text-4" id="text-homogeneous-results">
<p>
Use iPSCs derived from Yoruba LCLs
(<a href="https://www.biorxiv.org/content/early/2018/09/23/424192">Sarkar et
al. 2018</a>). The data were sequenced on the Fluidigm C1 platform to much
greater depth, allowing many more genes to be detected (especially lower
expressed genes), potentially including genes with bimodal steady state gene
expression. Because many more genes are detected, we need to randomly
subsample genes for CPU-based methods to complete in a reasonable budget.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org4a81530"><span class="org-keyword">def</span> <span class="org-function-name">ipsc</span>(prefix=<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data'</span>, return_df=<span class="org-constant">False</span>, query=<span class="org-constant">None</span>, n=<span class="org-constant">None</span>, seed=0):
  <span class="org-variable-name">annotations</span> = pd.read_csv(f<span class="org-string">'{prefix}/scqtl-annotation.txt'</span>, sep=<span class="org-string">'\t'</span>)
  <span class="org-variable-name">keep_samples</span> = pd.read_csv(f<span class="org-string">'{prefix}/quality-single-cells.txt'</span>, sep=<span class="org-string">'\t'</span>, index_col=0, header=<span class="org-constant">None</span>)
  <span class="org-variable-name">keep_genes</span> = pd.read_csv(f<span class="org-string">'{prefix}/genes-pass-filter.txt'</span>, sep=<span class="org-string">'\t'</span>, index_col=0, header=<span class="org-constant">None</span>)
  <span class="org-keyword">if</span> query <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">keep_genes</span> = keep_genes[keep_genes.index.isin(query)]
  <span class="org-keyword">if</span> n <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">keep_genes</span> = keep_genes.sample(n=n, random_state=seed)
  <span class="org-variable-name">annotations</span> = annotations.loc[keep_samples.values.ravel()]
  <span class="org-variable-name">result</span> = []
  <span class="org-keyword">for</span> chunk <span class="org-keyword">in</span> pd.read_csv(f<span class="org-string">'{prefix}/scqtl-counts.txt.gz'</span>, sep=<span class="org-string">'\t'</span>, index_col=0, chunksize=100):
     <span class="org-variable-name">x</span> = (chunk
          .loc[:,keep_samples.values.ravel()]
          .<span class="org-builtin">filter</span>(items=keep_genes[keep_genes.values.ravel()].index, axis=<span class="org-string">'index'</span>))
     <span class="org-keyword">if</span> <span class="org-keyword">not</span> x.empty:
       result.append(x)
  <span class="org-variable-name">result</span> = pd.concat(result)
  <span class="org-comment-delimiter"># </span><span class="org-comment">Return samples x genes</span>
  <span class="org-keyword">if</span> return_df:
    <span class="org-keyword">return</span> result.T
  <span class="org-keyword">else</span>:
    <span class="org-keyword">return</span> result.values.T
</pre>
</div>

<p>
Run the benchmark.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org9c255de"><span class="org-keyword">import</span> os
<span class="org-variable-name">cpu_methods</span> = [<span class="org-string">'unimix'</span>, <span class="org-string">'descend'</span>, <span class="org-string">'npmle'</span>, <span class="org-string">'saturated'</span>]
<span class="org-variable-name">task</span> = <span class="org-builtin">int</span>(os.environ[<span class="org-string">'SLURM_ARRAY_TASK_ID'</span>])
<span class="org-keyword">with</span> mp.Pool(maxtasksperchild=10) <span class="org-keyword">as</span> pool:
  <span class="org-variable-name">x</span> = ipsc(n=100, return_df=<span class="org-constant">True</span>)
  <span class="org-variable-name">res</span> = evaluate_generalization(x.values, pool=pool, test_size=0.1, random_state=0, methods=cpu_methods[task:task + 1])
  <span class="org-variable-name">res.index</span> = x.columns
  res.to_csv(f<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/ipsc-{cpu_methods[task]}.txt.gz'</span>, compression=<span class="org-string">'gzip'</span>, sep=<span class="org-string">'\t'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-sh">sbatch --partition=broadwl --mem=32G -a 0-3 -n1 -c28 --exclusive --time=6:00:00 --job-name=benchmark --output=benchmark-cpu.out
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/bash</span>
module load cuda/9.0
<span class="org-builtin">source</span> activate scmodes
python -i /project2/mstephens/aksarkar/projects/singlecell-modes/code/benchmark.py &lt;&lt;EOF
<span class="org-sh-heredoc">&lt;&lt;run-homogeneous-benchmark-cpu&gt;&gt;</span>
EOF
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython" id="orgacd8d98"><span class="org-variable-name">x</span> = ipsc(return_df=<span class="org-constant">True</span>)
<span class="org-variable-name">res</span> = evaluate_generalization(x.values, pool=<span class="org-constant">None</span>, test_size=0.1, random_state=0, methods=[<span class="org-string">'nb'</span>, <span class="org-string">'zinb'</span>])
<span class="org-variable-name">res.index</span> = x.columns
res.to_csv(f<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/ipsc-gpu.txt.gz'</span>, compression=<span class="org-string">'gzip'</span>, sep=<span class="org-string">'\t'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-sh">sbatch --partition=gpu2 --gres=gpu:1 --mem=16G --time=12:00:00 --job-name=benchmark --output=benchmark-gpu.out
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/bash</span>
module load cuda/9.0
<span class="org-builtin">source</span> activate scmodes
python -i /project2/mstephens/aksarkar/projects/singlecell-modes/code/benchmark.py &lt;&lt;EOF
<span class="org-sh-heredoc">&lt;&lt;run-homogeneous-benchmark-gpu&gt;&gt;</span>
EOF
</pre>
</div>

<p>
Read the results.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">benchmark</span> = {}
<span class="org-keyword">for</span> data <span class="org-keyword">in</span> (<span class="org-string">'cd8_cytotoxic_t_cells'</span>, <span class="org-string">'cd19_b_cells'</span>):
  <span class="org-variable-name">benchmark</span>[data] = pd.read_csv(f<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/{data}.txt.gz'</span>, sep=<span class="org-string">'\t'</span>, index_col=0)
<span class="org-variable-name">benchmark</span>[<span class="org-string">'ipsc'</span>] = (
  pd.read_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/ipsc-gpu.txt.gz'</span>, index_col=0, sep=<span class="org-string">'\t'</span>)
  .merge(pd.read_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/ipsc-unimix.txt.gz'</span>, index_col=0, sep=<span class="org-string">'\t'</span>), on=<span class="org-string">'gene'</span>)
  .merge(pd.read_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/ipsc-descend.txt.gz'</span>, sep=<span class="org-string">'\t'</span>), on=<span class="org-string">'gene'</span>)
  .merge(pd.read_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/ipsc-npmle.txt.gz'</span>, index_col=0, sep=<span class="org-string">'\t'</span>), on=<span class="org-string">'gene'</span>)
  .merge(pd.read_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/ipsc-saturated.txt.gz'</span>, index_col=0, sep=<span class="org-string">'\t'</span>), on=<span class="org-string">'gene'</span>)
  .set_index(<span class="org-string">'gene'</span>))
</pre>
</div>

<p>
Plot the generalization performance of each method.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 3)
fig.set_size_inches(7, 3)
<span class="org-keyword">for</span> i, k <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(<span class="org-builtin">sorted</span>(benchmark.keys())):
  ax[i].boxplot((benchmark[f<span class="org-string">'{k}'</span>].values - benchmark[f<span class="org-string">'{k}'</span>][<span class="org-string">'nb'</span>].values.reshape(-1, 1))[:,1:-1],
                widths=0.25, medianprops={<span class="org-string">'color'</span>: <span class="org-string">'k'</span>}, flierprops={<span class="org-string">'marker'</span>: <span class="org-string">'.'</span>, <span class="org-string">'markersize'</span>: 4})
  ax[i].set_xticklabels(benchmark[f<span class="org-string">'{k}'</span>].columns[1:-1], rotation=90)
  ax[i].set_xlabel(<span class="org-string">'Method'</span>)
  ax[i].set_title(k)
ax[0].set_ylabel(<span class="org-string">'Diff val set log lik from NB'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/deconvolution.org/homogeneous-benchmark.png" alt="homogeneous-benchmark.png">
</p>
</div>

<p>
The results suggest that NB (assuming Gamma-distributed latent gene
expression) is adequate to accurately estimate \(g\) from the data.
</p>

<p>
Of note, <code>ash</code> is &gt;10\(\times\) slower than <code>descend</code>, and <code>descend</code> is
faster on small data sets than GPU-based methods. 
</p>
</div>
</div>

<div id="outline-container-org17b0735" class="outline-4">
<h4 id="bimodal-example-ipsc"><a id="org17b0735"></a>Investigate ZINB vs. NB in high-depth data</h4>
<div class="outline-text-4" id="text-bimodal-example-ipsc">
<p>
Try to find a case in the full iPSC data where ZINB is required, by
performing likelihood ratio tests against NB. Use Bonferroni correction to
control FWER.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">ipsc_res</span> = pd.read_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/ipsc-gpu.txt.gz'</span>, index_col=0, sep=<span class="org-string">'\t'</span>)
<span class="org-variable-name">lrt</span> = st.chi2(1).sf(-2 * (ipsc_res[<span class="org-string">'nb'</span>] - ipsc_res[<span class="org-string">'zinb'</span>]))
ipsc_res[lrt &lt; .05 / lrt.shape[0]]
</pre>
</div>

<pre class="example">
nb         zinb
gene
ENSG00000112530 -2017.717750 -1987.894352
ENSG00000129824 -1803.651932 -1789.106410
</pre>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">ipsc_counts</span> = ipsc(query=<span class="org-builtin">list</span>(ipsc_res[lrt &lt; .05 / lrt.shape[0]].index), return_df=<span class="org-constant">True</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">gene_info</span> = pd.read_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-genes.txt.gz'</span>, sep=<span class="org-string">'\t'</span>, index_col=0)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2)
fig.set_size_inches(6, 3)
<span class="org-keyword">for</span> a, k <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(ax, ipsc_counts):
  a.hist(ipsc_counts[k], bins=ipsc_counts[k].<span class="org-builtin">max</span>() + 1, color=<span class="org-string">'k'</span>)
  a.set_title(gene_info.loc[k, <span class="org-string">'name'</span>])
  a.set_xlabel(<span class="org-string">'Number of molecules'</span>)
  a.set_ylabel(<span class="org-string">'Number of cells'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/deconvolution.org/ipsc-zinb.png" alt="ipsc-zinb.png">
</p>
</div>

<p>
<i>PACRG</i> (<a href="https://www.omim.org/entry/608427">Parkin coregulated gene</a>)
shares a promoter with <i>PKRG</i> (<i>PARK2</i>;
<a href="https://www.omim.org/entry/602544?search=park2&amp;highlight=park2">Parkin</a>). PARKIN
is involved in a <a href="https://www.pnas.org/content/115/2/E180">signaling
pathway for mitochondria damage</a>, and mitochrondrial damage appears to be
relevant to neurodegenerative disease etiology
(<a href="https://www.ncbi.nlm.nih.gov/pubmed/29414602">Alzheimer's disease</a> and
<a href="https://www.pnas.org/content/115/2/E180">Parkinson's disease</a>).
Mitochrondrial damage could explain the bimodal distribution of <i>PACRG</i> in
the iPSC data: the reprogramming protocol could induce oxidative stress on
some of the cells, resulting in damage.
</p>

<p>
<i>RPS4Y1</i> (<a href="https://www.omim.org/entry/470000">Ribosomal protein S4,
Y-linked</a>) is on the Y chromosome, and therefore we should expect it to
only be expressed in males. The homologous gene on the X chromosome
(<a href="https://www.omim.org/entry/312760"><i>RPS4X</i></a>) has a different coding
sequence. Sex-linkage should fully explain why this gene exhibits bimodal
gene expression.
</p>

<p>
Look at the full benchmark results for NB vs. ZINB (only the GPU
implementation is suitable for this).
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
plt.gcf().set_size_inches(3, 3)
plt.plot([ipsc_res[<span class="org-string">'nb'</span>].<span class="org-builtin">min</span>(), ipsc_res[<span class="org-string">'nb'</span>].<span class="org-builtin">max</span>()], [ipsc_res[<span class="org-string">'nb'</span>].<span class="org-builtin">min</span>(), ipsc_res[<span class="org-string">'nb'</span>].<span class="org-builtin">max</span>()], c=<span class="org-string">'r'</span>, ls=<span class="org-string">'--'</span>, lw=1)
plt.scatter(ipsc_res[<span class="org-string">'nb'</span>], ipsc_res[<span class="org-string">'zinb'</span>], c=<span class="org-string">'k'</span>, s=2, alpha=.25)
plt.xlabel(<span class="org-string">'NB validation log lik.'</span>)
plt.ylabel(<span class="org-string">'ZINB validation log lik.'</span>)
</pre>
</div>

<pre class="example">
Text(0, 0.5, 'ZINB validation log lik.')

</pre>

<div class="figure">
<p><img src="figure/deconvolution.org/homogeneous-nb-zinb-benchmark.png" alt="homogeneous-nb-zinb-benchmark.png">
</p>
</div>
</div>
</div>

<div id="outline-container-org2d2ec8d" class="outline-4">
<h4 id="org2d2ec8d">Investigate genes with support for more complex models</h4>
<div class="outline-text-4" id="text-org2d2ec8d">
<p>
In B cells, several genes are better fit by DESCEND than other methods.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">b_cells</span> = read_10x(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/b_cells/filtered_matrices_mex/hg19/'</span>, return_df=<span class="org-constant">True</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">query</span> = benchmark[<span class="org-string">'cd19_b_cells'</span>][benchmark[<span class="org-string">'cd19_b_cells'</span>][<span class="org-string">'descend'</span>] &gt; benchmark[<span class="org-string">'cd19_b_cells'</span>][<span class="org-string">'nb'</span>]].index
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(6, 6, sharey=<span class="org-constant">True</span>, sharex=<span class="org-constant">True</span>)
fig.set_size_inches(9, 9)
<span class="org-keyword">for</span> a, k <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(ax.ravel(), query):
  a.hist(b_cells.iloc[:,k], bins=b_cells.iloc[:,k].<span class="org-builtin">max</span>(), color=<span class="org-string">'k'</span>)
  a.set_title(gene_info.loc[b_cells.columns[k], <span class="org-string">'name'</span>] <span class="org-keyword">if</span> b_cells.columns[k] <span class="org-keyword">in</span> gene_info.index <span class="org-keyword">else</span> b_cells.columns[k])
<span class="org-keyword">for</span> y <span class="org-keyword">in</span> <span class="org-builtin">range</span>(ax.shape[0]):
  ax[y][0].set_ylabel(<span class="org-string">'Number of cells'</span>)
<span class="org-keyword">for</span> x <span class="org-keyword">in</span> <span class="org-builtin">range</span>(ax.shape[1]):
  ax[-1][x].set_xlabel(<span class="org-string">'Num mols'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/deconvolution.org/b-cell-descend-examples.png" alt="b-cell-descend-examples.png">
</p>
</div>

<p>
In iPSC, several genes appear to be better fit by mixture of uniforms than
other methods.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">ipsc_counts</span> = ipsc(n=100, return_df=<span class="org-constant">True</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">query</span> = benchmark[<span class="org-string">'ipsc'</span>][benchmark[<span class="org-string">'ipsc'</span>][<span class="org-string">'unimix'</span>] &gt; benchmark[<span class="org-string">'ipsc'</span>][<span class="org-string">'nb'</span>]].index
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(2, 3)
fig.set_size_inches(6, 4)
<span class="org-keyword">for</span> a, k <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(ax.ravel(), query):
  a.hist(ipsc_counts.loc[:,k], bins=ipsc_counts.loc[:,k].<span class="org-builtin">max</span>(), color=<span class="org-string">'k'</span>)
  a.set_title(gene_info.loc[k, <span class="org-string">'name'</span>])
<span class="org-keyword">for</span> y <span class="org-keyword">in</span> <span class="org-builtin">range</span>(ax.shape[0]):
  ax[y][0].set_ylabel(<span class="org-string">'Number of cells'</span>)
<span class="org-keyword">for</span> x <span class="org-keyword">in</span> <span class="org-builtin">range</span>(ax.shape[1]):
  ax[-1][x].set_xlabel(<span class="org-string">'Num mols'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/deconvolution.org/ipsc-unimix-examples.png" alt="ipsc-unimix-examples.png">
</p>
</div>

<p>
In iPSC, several genes appear to be better fit by NPMLE than other methods.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">llik_diff</span> = benchmark[<span class="org-string">'ipsc'</span>][<span class="org-string">'npmle'</span>] - benchmark[<span class="org-string">'ipsc'</span>][<span class="org-string">'nb'</span>]
<span class="org-variable-name">query</span> = benchmark[<span class="org-string">'ipsc'</span>].loc[llik_diff[llik_diff &gt; 0].sort_values(ascending=<span class="org-constant">False</span>).head(n=12).index].index
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(3, 4)
fig.set_size_inches(6, 4)
<span class="org-keyword">for</span> a, k <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(ax.ravel(), query):
  a.hist(ipsc_counts.loc[:,k], bins=ipsc_counts.loc[:,k].<span class="org-builtin">max</span>(), color=<span class="org-string">'k'</span>)
  a.set_title(gene_info.loc[k, <span class="org-string">'name'</span>])
<span class="org-keyword">for</span> y <span class="org-keyword">in</span> <span class="org-builtin">range</span>(ax.shape[0]):
  ax[y][0].set_ylabel(<span class="org-string">'Number of cells'</span>)
<span class="org-keyword">for</span> x <span class="org-keyword">in</span> <span class="org-builtin">range</span>(ax.shape[1]):
  ax[-1][x].set_xlabel(<span class="org-string">'Num mols'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/deconvolution.org/ipsc-npmle-examples.png" alt="ipsc-npmle-examples.png">
</p>
</div>

<p>
Overall, these examples suggest flexibility is needed for accurately
modeling the tails of the distribution.
</p>
</div>
</div>
</div>

<div id="outline-container-orgf65a0fe" class="outline-3">
<h3 id="orgf65a0fe">Synthetic cell mixtures</h3>
<div class="outline-text-3" id="text-orgf65a0fe">
</div>
<div id="outline-container-org118a8dd" class="outline-4">
<h4 id="org118a8dd">T cell-B cell mixture</h4>
<div class="outline-text-4" id="text-org118a8dd">
<p>
Create a synthetic heterogeneous population of cells by combining sorted
CD8+ T cells and CD19+ B cells from
<a href="https://www.nature.com/articles/ncomms14049">Zheng et al. 2017</a>.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">cd8_cd19_mix</span>():
  <span class="org-variable-name">cd8</span> = cd8_cytotoxic_t_cells(return_df=<span class="org-constant">True</span>)
  <span class="org-variable-name">cd19</span> = cd19_b_cells(return_df=<span class="org-constant">True</span>)
  <span class="org-variable-name">x</span> = pd.concat([cd8, cd19], axis=<span class="org-string">'index'</span>, join=<span class="org-string">'inner'</span>)
  <span class="org-variable-name">y</span> = np.zeros(x.shape[0]).astype(<span class="org-builtin">int</span>)
  y[:cd8.shape[0]] = 1
  <span class="org-keyword">return</span> x, y
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython" id="org8d144fb"><span class="org-variable-name">x</span>, <span class="org-variable-name">y</span> = cd8_cd19_mix()
<span class="org-variable-name">res</span> = evaluate_generalization(x.values, stratify=y, train_size=0.5, test_size=0.1, random_state=0)
res.to_csv(f<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/cd8-cd19-mix.txt.gz'</span>, compression=<span class="org-string">'gzip'</span>, sep=<span class="org-string">'\t'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-sh">sbatch --partition=gpu2 --gres=gpu:1 --mem=16G --time=6:00:00 --job-name=benchmark --output=benchmark.out
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/bash</span>
module load cuda/9.0
<span class="org-builtin">source</span> activate scmodes
python -i /project2/mstephens/aksarkar/projects/singlecell-modes/code/benchmark.py &lt;&lt;EOF
<span class="org-sh-heredoc">&lt;&lt;run-synthetic-mix-benchmark&gt;&gt;</span>
EOF
</pre>
</div>
</div>
</div>

<div id="outline-container-org42727dd" class="outline-4">
<h4 id="org42727dd">Naive/activated T cell mixture</h4>
<div class="outline-text-4" id="text-org42727dd">
<p>
Create a synthetic mixture of cells by combining sorted
CD8+ cytotoxic T cells and CD8+/CD45RA+ naive cytotoxic T cells
<a href="https://www.nature.com/articles/ncomms14049">Zheng et al. 2017</a>.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">cyto_naive_t_mix</span>():
  <span class="org-variable-name">cyto</span> = read_10x(prefix=<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/cytotoxic_t/filtered_matrices_mex/hg19'</span>, return_df=<span class="org-constant">True</span>)
  <span class="org-variable-name">naive</span> = read_10x(prefix=<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/naive_cytotoxic/filtered_matrices_mex/hg19'</span>, return_df=<span class="org-constant">True</span>)
  <span class="org-variable-name">x</span> = pd.concat([cyto, naive], axis=<span class="org-string">'index'</span>, join=<span class="org-string">'inner'</span>)
  <span class="org-variable-name">y</span> = np.zeros(x.shape[0]).astype(<span class="org-builtin">int</span>)
  y[:cyto.shape[0]] = 1
  <span class="org-keyword">return</span> x, y
</pre>
</div>

<div class="org-src-container">
<pre class="src src-sh">sbatch --partition=gpu2 --gres=gpu:1 --mem=16G --time=6:00:00 --job-name=benchmark --output=benchmark-gpu.out
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/bash</span>
<span class="org-builtin">source</span> activate scmodes
python -i /project2/mstephens/aksarkar/projects/singlecell-modes/code/benchmark.py &lt;&lt;EOF
<span class="org-sh-heredoc">x, y = cyto_naive_t_mix()</span>
<span class="org-sh-heredoc">res = evaluate_generalization(x.values, pool=None, stratify=y, train_size=0.5, test_size=0.1, random_state=0, methods=['nb', 'zinb'])</span>
<span class="org-sh-heredoc">res.index = x.columns</span>
<span class="org-sh-heredoc">res.to_csv(f'/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/cyto-naive-t-mix-gpu.txt.gz', compression='gzip', sep='\t')</span>
<span class="org-sh-heredoc">EOF</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-sh">sbatch --partition=broadwl -a 0-3 -n1 -c28 --exclusive --mem=16G --time=6:00:00 --job-name=benchmark --output=benchmark-cpu.out
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/bash</span>
<span class="org-builtin">source</span> activate scmodes
python -i /project2/mstephens/aksarkar/projects/singlecell-modes/code/benchmark.py &lt;&lt;EOF
<span class="org-sh-heredoc">import os</span>
<span class="org-sh-heredoc">cpu_methods = ['unimix', 'descend', 'npmle', 'saturated']</span>
<span class="org-sh-heredoc">task = int(os.environ['SLURM_ARRAY_TASK_ID'])</span>
<span class="org-sh-heredoc">with mp.Pool(maxtasksperchild=10) as pool:</span>
<span class="org-sh-heredoc">  x, y = cyto_naive_t_mix()</span>
<span class="org-sh-heredoc">  res = evaluate_generalization(x.values, pool=pool, stratify=y, train_size=0.5, test_size=0.1, random_state=0, methods=cpu_methods[task:task + 1])</span>
<span class="org-sh-heredoc">  res.index = x.columns</span>
<span class="org-sh-heredoc">  res.to_csv(f'/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/cyto-naive-t-mix-{cpu_methods[task]}.txt.gz', compression='gzip', sep='\t')</span>
<span class="org-sh-heredoc">EOF</span>
</pre>
</div>

<p>
Read the results.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">benchmark</span> = {}
<span class="org-variable-name">benchmark</span>[<span class="org-string">'cd8-cd19-mix'</span>] = pd.read_csv(f<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/cd8-cd19-mix.txt.gz'</span>, sep=<span class="org-string">'\t'</span>, index_col=0)
<span class="org-variable-name">benchmark</span>[<span class="org-string">'cyto-naive-t-mix'</span>] = (
  pd.read_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/cyto-naive-t-mix-gpu.txt.gz'</span>, sep=<span class="org-string">'\t'</span>)
  .merge(pd.read_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/cyto-naive-t-mix-unimix.txt.gz'</span>, sep=<span class="org-string">'\t'</span>), on=<span class="org-string">'0'</span>)
  .merge(pd.read_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/cyto-naive-t-mix-descend.txt.gz'</span>, sep=<span class="org-string">'\t'</span>), on=<span class="org-string">'0'</span>)
  .merge(pd.read_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/cyto-naive-t-mix-npmle.txt.gz'</span>, sep=<span class="org-string">'\t'</span>), on=<span class="org-string">'0'</span>)
  .merge(pd.read_csv(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-modes/data/deconv-generalization/cyto-naive-t-mix-saturated.txt.gz'</span>, sep=<span class="org-string">'\t'</span>), on=<span class="org-string">'0'</span>)
  .set_index(<span class="org-string">'0'</span>))
</pre>
</div>

<p>
Plot the results.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2, sharey=<span class="org-constant">True</span>)
fig.set_size_inches(5, 3)
<span class="org-keyword">for</span> a, k <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(ax, <span class="org-builtin">sorted</span>(benchmark.keys())):
  a.boxplot((benchmark[f<span class="org-string">'{k}'</span>].values - benchmark[f<span class="org-string">'{k}'</span>][<span class="org-string">'nb'</span>].values.reshape(-1, 1))[:,1:-1],
            widths=0.25, medianprops={<span class="org-string">'color'</span>: <span class="org-string">'k'</span>}, flierprops={<span class="org-string">'marker'</span>: <span class="org-string">'.'</span>, <span class="org-string">'markersize'</span>: 4})
  a.set_xticklabels(benchmark[f<span class="org-string">'{k}'</span>].columns[1:-1], rotation=90)
  a.set_xlabel(<span class="org-string">'Method'</span>)
  a.set_title(k)
ax[0].set_ylabel(<span class="org-string">'Diff val set log lik from NB'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/deconvolution.org/synthetic-mix-benchmark.png" alt="synthetic-mix-benchmark.png">
</p>
</div>
</div>
</div>

<div id="outline-container-orgcdbbb38" class="outline-4">
<h4 id="orgcdbbb38">Investigate genes favoring more complex models</h4>
<div class="outline-text-4" id="text-orgcdbbb38">
<p>
Read the data.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">mix1</span>, <span class="org-variable-name">y1</span> = cd8_cd19_mix()
<span class="org-variable-name">mix2</span>, <span class="org-variable-name">y2</span> = cyto_naive_t_mix()
</pre>
</div>

<p>
Look at genes where NPMLE has higher validation set log likelihood than NB.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">llik_diff</span> = benchmark[<span class="org-string">'cd8-cd19-mix'</span>][<span class="org-string">'npmle'</span>] - benchmark[<span class="org-string">'cd8-cd19-mix'</span>][<span class="org-string">'nb'</span>]
<span class="org-variable-name">query</span> = benchmark[<span class="org-string">'cd8-cd19-mix'</span>].loc[llik_diff[llik_diff &gt; 0].sort_values(ascending=<span class="org-constant">False</span>).head(n=4).index].index
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 4)
fig.set_size_inches(8, 2)
<span class="org-keyword">for</span> a, k <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(ax.ravel(), query):
  a.hist(mix1.iloc[:,k], bins=mix1.iloc[:,k].<span class="org-builtin">max</span>(), color=<span class="org-string">'k'</span>)
  a.set_title(gene_info.loc[mix1.columns[k], <span class="org-string">'name'</span>] <span class="org-keyword">if</span> mix1.columns[k] <span class="org-keyword">in</span> gene_info.index <span class="org-keyword">else</span> mix1.columns[k])
  a.set_xlabel(<span class="org-string">'Number of molecules'</span>)
ax[0].set_ylabel(<span class="org-string">'Number of cells'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/deconvolution.org/cd8-cd19-mix-npmle-examples.png" alt="cd8-cd19-mix-npmle-examples.png">
</p>
</div>

<p>
<i>CD74</i> appears to be the first good example of a gene (so far) showing
bimodal gene expression as predicted by a simple kinetic model (Munsky et
al. 2013, Kim and Marioni 2013).
</p>
</div>
</div>
</div>

<div id="outline-container-orgae913a7" class="outline-3">
<h3 id="orgae913a7">Heterogeneous cell populations</h3>
<div class="outline-text-3" id="text-orgae913a7">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">pbmcs_68k</span>(**kwargs):
  <span class="org-keyword">return</span> read_10x(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/68k_pbmcs/filtered_matrices_mex/hg19/'</span>, **kwargs)

<span class="org-keyword">def</span> <span class="org-function-name">cortex</span>():
  <span class="org-variable-name">counts</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/zeisel-2015/GSE60361_C1-3005-Expression.txt.gz'</span>, index_col=0)
  <span class="org-comment-delimiter"># </span><span class="org-comment">Follow scVI here</span>
  <span class="org-keyword">return</span> counts.loc[counts.var(axis=1).sort_values(ascending=<span class="org-constant">False</span>).head(n=500).index].values.T
</pre>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Abhishek Sarkar</p>
<p class="date">Created: 2019-04-06 Sat 16:45</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
