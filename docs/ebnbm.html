<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2020-09-10 Thu 11:29 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Negative binomial measurement model</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Abhishek Sarkar">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link href="css/bootstrap.min.css" rel="stylesheet" />
<link rel="stylesheet" type="text/css" href="css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="css/supp.css"/>
<style type="text/css">body {width: 60em; margin:auto} pre.src {overflow:auto}</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Negative binomial measurement model</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org040c2a7">Introduction</a></li>
<li><a href="#setup">Setup</a></li>
<li><a href="#orgf3cc87c">Results</a>
<ul>
<li><a href="#org1469cfa">Simulate from the NB measurement model</a></li>
<li><a href="#org72a0851">VBEM algorithm for Gamma expression model</a></li>
<li><a href="#org0473663">Simulated example</a></li>
<li><a href="#total-var">Moment-matching approximation</a></li>
<li><a href="#org9a061c5">Application to control data</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org040c2a7" class="outline-2">
<h2 id="org040c2a7">Introduction</h2>
<div class="outline-text-2" id="text-org040c2a7">
<p>
We, and Svensson 2020, found some evidence for overdispersion in control
scRNA-seq data. Here, we estimate to what extent that overdispersion could be
explained by an overdispersed measurement model, using the key fact that the
measurement overdispersion is described by a single parameter common across
all genes. We specifically consider combining an NB measurement model with a
Gamma expression model \(
  \DeclareMathOperator\Pois{Poisson}
  \DeclareMathOperator\Gam{Gamma}
  \DeclareMathOperator\NB{NB}
  \DeclareMathOperator\V{V}
  \newcommand\const{\mathrm{const}}
  \newcommand\lnb{l_{\mathrm{NB}}}
  \newcommand\E[1]{\left\langle #1 \right\rangle}
  \)
</p>

\begin{align}
  x_{ij} \mid s_i, \lambda_{ij}, u_{ij} &\sim \NB(s_i \lambda_{ij}, \theta)\\
  \lambda_{ij} \mid a_j, b_j &\sim \Gam(a_j, b_j),
\end{align}

<p>
where the NB distribution is parameterized by mean and dispersion, and the
Gamma distribution is parameterized by shape and rate.
</p>
</div>
</div>

<div id="outline-container-orgf1021bd" class="outline-2">
<h2 id="setup"><a id="orgf1021bd"></a>Setup</h2>
<div class="outline-text-2" id="text-setup">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> pandas <span class="org-keyword">as</span> pd
<span class="org-keyword">import</span> pickle
<span class="org-keyword">import</span> scanpy <span class="org-keyword">as</span> sc
<span class="org-keyword">import</span> scipy.optimize <span class="org-keyword">as</span> so
<span class="org-keyword">import</span> scipy.special <span class="org-keyword">as</span> sp
<span class="org-keyword">import</span> scipy.stats <span class="org-keyword">as</span> st
<span class="org-keyword">import</span> scmodes
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">%matplotlib inline
%config <span class="org-variable-name">InlineBackend.figure_formats</span> = <span class="org-builtin">set</span>([<span class="org-string">'retina'</span>])
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> colorcet
<span class="org-keyword">import</span> matplotlib
<span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'figure.facecolor'</span>] = <span class="org-string">'w'</span>
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'font.family'</span>] = <span class="org-string">'Nimbus Sans'</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-orgf3cc87c" class="outline-2">
<h2 id="orgf3cc87c">Results</h2>
<div class="outline-text-2" id="text-orgf3cc87c">
</div>
<div id="outline-container-org1469cfa" class="outline-3">
<h3 id="org1469cfa">Simulate from the NB measurement model</h3>
<div class="outline-text-3" id="text-org1469cfa">
<p>
Simulate data from the model.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org2b2e82e"><span class="org-keyword">def</span> <span class="org-function-name">simulate_nb_gamma</span>(n, p, theta, seed=0):
  np.random.seed(seed)
  <span class="org-variable-name">log_mean</span> = np.random.uniform(low=-12, high=-8, size=(1, p))
  <span class="org-variable-name">log_disp</span> = np.random.uniform(low=-6, high=0, size=(1, p))
  <span class="org-variable-name">s</span> = 1e5 * np.ones((n, 1))
  <span class="org-variable-name">lam</span> = st.gamma(a=np.exp(-log_disp), scale=np.exp(log_mean + log_disp)).rvs(size=(n, p))
  <span class="org-keyword">if</span> theta &gt; 0:
    <span class="org-variable-name">u</span> = st.gamma(a=1 / theta, scale=theta).rvs(size=(n, p))
  <span class="org-keyword">else</span>:
    <span class="org-variable-name">u</span> = 1
  <span class="org-variable-name">x</span> = st.poisson(s * lam * u).rvs()
  <span class="org-keyword">return</span> x, s, lam, u, log_mean, -log_disp, theta
</pre>
</div>
</div>
</div>

<div id="outline-container-org72a0851" class="outline-3">
<h3 id="org72a0851">VBEM algorithm for Gamma expression model</h3>
<div class="outline-text-3" id="text-org72a0851">
<p>
To estimate \(a_1, \ldots, a_p, b_1, \ldots, b_p, \theta\) from observed
data, we use a VBEM algorithm. First, introduce latent variables \(u_{ij}\)
</p>

\begin{align}
  x_{ij} \mid s_i, \lambda_{ij}, u_{ij} &\sim \Pois(s_i \lambda_{ij} u_{ij})\\
  u_{ij} \mid \theta &\sim \Gam(\theta^{-1}, \theta^{-1})\\
  \lambda_{ij} \mid a_j, b_j &\sim \Gam(a_j, b_j),
\end{align}

<p>
where the Gamma distribution is parameterized by shape and rate. It is
straightforward to show that marginalizing over \(u_{ij}\) yields the
original NB-Gamma compound model of interest. The log joint
</p>

\begin{multline}
  \ln p(x_{ij} \mid \lambda_{ij}, u_{ij}, a_j, b_j, \theta) = x_{ij} \ln (s_i \lambda_{ij} u_{ij}) - s_i \lambda_{ij} u_{ij} - \ln\Gamma(x_{ij} + 1)\\
  + (a_j - 1) \ln \lambda_{ij} - b_j \lambda_{ij} + a_j \ln b_j - \ln\Gamma(a_j) + (\theta^{-1} - 1) \ln u_{ij} - \theta^{-1} u_{ij} + \theta^{-1}\ln(\theta^{-1}) - \ln\Gamma(\theta^{-1}),
\end{multline}

<p>
and the posteriors
</p>

\begin{align}
  \ln p(\lambda_{ij} \mid x_{ij}, u_{ij}, a_j, b_j) &= (x_{ij} + a_j - 1) \ln \lambda_{ij} - (s_i u_{ij} + b_j) \lambda_{ij} + \const\\
  &= \Gam(x_{ij} + a_j, s_i u_{ij} + b_j)\\
  \ln p(u_{ij} \mid x_{ij}, \lambda_{ij}, a_j, b_j) &= (x_{ij} + \theta^{-1} - 1) \ln \lambda_{ij} - (s_i \lambda_{ij} + b_j) u_{ij} + \const\\
  &= \Gam(x_{ij} + \theta^{-1}, s_i \lambda_{ij} + b_j).
\end{align}

<p>
However, the required expectations for an EM algorithm that directly
maximizes the likelihood are non-analytic. To side-step this problem,
introduce a variational approximation
</p>

\begin{align}
  q &= \prod_{i,j} q(\lambda_{ij}) q(u_{ij})\\
  q^*(\lambda_{ij}) &\propto \exp((x_{ij} + a_j - 1) \ln \lambda_{ij} - (s_i \E{u_{ij}} + b_j) \lambda_{ij})\\
  &= \Gam(x_{ij} + a_j, s_i \E{u_{ij}} + b_j)\\
  &\triangleq \Gam(\alpha_{ij}, \beta_{ij})\\
  q^*(u_{ij}) &\propto \exp((x_{ij} + \theta^{-1} - 1) \ln u_{ij} - (s_i \E{\lambda_{ij}} + b_j) u_{ij})\\
  &= \Gam(x_{ij} + \theta^{-1}, s_i \E{\lambda_{ij}} + \theta^{-1})\\
  &\triangleq \Gam(\gamma_{ij}, \delta_{ij}).
\end{align}

<p>
The evidence lower bound
</p>

\begin{multline}
  \ell = \sum_{i, j} \left[ (x_{ij} + a_j - \alpha_{ij}) \E{\ln \lambda_{ij}} - (b_j - \beta_{ij}) \E{\lambda_{ij}} + (x_{ij} + \theta^{-1} - \gamma_{ij}) \E{\ln u_{ij}} - (\theta^{-1} - \delta_{ij}) \E{u_{ij}} - s_i \E{\lambda_{ij}} \E{u_{ij}}\right.\\
    + \left. a_j \ln b_j + \theta^{-1}\ln(\theta^{-1}) - \alpha_{ij} \ln \beta_{ij} - \gamma_{ij} \ln \delta_{ij} - \ln\Gamma(a_j) - \ln\Gamma(\theta^{-1}) + \ln\Gamma(\alpha_{ij}) + \ln\Gamma(\gamma_{ij})\right] + \const,
\end{multline}

<p>
where
</p>

\begin{align}
  \E{\lambda_{ij}} &= \alpha_{ij} / \beta_{ij}\\
  \E{\ln\lambda_{ij}} &= \psi(\alpha_{ij}) - \ln(\beta_{ij})\\
  \E{u_{ij}} &= \gamma_{ij} / \delta_{ij}\\
  \E{\ln u_{ij}} &= \psi(\gamma_{ij}) - \ln(\delta_{ij}),
\end{align}

<p>
and \(\psi\) denotes the digamma function. Then, we have analytic M step update
</p>

\begin{align}
  \frac{\partial \ell}{\partial b_j} &= \sum_{i} \frac{a_j}{b_j} - \E{\lambda_{ij}} = 0\\
  b_j &:= \frac{n a_j}{\sum_i \E{\lambda_{ij}}}
\end{align}

<p>
and Newton-Raphson partial M step updates
</p>

\begin{align}
  \eta &\triangleq \theta^{-1}\\
  \frac{\partial \ell}{\partial \eta} &= \sum_{i, j} 1 + \E{\ln u_{ij}} - \E{u_{ij}} - \psi(\eta)\\
  \frac{\partial^2 \ell}{\partial \eta^2} &= -n p \psi^{(1)}(\eta)\\
  \frac{\partial \ell}{\partial a_j} &= \sum_i \E{\ln\lambda_{ij}} + \ln b_j - \psi(a_j)\\
  \frac{\partial^2 \ell}{\partial a_j^2} &=  -n \psi^{(1)}(a_j),
\end{align}

<p>
where \(\psi^{(1)}\) denotes the trigamma function.
</p>
</div>
</div>

<div id="outline-container-org0473663" class="outline-3">
<h3 id="org0473663">Simulated example</h3>
<div class="outline-text-3" id="text-org0473663">
<p>
Fit the model to a simulated example, fixing the hyperparameters to the
ground truth values. (This only updates the variational approximation.) As a
baseline, fit a Gamma expression model to each gene assuming a Poisson
measurement model.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">x</span>, <span class="org-variable-name">s</span>, <span class="org-variable-name">lam</span>, <span class="org-variable-name">u</span>, <span class="org-variable-name">log_mean</span>, <span class="org-variable-name">log_inv_disp</span>, <span class="org-variable-name">theta</span> = simulate_nb_gamma(n=1000, p=5, theta=0.2, seed=1)
<span class="org-variable-name">par</span> = np.array([scmodes.ebpm.ebpm_gamma(x[:,j].ravel(), s.ravel()) <span class="org-keyword">for</span> j <span class="org-keyword">in</span> <span class="org-builtin">range</span>(x.shape[1])])
<span class="org-variable-name">log_mean_hat</span>, <span class="org-variable-name">log_inv_disp_hat</span>, <span class="org-variable-name">log_meas_disp_hat</span>, <span class="org-variable-name">alpha</span>, <span class="org-variable-name">beta</span>, <span class="org-variable-name">gamma</span>, <span class="org-variable-name">delta</span>, <span class="org-variable-name">elbo</span> = scmodes.ebnbm.ebnbm_gamma(
  x,
  s,
  init=np.hstack([np.exp(log_inv_disp).ravel(), np.exp(-log_mean + log_inv_disp).ravel(), theta]),
  tol=1e-5,
  extrapolate=<span class="org-constant">False</span>,
  fix_g=<span class="org-constant">True</span>,
  fix_theta=<span class="org-constant">True</span>)
</pre>
</div>

<p>
Make sure we didn&rsquo;t mess up the parameterization.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 5, sharey=<span class="org-constant">True</span>)
fig.set_size_inches(8, 2)

<span class="org-keyword">for</span> i, a <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(ax):
  <span class="org-variable-name">y</span> = np.arange(x[:,i].<span class="org-builtin">max</span>() + 1)
  <span class="org-comment-delimiter"># </span><span class="org-comment">Poisson measurement =&gt; NB observation</span>
  <span class="org-variable-name">pmf</span> = <span class="org-builtin">dict</span>()
  <span class="org-variable-name">pmf</span>[<span class="org-string">'Poisson'</span>] = st.nbinom(n=np.exp(par[i,1]), p=1 / (1 + (s * np.exp(par[i,0] - par[i,1])))).pmf(y).mean(axis=0)
  <span class="org-comment-delimiter"># </span><span class="org-comment">NB measurement =&gt; Monte Carlo integral</span>
  <span class="org-variable-name">n_samples</span> = 1000
  <span class="org-variable-name">Ghat</span> = st.gamma(a=np.exp(log_inv_disp_hat[:,i]), scale=np.exp(log_mean_hat[:,i] - log_inv_disp_hat[:,i]))
  <span class="org-variable-name">temp</span> = Ghat.rvs(size=(n_samples, y.shape[0], 1))
  <span class="org-variable-name">pmf</span>[rf<span class="org-string">'NB ($\hat\theta$ = {np.exp(log_meas_disp_hat):.2g})'</span>] = st.nbinom(n=np.exp(-log_meas_disp_hat), p=1 / (1 + s[0] * temp * np.exp(log_meas_disp_hat))).pmf(y.reshape(-1, 1)).mean(axis=0)

  ax[i].hist(x[:,i], bins=y, color=<span class="org-string">'0.7'</span>, density=<span class="org-constant">True</span>)
  <span class="org-keyword">for</span> j, k <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(pmf):
    ax[i].plot(y + .5, pmf[k], c=cm(j), lw=1, label=k)
  ax[i].set_title(f<span class="org-string">'Gene {i+1}'</span>)
  ax[i].set_xlim(0, x[:,i].<span class="org-builtin">max</span>())

ax[0].set_ylabel(<span class="org-string">'Density'</span>)
ax[-1].legend(title=<span class="org-string">'Measurement model'</span>, frameon=<span class="org-constant">False</span>, bbox_to_anchor=(1, .5), loc=<span class="org-string">'center left'</span>)
<span class="org-variable-name">a</span> = fig.add_subplot(111, frameon=<span class="org-constant">False</span>, xticks=[], yticks=[])
a.set_xlabel(<span class="org-string">'Number of molecules'</span>, labelpad=16)
fig.tight_layout(pad=0.5)
</pre>
</div>


<div class="figure">
<p><img src="figure/ebnbm.org/sim-ex.png" alt="sim-ex.png">
</p>
</div>

<p>
Compare the true posterior against the variational approximation.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2)
fig.set_size_inches(5, 2.5)
<span class="org-keyword">for</span> a <span class="org-keyword">in</span> ax:
  a.set_aspect(<span class="org-string">'equal'</span>, adjustable=<span class="org-string">'datalim'</span>)

ax[0].set_xscale(<span class="org-string">'log'</span>)
ax[0].set_yscale(<span class="org-string">'log'</span>)
ax[0].scatter(((x + np.exp(log_inv_disp)) / (s * u + np.exp(log_inv_disp - log_mean))).ravel(), (alpha / beta).ravel(), s=1, c=<span class="org-string">'k'</span>, alpha=0.1)
<span class="org-variable-name">lim</span> = ax[0].get_xlim()
ax[0].plot(lim, lim, lw=1, ls=<span class="org-string">':'</span>, c=<span class="org-string">'r'</span>)
ax[0].set_xlabel(r<span class="org-string">'$\mathrm{E}[\lambda \mid x, u]$'</span>)
ax[0].set_ylabel(r<span class="org-string">'$\mathrm{E}_q[\lambda]$'</span>)

ax[1].scatter(((x + theta) / (s * lam + theta)), (gamma / delta).ravel(), s=1, c=<span class="org-string">'k'</span>, alpha=0.1)
<span class="org-variable-name">lim</span> = ax[1].get_xlim()
ax[1].plot(lim, lim, lw=1, ls=<span class="org-string">':'</span>, c=<span class="org-string">'r'</span>)
ax[1].set_xlabel(r<span class="org-string">'$\mathrm{E}[u \mid x, \lambda]$'</span>)
ax[1].set_ylabel(r<span class="org-string">'$\mathrm{E}_q[u]$'</span>)

fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/ebnbm.org/sim-ex2.png" alt="sim-ex2.png">
</p>
</div>

<p>
Now, fit the model initialized at the ground truth hyperparameters, fixing
\(\theta\).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">fit0</span> = scmodes.ebnbm.ebnbm_gamma(
  x,
  s,
  init=np.hstack([np.exp(log_inv_disp).ravel(), np.exp(-log_mean + log_inv_disp).ravel(), theta]),
  tol=1e-4,
  max_iters=300_000,
  extrapolate=<span class="org-constant">True</span>,
  fix_g=<span class="org-constant">False</span>,
  fix_theta=<span class="org-constant">True</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">fit1</span> = scmodes.ebnbm.ebnbm_gamma(
  x,
  s,
  init=np.hstack([np.exp(log_inv_disp).ravel(), np.exp(-log_mean + log_inv_disp).ravel(), theta]),
  tol=1e-5,
  max_iters=300_000,
  extrapolate=<span class="org-constant">True</span>,
  fix_g=<span class="org-constant">False</span>,
  fix_theta=<span class="org-constant">True</span>)
<span class="org-keyword">with</span> <span class="org-builtin">open</span>(<span class="org-string">'/scratch/midway2/aksarkar/modes/ebnbm-sim-ex-1e-5.pkl'</span>, <span class="org-string">'wb'</span>) <span class="org-keyword">as</span> f:
  pickle.dump(fit1, f)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">with</span> <span class="org-builtin">open</span>(<span class="org-string">'/scratch/midway2/aksarkar/modes/ebnbm-sim-ex-1e-5.pkl'</span>, <span class="org-string">'rb'</span>) <span class="org-keyword">as</span> f:
  <span class="org-variable-name">log_mean_hat</span>, <span class="org-variable-name">log_inv_disp_hat</span>, <span class="org-variable-name">log_meas_disp_hat</span>, <span class="org-variable-name">alpha</span>, <span class="org-variable-name">beta</span>, <span class="org-variable-name">gamma</span>, <span class="org-variable-name">delta</span>, <span class="org-variable-name">elbo</span> = pickle.load(f)
</pre>
</div>

<p>
Plot the fitted observation models against the observed data.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 5, sharey=<span class="org-constant">True</span>)
fig.set_size_inches(8, 2)

<span class="org-keyword">for</span> i, a <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(ax):
  <span class="org-variable-name">y</span> = np.arange(x[:,i].<span class="org-builtin">max</span>() + 1)
  <span class="org-comment-delimiter"># </span><span class="org-comment">Poisson measurement =&gt; NB observation</span>
  <span class="org-variable-name">pmf</span> = <span class="org-builtin">dict</span>()
  <span class="org-variable-name">pmf</span>[<span class="org-string">'Poisson'</span>] = st.nbinom(n=np.exp(par[i,1]), p=1 / (1 + (s * np.exp(par[i,0] - par[i,1])))).pmf(y).mean(axis=0)
  <span class="org-comment-delimiter"># </span><span class="org-comment">NB measurement =&gt; Monte Carlo integral</span>
  <span class="org-variable-name">n_samples</span> = 1000
  <span class="org-variable-name">Ghat</span> = st.gamma(a=np.exp(log_inv_disp_hat[:,i]), scale=np.exp(log_mean_hat[:,i] - log_inv_disp_hat[:,i]))
  <span class="org-variable-name">temp</span> = Ghat.rvs(size=(n_samples, y.shape[0], 1))
  <span class="org-variable-name">pmf</span>[rf<span class="org-string">'NB ($\hat\theta$ = {np.exp(log_meas_disp_hat):.2g})'</span>] = st.nbinom(n=np.exp(-log_meas_disp_hat), p=1 / (1 + s[0] * temp * np.exp(log_meas_disp_hat))).pmf(y.reshape(-1, 1)).mean(axis=0)

  ax[i].hist(x[:,i], bins=y, color=<span class="org-string">'0.7'</span>, density=<span class="org-constant">True</span>)
  <span class="org-keyword">for</span> j, k <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(pmf):
    ax[i].plot(y + .5, pmf[k], c=cm(j), lw=1, label=k)
  ax[i].set_title(f<span class="org-string">'Gene {i+1}'</span>)
  ax[i].set_xlim(0, x[:,i].<span class="org-builtin">max</span>())

ax[0].set_ylabel(<span class="org-string">'Density'</span>)
ax[-1].legend(title=<span class="org-string">'Measurement model'</span>, frameon=<span class="org-constant">False</span>, bbox_to_anchor=(1, .5), loc=<span class="org-string">'center left'</span>)
<span class="org-variable-name">a</span> = fig.add_subplot(111, frameon=<span class="org-constant">False</span>, xticks=[], yticks=[])
a.set_xlabel(<span class="org-string">'Number of molecules'</span>, labelpad=16)
fig.tight_layout(pad=0.5)
</pre>
</div>


<div class="figure">
<p><img src="figure/ebnbm.org/sim-ex3.png" alt="sim-ex3.png">
</p>
</div>

<p>
Compare the estimated expression models against the ground truth.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2)
fig.set_size_inches(5, 2.5)
<span class="org-keyword">for</span> a <span class="org-keyword">in</span> ax:
  a.set_aspect(<span class="org-string">'equal'</span>, <span class="org-string">'datalim'</span>)
ax[0].scatter(log_mean, log_mean_hat, s=4, c=<span class="org-string">'k'</span>)
<span class="org-variable-name">lim</span> = ax[0].get_xlim()
ax[0].plot(lim, lim, lw=1, ls=<span class="org-string">':'</span>, c=<span class="org-string">'r'</span>)
ax[0].set_xlabel(<span class="org-string">'$\ln(\mu)$'</span>)
ax[0].set_ylabel(<span class="org-string">'$\ln(\hat\mu)$'</span>)
ax[1].scatter(log_inv_disp, log_inv_disp_hat, s=4, c=<span class="org-string">'k'</span>)
<span class="org-variable-name">lim</span> = ax[1].get_xlim()
ax[1].plot(lim, lim, lw=1, ls=<span class="org-string">':'</span>, c=<span class="org-string">'r'</span>)
ax[1].set_xlabel(<span class="org-string">'$\ln(\phi)$'</span>)
ax[1].set_ylabel(<span class="org-string">'$\ln(\hat\phi)$'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/ebnbm.org/sim-ex4.png" alt="sim-ex4.png">
</p>
</div>

<p>
Now fit the model, fixing \(\theta\) to the ground truth, and initializing
the expression models at the MLE of a Gamma expression model assuming a
Poisson measurement model.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">par</span> = np.array([scmodes.ebpm.ebpm_gamma(x[:,j], s.ravel()) <span class="org-keyword">for</span> j <span class="org-keyword">in</span> <span class="org-builtin">range</span>(x.shape[1])])
<span class="org-variable-name">init</span> = np.hstack([np.exp(par[:,1]), np.exp(par[:,1] - par[:,0]), theta])
<span class="org-variable-name">fit2</span> = scmodes.ebnbm.ebnbm_gamma(
  x,
  s,
  init=init,
  tol=1e-5,
  max_iters=300_000,
  extrapolate=<span class="org-constant">True</span>,
  fix_g=<span class="org-constant">False</span>,
  fix_theta=<span class="org-constant">True</span>)
<span class="org-keyword">with</span> <span class="org-builtin">open</span>(<span class="org-string">'/scratch/midway2/aksarkar/modes/ebnbm-sim-ex-1e-5-pois-init.pkl'</span>, <span class="org-string">'wb'</span>) <span class="org-keyword">as</span> f:
  pickle.dump(fit2, f)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">with</span> <span class="org-builtin">open</span>(<span class="org-string">'/scratch/midway2/aksarkar/modes/ebnbm-sim-ex-1e-5-pois-init.pkl'</span>, <span class="org-string">'rb'</span>) <span class="org-keyword">as</span> f:
  <span class="org-variable-name">log_mean_hat</span>, <span class="org-variable-name">log_inv_disp_hat</span>, <span class="org-variable-name">log_meas_disp_hat</span>, <span class="org-variable-name">alpha</span>, <span class="org-variable-name">beta</span>, <span class="org-variable-name">gamma</span>, <span class="org-variable-name">delta</span>, <span class="org-variable-name">elbo</span> = pickle.load(f)
</pre>
</div>

<p>
Compare the fitted expression models against the ground truth.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2)
fig.set_size_inches(5, 2.5)
<span class="org-keyword">for</span> a <span class="org-keyword">in</span> ax:
  a.set_aspect(<span class="org-string">'equal'</span>, <span class="org-string">'datalim'</span>)
ax[0].scatter(log_mean, par[:,0], s=16, c=<span class="org-string">'r'</span>, marker=<span class="org-string">'x'</span>, label=<span class="org-string">'Initialization'</span>)
ax[0].scatter(log_mean, log_mean_hat, s=16, c=<span class="org-string">'k'</span>, marker=<span class="org-string">'+'</span>, label=<span class="org-string">'Estimate'</span>)
<span class="org-variable-name">lim</span> = ax[0].get_xlim()
ax[0].plot(lim, lim, lw=1, ls=<span class="org-string">':'</span>, c=<span class="org-string">'r'</span>)
ax[0].legend(handletextpad=0, frameon=<span class="org-constant">False</span>)
ax[0].set_xlabel(<span class="org-string">'$\ln(\mu)$'</span>)
ax[0].set_ylabel(<span class="org-string">'$\ln(\hat\mu)$'</span>)

ax[1].scatter(log_inv_disp, par[:,1], s=16, c=<span class="org-string">'r'</span>, marker=<span class="org-string">'x'</span>, label=<span class="org-string">'Initialization'</span>)
ax[1].scatter(log_inv_disp, log_inv_disp_hat, s=16, c=<span class="org-string">'k'</span>, marker=<span class="org-string">'+'</span>, label=<span class="org-string">'Estimate'</span>)
<span class="org-variable-name">lim</span> = ax[1].get_ylim()
ax[1].plot(lim, lim, lw=1, ls=<span class="org-string">':'</span>, c=<span class="org-string">'r'</span>)
ax[1].set_xlabel(<span class="org-string">'$\ln(\phi)$'</span>)
ax[1].set_ylabel(<span class="org-string">'$\ln(\hat\phi)$'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/ebnbm.org/sim-ex5.png" alt="sim-ex5.png">
</p>
</div>

<p>
Compute the ELBO as a function of \(\theta\).
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org558c104"><span class="org-keyword">def</span> <span class="org-function-name">map_ebnbm_gamma</span>(x, s, grid):
  <span class="org-keyword">print</span>(<span class="org-string">'initializing'</span>)
  <span class="org-variable-name">par</span> = np.array([scmodes.ebpm.ebpm_gamma(x[:,j], s.ravel()) <span class="org-keyword">for</span> j <span class="org-keyword">in</span> <span class="org-builtin">range</span>(x.shape[1])])
  <span class="org-comment-delimiter"># </span><span class="org-comment">exp(20) is finite and large enough</span>
  <span class="org-variable-name">par</span> = np.ma.masked_invalid(par).filled(20)
  <span class="org-variable-name">fits</span> = []
  <span class="org-keyword">for</span> theta <span class="org-keyword">in</span> grid:
    <span class="org-keyword">print</span>(f<span class="org-string">'fitting theta={theta:.3g}'</span>)
    <span class="org-variable-name">fit</span> = scmodes.ebnbm.ebnbm_gamma(
      x,
      s,
      init=np.hstack([np.exp(par[:,1]), np.exp(par[:,1] - par[:,0]), theta]),
      tol=1e-4,
      max_iters=100_000,
      extrapolate=<span class="org-constant">True</span>,
      fix_g=<span class="org-constant">False</span>,
      fix_theta=<span class="org-constant">True</span>)
    <span class="org-comment-delimiter"># </span><span class="org-comment">Warm start</span>
    <span class="org-variable-name">par</span> = np.vstack(fit[:2]).T
    fits.append(fit)
  <span class="org-keyword">return</span> fits
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">grid</span> = np.logspace(-3, 0, 40)
<span class="org-variable-name">fits</span> = map_ebnbm_gamma(x, s, grid)
<span class="org-variable-name">elbo</span> = np.array([f[-1] <span class="org-keyword">for</span> f <span class="org-keyword">in</span> fits])
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
plt.gcf().set_size_inches(2.5, 2.5)
plt.xscale(<span class="org-string">'log'</span>)
plt.plot(grid, elbo, lw=1, c=<span class="org-string">'k'</span>)
plt.axvline(x=0.2, lw=1, ls=<span class="org-string">':'</span>, c=<span class="org-string">'r'</span>)
plt.xticks(np.logspace(-3, 0, 4))
plt.xlabel(r<span class="org-string">'Measurement dispersion $\theta$'</span>)
plt.ylabel(<span class="org-string">'ELBO'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/ebnbm.org/sim-elbo.png" alt="sim-elbo.png">
</p>
</div>

<p>
Find the local minimum in the ELBO.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">idx</span> = np.where((np.sign(np.diff(elbo)) + 1) / 2)
grid[idx][0], elbo[idx][0]
</pre>
</div>

<pre class="example">
(0.05878016072274915, -9681.893026800943)

</pre>

<p>
Try running VBEM to stricter tolerance for this choice of \(\theta\).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">par</span> = np.array([scmodes.ebpm.ebpm_gamma(x[:,j], s.ravel()) <span class="org-keyword">for</span> j <span class="org-keyword">in</span> <span class="org-builtin">range</span>(x.shape[1])])
<span class="org-variable-name">init</span> = np.hstack([np.exp(par[:,1]), np.exp(par[:,1] - par[:,0]), grid[idx][0]])
<span class="org-variable-name">fit3</span> = scmodes.ebnbm.ebnbm_gamma(
  x,
  s,
  init=init,
  tol=1e-6,
  max_iters=300_000,
  extrapolate=<span class="org-constant">True</span>,
  fix_g=<span class="org-constant">False</span>,
  fix_theta=<span class="org-constant">True</span>)
fit3[-1]
</pre>
</div>

<pre class="example">
-9681.60485796223

</pre>

<p>
Try initializing VBEM at the ground truth for this choice of \(\theta\).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">init</span> = np.hstack([np.exp(log_inv_disp).ravel(), np.exp(-log_mean + log_inv_disp).ravel(), grid[idx][0]])
<span class="org-variable-name">fit4</span> = scmodes.ebnbm.ebnbm_gamma(
  x,
  s,
  init=init,
  tol=1e-6,
  max_iters=300_000,
  extrapolate=<span class="org-constant">True</span>,
  fix_g=<span class="org-constant">False</span>,
  fix_theta=<span class="org-constant">True</span>)
fit4[-1]
</pre>
</div>

<pre class="example">
-9718.192640132213

</pre>

<p>
Look at what&rsquo;s happening for small \(\theta\), by comparing the estimated
expression models.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = colorcet.cm[<span class="org-string">'bmy'</span>]
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2)
fig.set_size_inches(6, 2.5)
<span class="org-keyword">for</span> a <span class="org-keyword">in</span> ax:
  a.set_aspect(<span class="org-string">'equal'</span>, <span class="org-string">'datalim'</span>)

<span class="org-keyword">for</span> theta, fit <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(grid, fits):
  <span class="org-variable-name">c</span> = (np.log(theta) - np.log(1e-3)) / (-np.log(1e-3))
  ax[0].scatter(log_mean, fit[0], s=4, c=np.array(cm(c)).reshape(1, -1))
  ax[1].scatter(log_inv_disp, fit[1], s=4, c=np.array(cm(c)).reshape(1, -1))

<span class="org-variable-name">lim</span> = ax[0].get_xlim()
ax[0].plot(lim, lim, lw=1, ls=<span class="org-string">':'</span>, c=<span class="org-string">'r'</span>)
ax[0].set_xlabel(<span class="org-string">'$\ln(\mu)$'</span>)
ax[0].set_ylabel(<span class="org-string">'$\ln(\hat\mu)$'</span>)

<span class="org-variable-name">lim</span> = ax[1].get_ylim()
ax[1].plot(lim, lim, lw=1, ls=<span class="org-string">':'</span>, c=<span class="org-string">'r'</span>)
ax[1].set_xlabel(<span class="org-string">'$\ln(\phi)$'</span>)
ax[1].set_ylabel(<span class="org-string">'$\ln(\hat\phi)$'</span>)

<span class="org-variable-name">cb</span> = plt.colorbar(matplotlib.cm.ScalarMappable(matplotlib.colors.LogNorm(vmin=1e-3, vmax=1), cmap=cm),
                  fraction=0.05, shrink=0.5)
cb.set_label(r<span class="org-string">'Dispersion $\theta$'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/ebnbm.org/sim-ex6.png" alt="sim-ex6.png">
</p>
</div>

<p>
Compare the estimated observation models, focusing on simulated genes 1 and 2.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = colorcet.cm[<span class="org-string">'fire'</span>]
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, 2)
fig.set_size_inches(8, 3)

<span class="org-keyword">for</span> i, a <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(ax):
  <span class="org-variable-name">y</span> = np.arange(x[:,i].<span class="org-builtin">max</span>() + 1)
  <span class="org-comment-delimiter"># </span><span class="org-comment">NB measurement =&gt; Monte Carlo integral</span>
  <span class="org-variable-name">pmf</span> = <span class="org-builtin">dict</span>()
  <span class="org-variable-name">n_samples</span> = 5000
  <span class="org-variable-name">query</span> = (0, 13, 23, 26, 30, 35)
  <span class="org-keyword">for</span> j <span class="org-keyword">in</span> query:
    <span class="org-variable-name">Ghat</span> = st.gamma(a=np.exp(fits[j][1][:,i]), scale=np.exp(fits[j][0][:,i] - fits[j][1][:,i]))
    <span class="org-variable-name">temp</span> = Ghat.rvs(size=(n_samples, y.shape[0], 1))
    pmf[rf<span class="org-string">'NB ($\theta$ = {grid[j]:.2g})'</span>] = st.nbinom(n=1 / grid[j], p=1 / (1 + s[0] * temp * grid[j])).pmf(y.reshape(-1, 1)).mean(axis=0)

  ax[i].hist(x[:,i], bins=y, color=<span class="org-string">'0.7'</span>, density=<span class="org-constant">True</span>)
  <span class="org-keyword">for</span> j, k <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(query, pmf):
    <span class="org-variable-name">z</span> = (np.log(grid[j]) - np.log(1e-3)) / (-np.log(1e-3))
    ax[i].plot(y + .5, pmf[k], c=cm(z), lw=1, label=k)
  ax[i].set_title(f<span class="org-string">'Gene {i+1}'</span>)
  ax[i].set_xlim(0, x[:,i].<span class="org-builtin">max</span>())

ax[0].set_ylabel(<span class="org-string">'Density'</span>)
ax[-1].legend(title=<span class="org-string">'Measurement model'</span>, frameon=<span class="org-constant">False</span>, bbox_to_anchor=(1, .5), loc=<span class="org-string">'center left'</span>)
<span class="org-variable-name">a</span> = fig.add_subplot(111, frameon=<span class="org-constant">False</span>, xticks=[], yticks=[])
a.set_xlabel(<span class="org-string">'Number of molecules'</span>, labelpad=24)
fig.tight_layout(pad=0.5)
</pre>
</div>


<div class="figure">
<p><img src="figure/ebnbm.org/sim-ex7.png" alt="sim-ex7.png">
</p>
</div>

<p>
Look at the ELBO as a function of \(\theta\), for data simulated under
several different choices of \(\theta\).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">elbo</span> = <span class="org-builtin">dict</span>()
<span class="org-variable-name">grid</span> = np.logspace(-3, 0, 100)
<span class="org-keyword">for</span> theta0 <span class="org-keyword">in</span> (0, 1e-3, 1e-2, 0.1, 1):
  <span class="org-keyword">print</span>(f<span class="org-string">'theta0 = {theta0:.3g}'</span>)
  <span class="org-variable-name">x</span>, <span class="org-variable-name">s</span>, <span class="org-variable-name">lam</span>, <span class="org-variable-name">u</span>, <span class="org-variable-name">log_mean</span>, <span class="org-variable-name">log_inv_disp</span>, <span class="org-variable-name">theta</span> = simulate_nb_gamma(n=100, p=5, theta=theta0, seed=10)
  <span class="org-variable-name">fits</span> = map_ebnbm_gamma(x, s, grid)
  <span class="org-variable-name">elbo</span>[theta0] = np.array([f[-1] <span class="org-keyword">for</span> f <span class="org-keyword">in</span> fits])
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = colorcet.cm[<span class="org-string">'bmy'</span>]
plt.clf()
plt.gcf().set_size_inches(3, 3)
plt.xscale(<span class="org-string">'log'</span>)
<span class="org-keyword">for</span> i, k <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(elbo):
  <span class="org-keyword">if</span> k &gt; 0:
    <span class="org-variable-name">z</span> = (np.log(k) - np.log(1e-5)) / (1 - np.log(1e-5))
  <span class="org-keyword">else</span>:
    <span class="org-variable-name">z</span> = 0
  plt.plot(grid, elbo[k] - elbo[k].<span class="org-builtin">max</span>(), lw=1, c=cm(z), label=rf<span class="org-string">'$\theta$ = {k:.3g}'</span>)
plt.xticks(np.logspace(-3, 0, 4))
plt.legend(frameon=<span class="org-constant">False</span>)
plt.xlabel(r<span class="org-string">'Measurement dispersion $\theta$'</span>)
plt.ylabel(<span class="org-string">'Diff ELBO from best'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/ebnbm.org/sim-elbo-theta-seed=10.png" alt="sim-elbo-theta-seed=10.png">
</p>
</div>

<p>
Look more closely at the &ldquo;phase change&rdquo; between \(\theta = 0.01\) and
\(\theta = 0.1\)
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">elbo</span> = <span class="org-builtin">dict</span>()
<span class="org-variable-name">grid</span> = np.logspace(-3, 0, 100)
<span class="org-keyword">for</span> theta0 <span class="org-keyword">in</span> np.logspace(-2, -1, 5):
  <span class="org-keyword">print</span>(f<span class="org-string">'theta0 = {theta0:.3g}'</span>)
  <span class="org-variable-name">x</span>, <span class="org-variable-name">s</span>, <span class="org-variable-name">lam</span>, <span class="org-variable-name">u</span>, <span class="org-variable-name">log_mean</span>, <span class="org-variable-name">log_inv_disp</span>, <span class="org-variable-name">theta</span> = simulate_nb_gamma(n=100, p=5, theta=theta0, seed=10)
  <span class="org-variable-name">fits</span> = map_ebnbm_gamma(x, s, grid)
  <span class="org-variable-name">elbo</span>[theta0] = np.array([f[-1] <span class="org-keyword">for</span> f <span class="org-keyword">in</span> fits])
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = colorcet.cm[<span class="org-string">'bmy'</span>]
plt.clf()
plt.gcf().set_size_inches(3, 3)
plt.xscale(<span class="org-string">'log'</span>)
<span class="org-keyword">for</span> i, k <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(elbo):
  <span class="org-variable-name">z</span> = (np.log(k) - np.log(1e-2)) / (np.log(.1) - np.log(1e-2))
  plt.plot(grid, elbo[k] - elbo[k].<span class="org-builtin">max</span>(), lw=1, c=cm(z), label=rf<span class="org-string">'$\theta$ = {k:.3g}'</span>)
plt.xticks(np.logspace(-3, 0, 4))
plt.legend(frameon=<span class="org-constant">False</span>)
plt.xlabel(r<span class="org-string">'Measurement dispersion $\theta$'</span>)
plt.ylabel(<span class="org-string">'Diff ELBO from best'</span>)
plt.ylim(-50, 0)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/ebnbm.org/sim-theta-elbo-seed=10-inset.png" alt="sim-theta-elbo-seed=10-inset.png">
</p>
</div>
</div>
</div>

<div id="outline-container-orgedab333" class="outline-3">
<h3 id="total-var"><a id="orgedab333"></a>Moment-matching approximation</h3>
<div class="outline-text-3" id="text-total-var">
<p>
Consider gene \(j\), and assume \(a = \phi^{-1}\), \(b =
   \mu^{-1}\phi^{-1}\). Then,
</p>

\begin{align}
  \E{\lambda_i} &= \mu\\
  \V[\lambda_i] &= \mu^2\phi\\
  \E{x_i} &= \E{\E{x_i \mid s_i, \lambda_i}} = s_i \mu\\
  \V[x_i] &= \E{\V[x_i \mid s_i, \lambda_i]} + \V[\E{x_i \mid s_i \lambda_i}]\\
  &= \E{s_i \lambda_i + (s_i \lambda_i)^2 \theta} + \V[s_i \lambda_i]\\
  &= s_i \mu + s_i^2 \mu^2 (\phi + \theta + \phi\theta)
\end{align}

<p>
This result suggests an approximate approach to characterize the profile
likelihood of the data with respect to \(\theta\), that uses an NB model
with dispersion \(d = \phi + \theta + \phi\theta\). As a proof of concept,
fit the model using the Nelder-Mead algorithm.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">_loss</span>(par, x, s, theta0):
  <span class="org-variable-name">mu</span>, <span class="org-variable-name">phi</span> = np.exp(par)
  <span class="org-variable-name">theta</span> = phi + theta0 + phi * theta0
  <span class="org-keyword">return</span> -st.nbinom(n=1 / theta, p=1 / (1 + s * mu * theta)).logpmf(x).mean()

np.random.seed(1)
<span class="org-variable-name">s</span> = 1e5
<span class="org-variable-name">n</span> = 100

<span class="org-variable-name">log_mu</span> = -10
<span class="org-variable-name">log_phi</span> = -6
<span class="org-variable-name">lam</span> = st.gamma(a=np.exp(-log_phi), scale=np.exp(log_mu + log_phi)).rvs(n)

<span class="org-variable-name">fits</span> = <span class="org-builtin">dict</span>()
<span class="org-variable-name">grid</span> = np.logspace(-3, 1, 100)
<span class="org-keyword">for</span> theta0 <span class="org-keyword">in</span> np.logspace(-4, 0, 5):
  <span class="org-keyword">print</span>(f<span class="org-string">'fitting theta0 = {theta0:.1g}'</span>)
  <span class="org-variable-name">u</span> = st.gamma(a=1 / theta0, scale=theta0).rvs(n)
  <span class="org-variable-name">x</span> = st.poisson(s * lam * u).rvs(n)
  <span class="org-variable-name">fits</span>[theta0] = [so.minimize(_loss, x0=[log_mu, log_phi], args=(x, s, theta), method=<span class="org-string">'Nelder-Mead'</span>)
                  <span class="org-keyword">for</span> theta <span class="org-keyword">in</span> grid]
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = colorcet.cm[<span class="org-string">'bmy'</span>]
plt.clf()
plt.gcf().set_size_inches(4, 2.5)
plt.xscale(<span class="org-string">'log'</span>)
<span class="org-keyword">for</span> i, k <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(fits):
  <span class="org-variable-name">temp</span> = n * np.array([-f.fun <span class="org-keyword">for</span> f <span class="org-keyword">in</span> fits[k]])
  <span class="org-variable-name">temp</span> -= temp.<span class="org-builtin">max</span>()
  <span class="org-variable-name">c</span> = (np.log(k) - np.log(1e-4)) / (1 - np.log(1e-4))
  plt.plot(grid, temp, lw=1, c=cm(c), label=rf<span class="org-string">'$\theta$ = {k:.1g}'</span>)
plt.axvline(x=np.exp(log_phi), lw=1, ls=<span class="org-string">':'</span>, c=<span class="org-string">'k'</span>)
plt.axhline(y=-np.log(10), lw=1, ls=<span class="org-string">':'</span>, c=<span class="org-string">'k'</span>)
plt.legend(title=r<span class="org-string">'True $\theta$'</span>, frameon=<span class="org-constant">False</span>, loc=<span class="org-string">'center left'</span>, bbox_to_anchor=(1, .5))
plt.xticks(np.logspace(-3, 1, 5))
plt.ylim(-10, 0)
plt.xlabel(r<span class="org-string">'Assumed dispersion $\theta$'</span>)
plt.ylabel(<span class="org-string">'Diff ln lik from best'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/ebnbm.org/var-heuristic.png" alt="var-heuristic.png">
</p>
</div>

<p>
Now, fit the model in stages:
</p>

<ol class="org-ol">
<li>Fit the NB model via unconstrained optimization, yielding MLE \(\hat\mu,
      \hat{d}\)</li>
<li>If \(\hat\phi = (\hat{d} - \theta) / (1 + \theta) < 0\), then fit the NB
model, fixing dispersion \(\theta\).</li>
</ol>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">_fit_one</span>(x, s, theta, tol=1e-7):
  <span class="org-variable-name">fit</span> = scmodes.ebpm.wrappers.ebpm_gamma(x, s, tol=tol)
  <span class="org-keyword">if</span> (np.exp(-fit[1]) - theta) / (1 + theta) &lt; 0:
    <span class="org-variable-name">fit</span> = scmodes.ebpm.wrappers.ebpm_gamma(x, s, a=1 / theta, tol=tol)
  <span class="org-keyword">return</span> fit

<span class="org-keyword">def</span> <span class="org-function-name">_fit</span>(x, s, grid):
  <span class="org-keyword">return</span> np.array([_fit_one(x, s, theta) <span class="org-keyword">for</span> theta <span class="org-keyword">in</span> grid])

<span class="org-variable-name">s</span> = 1e5
<span class="org-variable-name">n</span> = 1600
<span class="org-variable-name">log_mu</span> = -10
<span class="org-variable-name">log_phi</span> = -6

np.random.seed(9)
<span class="org-variable-name">lam</span> = st.gamma(a=np.exp(-log_phi), scale=np.exp(log_mu + log_phi)).rvs(n)
<span class="org-variable-name">x</span> = st.poisson(s * lam).rvs(n)

<span class="org-variable-name">fits</span> = <span class="org-builtin">dict</span>()
<span class="org-variable-name">grid</span> = np.logspace(-3, 1, 40)

<span class="org-keyword">print</span>(f<span class="org-string">'fitting theta0 = 0'</span>)
<span class="org-variable-name">fits</span>[0] = _fit(x, s, grid)
<span class="org-keyword">for</span> i, theta0 <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(np.logspace(-4, 0, 5)):
  <span class="org-keyword">print</span>(f<span class="org-string">'fitting theta0 = {theta0:.1g}'</span>)
  np.random.seed(i)
  <span class="org-variable-name">u</span> = st.gamma(a=1 / theta0, scale=theta0).rvs(n)
  <span class="org-variable-name">x</span> = st.poisson(s * lam * u).rvs(n)
  <span class="org-variable-name">fits</span>[theta0] = _fit(x, s, grid)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = colorcet.cm[<span class="org-string">'bmy'</span>]
plt.clf()
plt.gcf().set_size_inches(4, 2.5)
plt.xscale(<span class="org-string">'log'</span>)
<span class="org-keyword">for</span> i, k <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(fits):
  <span class="org-variable-name">temp</span> = fits[k][:,-1] - fits[k][:,-1].<span class="org-builtin">max</span>()
  <span class="org-keyword">if</span> k &gt; 0:
    <span class="org-variable-name">c</span> = cm((np.log(k) - np.log(1e-4)) / (1 - np.log(1e-4)))
  <span class="org-keyword">else</span>:
    <span class="org-variable-name">c</span> = <span class="org-string">'k'</span>
  plt.plot(grid, temp, lw=1, c=c, label=rf<span class="org-string">'$\theta$ = {k:.1g}'</span>)
plt.axvline(x=np.exp(log_phi), lw=1, ls=<span class="org-string">':'</span>, c=<span class="org-string">'k'</span>)
plt.axhline(y=-np.log(10), lw=1, ls=<span class="org-string">':'</span>, c=<span class="org-string">'k'</span>)
plt.legend(title=r<span class="org-string">'True $\theta$'</span>, frameon=<span class="org-constant">False</span>, loc=<span class="org-string">'center left'</span>, bbox_to_anchor=(1, .5))
plt.xticks(np.logspace(-3, 1, 5))
plt.ylim(-10, 0)
plt.xlabel(r<span class="org-string">'Assumed dispersion $\theta$'</span>)
plt.ylabel(<span class="org-string">'Diff ln lik from best'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/ebnbm.org/var-heuristic-squarem.png" alt="var-heuristic-squarem.png">
</p>
</div>
</div>
</div>

<div id="outline-container-org9a061c5" class="outline-3">
<h3 id="org9a061c5">Application to control data</h3>
<div class="outline-text-3" id="text-org9a061c5">
<p>
Fit the EBNBM model to each control data set.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="orga8877d5"><span class="org-keyword">def</span> <span class="org-function-name">_init</span>(j, x, s):
  <span class="org-keyword">return</span> scmodes.ebpm.ebpm_gamma(x[:,j].ravel(), s.ravel())

<span class="org-keyword">def</span> <span class="org-function-name">_fit</span>(theta, x, s, par):
  <span class="org-keyword">return</span> scmodes.ebnbm.ebnbm_gamma(
    x,
    s,
    init=np.hstack([np.exp(par[:,1]), np.exp(par[:,1] - par[:,0]), theta]),
    tol=1e-3,
    max_iters=100_000,
    extrapolate=<span class="org-constant">True</span>,
    fix_g=<span class="org-constant">False</span>,
    fix_theta=<span class="org-constant">True</span>)

<span class="org-variable-name">tasks</span> = control
<span class="org-variable-name">d</span> = tasks[<span class="org-builtin">int</span>(os.environ[<span class="org-string">'SLURM_ARRAY_TASK_ID'</span>])]
<span class="org-keyword">with</span> mp.Pool() <span class="org-keyword">as</span> pool:
  <span class="org-variable-name">dat</span> = data[d]()
  <span class="org-variable-name">x</span> = dat.X.A
  <span class="org-variable-name">s</span> = x.<span class="org-builtin">sum</span>(axis=1, keepdims=<span class="org-constant">True</span>)
  <span class="org-variable-name">par</span> = pool.<span class="org-builtin">map</span>(ft.partial(_init, x=x, s=s), <span class="org-builtin">range</span>(x.shape[1]))
  <span class="org-comment-delimiter"># </span><span class="org-comment">Important: ebpm_gamma can return np.inf is data is consistent with Poisson</span>
  <span class="org-comment-delimiter"># </span><span class="org-comment">observation model. exp(20) is finite and large enough.</span>
  <span class="org-variable-name">par</span> = np.ma.masked_invalid(np.array(par)).filled(20)
  <span class="org-variable-name">grid</span> = np.logspace(-3, 1, 20)
  <span class="org-variable-name">fits</span> = pool.<span class="org-builtin">map</span>(ft.partial(_fit, x=x, s=s, par=par), grid)
<span class="org-keyword">with</span> <span class="org-builtin">open</span>(f<span class="org-string">'/scratch/midway2/aksarkar/modes/ebnbm/ebnbm-{d}.pkl'</span>, <span class="org-string">'wb'</span>) <span class="org-keyword">as</span> f:
  pickle.dump(<span class="org-builtin">dict</span>(<span class="org-builtin">zip</span>(grid, fits)), f)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-sh">sbatch --partition=gpu2 --gres=gpu:1 -w midway2-gpu05 -n1 -c28 --exclusive -a 4
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/bash</span>
<span class="org-builtin">source</span> activate scmodes
python &lt;&lt;EOF
<span class="org-sh-heredoc">&lt;&lt;imports&gt;&gt;</span>
<span class="org-sh-heredoc">import multiprocessing as mp</span>
<span class="org-sh-heredoc">import pickle</span>
<span class="org-sh-heredoc">import os</span>
<span class="org-sh-heredoc">&lt;&lt;data&gt;&gt;</span>
<span class="org-sh-heredoc">&lt;&lt;fit&gt;&gt;</span>
<span class="org-sh-heredoc">EOF</span>
</pre>
</div>

<p>
Read the fitted models.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">elbo</span> = <span class="org-builtin">dict</span>()
<span class="org-keyword">for</span> k <span class="org-keyword">in</span> data:
  <span class="org-keyword">if</span> k <span class="org-keyword">in</span> control:
    <span class="org-keyword">with</span> <span class="org-builtin">open</span>(f<span class="org-string">'/scratch/midway2/aksarkar/modes/ebnbm/ebnbm-{k}.pkl'</span>, <span class="org-string">'rb'</span>) <span class="org-keyword">as</span> f:
      <span class="org-variable-name">fits</span> = pickle.load(f)
      <span class="org-variable-name">elbo</span>[k] = pd.Series({theta: fits[theta][-1] <span class="org-keyword">for</span> theta <span class="org-keyword">in</span> fits})
<span class="org-variable-name">elbo</span> = pd.DataFrame(elbo)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">labels</span> = [<span class="org-string">'Chromium (1)'</span>, <span class="org-string">'Chromium (2)'</span>, <span class="org-string">'Drop-Seq'</span>, <span class="org-string">'GemCode'</span>, <span class="org-string">'InDrops'</span>]
<span class="org-variable-name">keys</span> = [<span class="org-string">'chromium1'</span>, <span class="org-string">'chromium2'</span>, <span class="org-string">'dropseq'</span>, <span class="org-string">'gemcode'</span>, <span class="org-string">'indrops'</span>]
<span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(4, 2.5)
plt.xscale(<span class="org-string">'log'</span>)
<span class="org-keyword">for</span> i, (k, l) <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(<span class="org-builtin">zip</span>(keys, labels)):
  <span class="org-variable-name">n</span>, <span class="org-variable-name">p</span> = data[k]().shape
  <span class="org-variable-name">diff</span> = elbo[k] - elbo[k].<span class="org-builtin">max</span>()
  plt.plot(elbo.index, diff / (n * p), lw=1, label=l, c=cm(i))
plt.legend(title=<span class="org-string">'Dataset'</span>, frameon=<span class="org-constant">False</span>, loc=<span class="org-string">'center left'</span>, bbox_to_anchor=(1, .5))
plt.xticks(np.logspace(-3, 1, 5))
plt.xlabel(r<span class="org-string">'Measurement dispersion'</span>)
plt.ylabel(<span class="org-string">'Diff ELBO per obs\nfrom best'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/ebnbm.org/ebnbm-control.png" alt="ebnbm-control.png">
</p>
</div>

<p>
Fit the heuristic to each data set.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="orgfae5bc5"><span class="org-keyword">def</span> <span class="org-function-name">_fit_one</span>(x, s, theta, tol=1e-7):
  <span class="org-variable-name">fit</span> = scmodes.ebpm.wrappers.ebpm_gamma(x, s, tol=tol)
  <span class="org-keyword">if</span> (np.exp(-fit[1]) - theta) / (1 + theta) &lt; 0:
    <span class="org-variable-name">fit</span> = scmodes.ebpm.wrappers.ebpm_gamma(x, s, a=1 / theta, tol=tol)
  <span class="org-keyword">return</span> fit

<span class="org-variable-name">tasks</span> = control
<span class="org-variable-name">d</span> = tasks[<span class="org-builtin">int</span>(os.environ[<span class="org-string">'SLURM_ARRAY_TASK_ID'</span>])]
<span class="org-keyword">with</span> mp.Pool() <span class="org-keyword">as</span> pool:
  <span class="org-variable-name">dat</span> = data[d]()
  <span class="org-variable-name">x</span> = dat.X.A
  <span class="org-variable-name">s</span> = x.<span class="org-builtin">sum</span>(axis=1)
  <span class="org-variable-name">fits</span> = []
  <span class="org-keyword">for</span> theta <span class="org-keyword">in</span> np.logspace(-3, -2, 100):
    <span class="org-variable-name">_fit</span> = ft.partial(_fit_one, s=s, theta=theta)
    fits.append(pool.<span class="org-builtin">map</span>(_fit, x.T))
  <span class="org-variable-name">fits</span> = np.stack(fits)
  np.save(f<span class="org-string">'/scratch/midway2/aksarkar/modes/ebnbm/nb-heuristic-{d}.npy'</span>, fits)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-sh">sbatch --partition=broadwl -n1 -c28 --exclusive -a 0-4
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/bash</span>
<span class="org-builtin">source</span> activate scmodes
python &lt;&lt;EOF
<span class="org-sh-heredoc">&lt;&lt;imports&gt;&gt;</span>
<span class="org-sh-heredoc">import multiprocessing as mp</span>
<span class="org-sh-heredoc">import os</span>
<span class="org-sh-heredoc">&lt;&lt;data&gt;&gt;</span>
<span class="org-sh-heredoc">&lt;&lt;fit-heuristic&gt;&gt;</span>
<span class="org-sh-heredoc">EOF</span>
</pre>
</div>

<p>
Read the fitted models.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">fits</span> = <span class="org-builtin">dict</span>()
<span class="org-keyword">for</span> k <span class="org-keyword">in</span> control:
  <span class="org-variable-name">fits</span>[k] = np.load(f<span class="org-string">'/scratch/midway2/aksarkar/modes/ebnbm/nb-heuristic-{k}.npy'</span>)
</pre>
</div>

<p>
Plot the profile likelihood.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">labels</span> = [<span class="org-string">'Chromium (1)'</span>, <span class="org-string">'Chromium (2)'</span>, <span class="org-string">'Drop-Seq'</span>, <span class="org-string">'GemCode'</span>, <span class="org-string">'InDrops'</span>]
<span class="org-variable-name">keys</span> = [<span class="org-string">'chromium1'</span>, <span class="org-string">'chromium2'</span>, <span class="org-string">'dropseq'</span>, <span class="org-string">'gemcode'</span>, <span class="org-string">'indrops'</span>]

<span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(4, 2.5)
plt.xscale(<span class="org-string">'log'</span>)
<span class="org-variable-name">grid</span> = np.logspace(-3, -2, 100)
<span class="org-keyword">for</span> i, (k, l) <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(<span class="org-builtin">zip</span>(keys, labels)):
  <span class="org-variable-name">temp</span> = fits[k][:,:,-1].<span class="org-builtin">sum</span>(axis=1)
  <span class="org-variable-name">diff</span> = temp - temp.<span class="org-builtin">max</span>()
  plt.plot(grid, diff, lw=1, label=l, c=cm(i))
plt.ylim(-20, 0)
plt.axhline(y=-np.log(10), lw=1, ls=<span class="org-string">':'</span>, c=<span class="org-string">'k'</span>)
plt.legend(title=<span class="org-string">'Dataset'</span>, frameon=<span class="org-constant">False</span>, loc=<span class="org-string">'center left'</span>, bbox_to_anchor=(1, .5))
plt.xlabel(r<span class="org-string">'Assumed dispersion'</span>)
plt.ylabel(<span class="org-string">'Difference in ln likelihood\nfrom best'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/ebnbm.org/nb-heuristic-control.png" alt="nb-heuristic-control.png">
</p>
</div>

<p>
Report the smallest \(\theta\) for which the likelihood ratio is \(< 1/10\)
compared to the best \(\theta\).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">thresh</span> = -np.log(10)
<span class="org-variable-name">grid</span> = np.logspace(-3, -2, 100)
<span class="org-variable-name">thetahat</span> = <span class="org-builtin">dict</span>()
<span class="org-keyword">for</span> k <span class="org-keyword">in</span> keys:
  <span class="org-variable-name">l</span> = fits[k][:,:,-1].<span class="org-builtin">sum</span>(axis=1)
  <span class="org-variable-name">l</span> -= l.<span class="org-builtin">max</span>()
  <span class="org-variable-name">thetahat</span>[k] = grid[np.where(l &lt; thresh)[0][0]]
pd.Series(thetahat)
</pre>
</div>

<pre class="example">
chromium1    0.004863
chromium2    0.002477
dropseq      0.003199
gemcode      0.001024
indrops      0.001205
dtype: float64
</pre>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Abhishek Sarkar</p>
<p class="date">Created: 2020-09-10 Thu 11:29</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
