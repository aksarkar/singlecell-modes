<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2019-12-10 Tue 11:09 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Imputation of count matrices</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Abhishek Sarkar">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link href="css/bootstrap.min.css" rel="stylesheet" />
<link rel="stylesheet" type="text/css" href="css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="css/supp.css"/>
<style type="text/css">body {width: 60em; margin:auto} pre.src {overflow:auto}</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Imputation of count matrices</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#setup">Setup</a></li>
<li><a href="#methods">Methods</a>
<ul>
<li><a href="#wnmf">Weighted non-negative matrix factorization</a></li>
<li><a href="#wnbmf">Weighted Negative Binomial Matrix Factorization</a></li>
<li><a href="#wglmpca">Weighted GLM-PCA</a></li>
<li><a href="#vae">Incomplete data VAE</a></li>
<li><a href="#datasets">Datasets</a></li>
</ul>
</li>
<li><a href="#results">Results</a>
<ul>
<li><a href="#simulation">Simulated Poisson example</a></li>
<li><a href="#orgfd4d399">Imputation benchmark</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orga364103" class="outline-2">
<h2 id="introduction"><a id="orga364103"></a>Introduction</h2>
<div class="outline-text-2" id="text-introduction">
<p>
The key idea of our approach to modeling scRNA-seq is to separate sampling
variation and expression variation. This approach leads to the following
multi-gene model for scRNA-seq data: \(
  \newcommand\const{\mathrm{const}}
  \newcommand\E[1]{\left\langle #1 \right\rangle}
  \newcommand\vx{\mathbf{x}}
  \newcommand\vw{\mathbf{w}}
  \newcommand\vz{\mathbf{z}}
  \newcommand\mx{\mathbf{X}}
  \newcommand\mU{\mathbf{U}}
  \newcommand\mw{\mathbf{W}}
  \newcommand\mz{\mathbf{Z}}
  \newcommand\ml{\mathbf{L}}
  \newcommand\mf{\mathbf{F}}
  \)
</p>

\begin{align*}
  x_{ij} &\sim \operatorname{Poisson}(\lambda_{ij})\\
  \lambda_{ij} &= h^{-1}((\ml\mf')_{ij})
\end{align*}

<p>
where \(i = 1, \ldots, n\), \(j = 1, \ldots, p\), \(\ml\) is an \(n \times
  K\) matrix, and \(\mf\) is a \(p \times K\) matrix. (Here, we absorb the size
factor into \(\ml\).) We <a href="lra.html">previously used Poisson thinning of
real data sets to benchmark methods</a> which fit this model. The key idea of
this approach is that binomial sampling results in two data matrices which
have the same \(\lambda_{ij}\), and this works even without knowing the
ground truth \(\lambda_{ij}\). However, it is natural to further assume that
expression variation itself can be partitioned into <i>structured</i> and
<i>unstructured</i> variation, e.g.
</p>

\begin{align*}
  x_{ij} &\sim \operatorname{Poisson}(\lambda_{ij})\\
  \lambda_{ij} &= \mu_{ij} u_{ij}\\
  \mu_{ij} &= h^{-1}((\ml\mf')_{ij})\\
  u_{ij} &\sim \operatorname{Gamma}(1/\phi, 1/\phi)
\end{align*}

<p>
<b>Remark</b> This formulation suggests against making a point-Gamma assumption on
\(\lambda_{ij}\), because the implicit assumption on multiplicative random
effect \(u_{ij}\) is that it does not have mean 1.
</p>

<p>
We cannot use Poisson thinning to evaluate methods on fitting this model,
because the evaluation requires the ground truth \(\mu_{ij}\). Here, we study
the problem of masking data entries in an scRNA-seq count matrix and imputing
them by estimating \(\mu_{ij}\). Critically, this masking <i>cannot</i> be
achieved simply by setting some entries to zero, because zero is a valid
non-missing observation in scRNA-seq data. Instead, we require methods which
can handle missing data, e.g., having associated weights. The key idea which
enables fast methods for our Poisson model of interest is that the likelihood
factorizes, so we can maximize the incomplete data log likelihood (or a lower
bound) <i>without</i> marginalizing over unobserved data.
</p>
</div>
</div>

<div id="outline-container-orgd313ff0" class="outline-2">
<h2 id="setup"><a id="orgd313ff0"></a>Setup</h2>
<div class="outline-text-2" id="text-setup">
<div class="org-src-container">
<pre class="src src-ipython" id="orgac614da"><span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> pandas <span class="org-keyword">as</span> pd
<span class="org-keyword">import</span> scipy.stats <span class="org-keyword">as</span> st
<span class="org-keyword">import</span> scmodes
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">%matplotlib inline
%config <span class="org-variable-name">InlineBackend.figure_formats</span> = <span class="org-builtin">set</span>([<span class="org-string">'retina'</span>])
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'figure.facecolor'</span>] = <span class="org-string">'w'</span>
<span class="org-variable-name">plt.rcParams</span>[<span class="org-string">'font.family'</span>] = <span class="org-string">'Nimbus Sans'</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-orge24e3e1" class="outline-2">
<h2 id="methods"><a id="orge24e3e1"></a>Methods</h2>
<div class="outline-text-2" id="text-methods">
</div>
<div id="outline-container-org8f78969" class="outline-3">
<h3 id="wnmf"><a id="org8f78969"></a>Weighted non-negative matrix factorization</h3>
<div class="outline-text-3" id="text-wnmf">
<p>
Weighted non-negative matrix factorization (WNMF;
<a href="https://epubs.siam.org/doi/abs/10.1137/1.9781611972764.58">Zhang et
al. 2006</a>, <a href="https://www.cs.umd.edu/~bhargav/nips2010.pdf">Khanagal et
al. 2010</a>) is an extension of NMF (Lee &amp; Seung 2001) to handle incomplete
data associated with weights \(w_{ij} \in \{0, 1\}\). In our model of
interest, the classical multiplicative updates are in fact EM updates of an
augmented model (Cemgil 2009)
</p>

\begin{align*}
  x_{ij} &= \sum_k z_{ijk}\\
  z_{ijk} &\sim \operatorname{Poisson}(l_{ik} f_{jk})
\end{align*}

<p>
and we can derive how the updates should change for incomplete data. The log
posterior
</p>

\begin{align*}
  \ln p(\mz \mid \mx, \mw, \ml, \mf) &= \ln p(\mx, \mz \mid \ml, \mf, \mw) - \ln p(\mx \mid \ml, \mf, \mw)\\
  &= \sum_{i,j,k} w_{ij} \left[z_{ijk} \ln (l_{ik}f_{jk}) - l_{ik} f_{jk} + \ln\Gamma(z_{ijk} + 1)\right] - \sum_{i,j,k} w_{ij} \left[x_{ij} \ln\left(\sum_k l_{ik}f_{jk}\right) - \sum_k l_{ik} f_{jk} + \ln\Gamma(x_{ij} + 1)\right]\\
  &= \sum_{i, j, k} \left[w_{ij} z_{ijk} \ln\left(\frac{l_{ik}{f_{jk}}}{\sum_t l_{it} f_{jt}}\right)\right] + \const\\
  &= \sum_{i, j} w_{ij} \operatorname{Multinomial}(\cdot; x_{ij}, \frac{l_{i1} f_{j1}}{\sum_t l_{it} f_{jt}}, \ldots, \frac{l_{iK} f_{jK}}{\sum_t l_{it} f_{jt}})
\end{align*}

<p>
Therefore
</p>

<p>
\[ \E{z_{ijk}} = x_{ij} \frac{l_{ik} f_{jk}}{\sum_t l_{it} f_{jt}} \]
</p>

<p>
when \(w_{ij} = 1\), and is missing otherwise. The expected log joint
</p>

<p>
\[ \E{\ln p(\mx, \mz \mid \ml, \mf, \mw)} = \sum_{i, j, k} w_{ij} [\E{z_{ijk}} \ln(l_{ij} f_{jk}) - l_{ik} f_{jk}] + \const \]
</p>

<p>
yielding M step updates
</p>

\begin{align*}
  l_{ik} &:= \frac{\sum_j w_{ij} \E{z_{ijk}}}{\sum_j w_{ij} f_{jk}}\\
  f_{jk} &:= \frac{\sum_i w_{ij} \E{z_{ijk}}}{\sum_i w_{ij} l_{ik}}
\end{align*}

<p>
Plugging in \(\E{z_{ijk}}\) yields the classical multiplicative updates,
modified by introducing the weights in the numerator and denominator.
</p>
</div>
</div>

<div id="outline-container-orgd53f08d" class="outline-3">
<h3 id="wnbmf"><a id="orgd53f08d"></a>Weighted Negative Binomial Matrix Factorization</h3>
<div class="outline-text-3" id="text-wnbmf">
<p>
Negative Binomial Matrix Factorization (NBMF; Gouvert et al 2018) is the
(augmented) model
</p>

\begin{align*}
  x_{ij} &= \sum_{k=1}^K z_{ijk}\\
  z_{ijk} &\sim \operatorname{Poisson}(l_{ik} f_{jk} u_{ij})\\
  u_{ij} &\sim \operatorname{Gamma}(1/\phi_{ij}, 1/\phi_{ij})
\end{align*}

<p>
where the Gamma distribution is parameterized by a shape and a rate. (The
mean of the Gamma distribution is 1, and its variance is \(\phi_{ij}\).)
Gouvert et al. 2018 only consider the case \(\phi_{ij} = \phi\); however,
other natural choices are \(\phi_{ij} = \phi_j\) and \(\phi_{ij} = \phi_i
   \phi_j\). To derive an EM algorithm, first note that
</p>

<p>
\[ p(\mz \mid \mx, \mU, \ml, \mf) 
   = p(\mz \mid \mx, \ml, \mf)
   = \sum_{i, j} \operatorname{Multinomial}(\cdot; x_{ij}, \frac{l_{i1} f_{j1}}{\mu_{ij}}, \ldots, \frac{l_{iK} f_{jK}}{\mu_{ij}}) \]
</p>

<p>
where \(\mu_{ij} \triangleq \sum_t l_{it} f_{jt}\). Further,
</p>

\begin{align*}
  p(u_{ij} \mid \mx, \mz, \ml, \mf) &\propto p(\mx, \mz, u_{ij} \mid \ml, \mf)\\
  &\propto \prod_k u_{ij}^{z_{ijk}} \exp(-l_{ik} f_{jk} u_{ij}) u_{ij}^{1/\phi_{ij} - 1} \exp(-u_{ij} / \phi_{ij})\\
  &\propto u_{ij}^{x_{ij} + 1 / \phi_{ij} - 1} \exp\left(-u_{ij}\left(\mu_{ij} + 1 / \phi_{ij}\right)\right)\\
  &= \operatorname{Gamma}(x_{ij} + 1 / \phi_{ij}, \mu_{ij} + 1 / \phi_{ij})
\end{align*}

<p>
The expected log joint
</p>

\begin{multline*}
  \ell \triangleq \E{\ln p(\mx, \mz, \mU \mid \ml, \mf)} = \sum_{i, j, k} [\E{z_{ijk}}(\E{\ln u_{ij}} + \ln (l_{ik} f_{jk})) - l_{ik} f_{jk} \E{u_{ij}} - \ln\Gamma(x_{ij} + 1)]\\
  + \sum_{i, j} [(1 / \phi_{ij}) \ln(1 / \phi_{ij}) + (1 / \phi_{ij} - 1) \E{\ln u_{ij}} - \E{u_{ij}} / \phi_{ij} - \ln\Gamma(1 / \phi_{ij})]
\end{multline*}

<p>
yielding analytic M step updates
</p>

\begin{align*}
  l_{ik} &= \frac{\sum_j \E{z_{ijk}}}{\sum_j f_{jk} \E{u_{ij}}}\\
  f_{jk} &= \frac{\sum_i \E{z_{ijk}}}{\sum_i l_{ik} \E{u_{ij}}}
\end{align*}

<p>
Using properties of the Gamma distribution,
</p>

\begin{align*}
  \E{u_{ij}} &= \frac{x_{ij} + 1/\phi_{ij}}{\mu_{ij} + 1/\phi_{ij}}\\
  \E{\ln u_{ij}} &= \psi(x_{ij} + 1 / \phi_{ij}) - \ln(\mu_{ij} + 1 / \phi_{ij})
\end{align*}

<p>
Plugging in \(\E{z_{ijk}}\), \(\E{u_{ij}}\) yields multiplicative updates of
a form similar to WNMF. Following the arguments for WNMF, if we introduce
weights \(w_{ij} \in \{0, 1\}\), they will enter the M step updates in the
numerator and denominator.
</p>

<p>
Gouvert et al. 2018 treat \(\phi_{ij}\) as fixed, rather than estimating it
from data. For the easy case \(\phi_{ij} = \phi\), numerical optimization of
the expected log joint with respect to \(\theta = 1 / \phi\) is
straightforward (using e.g. Newton's method):
</p>

\begin{align*}
  \frac{\partial \ell}{\partial \theta} &= \sum_{i, j} \Bigg[1 + \ln \theta + \E{\ln u_{ij}} - \E{u_{ij}} - \psi(\theta)\Bigg]\\
  \frac{\partial^2 \ell}{\partial \theta^2} &= \sum_{i, j} \Bigg[\frac{1}{\theta} - \psi_1(\theta)\Bigg]
\end{align*}

<p>
where \(\psi(\cdot)\) denotes the digamma function and \(\psi_1(\cdot)\)
denotes the trigamma function. It is clear that weights \(w_{ij} \in \{0,
   1\}\) enter inside the sums in these expressions.
</p>
</div>
</div>

<div id="outline-container-org288f584" class="outline-3">
<h3 id="wglmpca"><a id="org288f584"></a>Weighted GLM-PCA</h3>
<div class="outline-text-3" id="text-wglmpca">
<p>
GLM-PCA (<a href="https://arxiv.org/abs/1907.02647">Townes 2019</a>) uses
<a href="https://en.wikipedia.org/wiki/Scoring_algorithm">Fisher scoring</a> to fit
the model assuming \(h = \log\). The weighted version of the problem is
</p>

\begin{align*}
  \ell \triangleq \ln p(\mx \mid \ml, \mf, \mw) &= \sum_{i, j} w_{ij} \left[x_{ij} \sum_k l_{ik} f_{jk} - \exp\left(\sum_t l_{it} f_{jt}\right) \right] + \const\\
  \frac{\partial \ell}{\partial l_{ik}} &= \sum_j w_{ij} \left[x_{ij} f_{jk} - \exp\left(\sum_t l_{it} f_{jt}\right) f_{jk}\right]\\
  \frac{\partial \ell}{\partial f_{jk}} &= \sum_i w_{ij} \left[x_{ij} l_{ik} - \exp\left(\sum_t l_{it} f_{jt}\right) l_{ik}\right]\\
  \mathcal{I}(l_{ik}) &= \sum_j w_{ij} \exp\left(\sum_t l_{it} f_{jt}\right) f_{jk}^2\\
  \mathcal{I}(f_{jk}) &= \sum_i w_{ij} \exp\left(\sum_t l_{it} f_{jt}\right) l_{ik}^2
\end{align*}
</div>
</div>

<div id="outline-container-org52c6bb5" class="outline-3">
<h3 id="vae"><a id="org52c6bb5"></a>Incomplete data VAE</h3>
<div class="outline-text-3" id="text-vae">
<p>
Variational autoencoders fit a generative model parameterized by a neural
network (Kingma and Welling 2014, Rezende and Mohammed 2014). Our model of
interest is
</p>

\begin{align*}
  x_{ij} \mid \lambda_{ij} &\sim \operatorname{Poisson}(\lambda_{ij})\\
  \lambda_{ij} \mid \vz_i, u_{ij} &= \mu(\vz_i)_j\, u_{ij}\\
  u_{ij} &\sim p(u_{ij})\\
  \vz_i &\sim \mathcal{N}(\boldsymbol{0}, \mathbf{I}_K)
\end{align*}

<p>
where \(\mu(\cdot)\) is a \(p\)-dimensional output of a fully connected
feed-forward neural network. To approximate the intractable posterior
\(p(\vz_i \mid \vx_i)\), we use a variational approximation
</p>

<p>
\[ q(\vz_i \mid \vx_i) = \mathcal{N}(m(\vz_i), \operatorname{diag}(S(\vz_i))) \]
</p>

<p>
where \(m(\cdot), S(\cdot)\) are \(K\)-dimensional outputs of a FF
network. Fitting VAEs with incomplete data has only recently been studied
(<a href="https://arxiv.org/abs/1807.03653">Nazabal et al. 2018</a>,
<a href="https://arxiv.org/abs/1812.02633">Mattei and Frellsen 2018</a>). The key
idea is that if the coordinates of \(\vx_i\) are separable (meaning the
likelihood factorizes) and coordinates are missing at random, then
maximizing the log likelihood (or a lower bound to the log likelihood) of
only the observed coordinates is a statistically sound procedure. In this
case, the lower bound is
</p>

<p>
\[ \ell \triangleq \sum_{i, j} \E{w_{ij} \ln\left(\int_0^\infty \operatorname{Poisson}(x_{ij}; \mu(\vz_i)_j\, u_{ij})\; dp(u_{ij})\right)} - \sum_{i} \E{\ln\left(\frac{q(\vz_i \mid f(\vx_i))}{p(\vz_i)}\right)}\]
</p>

<p>
where \(f\) is some imputation function (e.g., fill in missing values with
zero) and expectations are taken with respect to \(q\). For easy choices of
\(p(u_{ij})\), the integrals inside the expectation are analytic. Mattei and
Frellsen 2018 suggest filling missing data with 0 in the encoder network
works, and that a tighter bound can be achieved using importance sampling
(<a href="https://arxiv.org/abs/1509.00519">Burda et al. 2016</a>)
</p>

<p>
\[ \ell_{\text{IWAE}} \triangleq \sum_i \E{\ln\left(\frac{1}{S} \sum_{s=1}^S \frac{p(\vx_i, \vz_i^{(s)})}{q(\vz_i^{(s)} \mid \vx_i)}\right)} \]
</p>
</div>
</div>

<div id="outline-container-orgdb67a90" class="outline-3">
<h3 id="datasets"><a id="orgdb67a90"></a>Datasets</h3>
<div class="outline-text-3" id="text-datasets">
<div class="org-src-container">
<pre class="src src-ipython" id="org2a19ee0"><span class="org-keyword">def</span> <span class="org-function-name">_read_10x</span>(k, min_detect=0.01, n_cells=1000, seed=1):
  <span class="org-keyword">return</span> scmodes.dataset.read_10x(f<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/{k}/filtered_matrices_mex/hg19/'</span>, min_detect=0.01, return_df=<span class="org-constant">True</span>).sample(n=n_cells, axis=0, random_state=seed)

<span class="org-keyword">def</span> <span class="org-function-name">_mix_10x</span>(k1, k2, min_detect=0.01, n_cells=1000, seed=1):
  <span class="org-variable-name">x1</span> = scmodes.dataset.read_10x(f<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/{k1}/filtered_matrices_mex/hg19/'</span>, return_df=<span class="org-constant">True</span>, min_detect=0)
  <span class="org-variable-name">x2</span> = scmodes.dataset.read_10x(f<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-ideas/data/10xgenomics/{k2}/filtered_matrices_mex/hg19/'</span>, return_df=<span class="org-constant">True</span>, min_detect=0)
  <span class="org-keyword">return</span> scmodes.dataset.synthetic_mix(x1, x2, min_detect=min_detect)[0].sample(n=n_cells, axis=0, random_state=seed)

<span class="org-keyword">def</span> <span class="org-function-name">_cd8_cd19_mix</span>(**kwargs):
  <span class="org-keyword">return</span> _mix_10x(<span class="org-string">'cytotoxic_t'</span>, <span class="org-string">'b_cells'</span>, **kwargs)

<span class="org-keyword">def</span> <span class="org-function-name">_cyto_naive_mix</span>(**kwargs):
  <span class="org-keyword">return</span> _mix_10x(<span class="org-string">'cytotoxic_t'</span>, <span class="org-string">'naive_t'</span>, **kwargs)

<span class="org-variable-name">data</span> = {
  <span class="org-string">'cytotoxic_t'</span>: <span class="org-keyword">lambda</span>: _read_10x(<span class="org-string">'cytotoxic_t'</span>),
  <span class="org-string">'b_cells'</span>: <span class="org-keyword">lambda</span>: _read_10x(<span class="org-string">'b_cells'</span>),
  <span class="org-string">'ipsc'</span>: <span class="org-keyword">lambda</span>: scmodes.dataset.ipsc(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/'</span>, return_df=<span class="org-constant">True</span>).sample(n=1000, axis=0, random_state=1),
  <span class="org-string">'cytotoxic_t-b_cells'</span>: _cd8_cd19_mix,
  <span class="org-string">'cytotoxic_t-naive_t'</span>: _cyto_naive_mix,
  <span class="org-string">'pbmcs_68k'</span>: <span class="org-keyword">lambda</span>: _read_10x(<span class="org-string">'fresh_68k_pbmc_donor_a'</span>),
}
</pre>
</div>

<p>
Report the data dimensions.
</p>

<div class="org-src-container">
<pre class="src src-ipython">pd.DataFrame([data[k]().shape <span class="org-keyword">for</span> k <span class="org-keyword">in</span> data],
             columns=[<span class="org-string">'num_cells'</span>, <span class="org-string">'num_genes'</span>],
             index=data.keys())
</pre>
</div>

<pre class="example">
num_cells  num_genes
cytotoxic_t               1000       6530
b_cells                   1000       6417
ipsc                      1000       9957
cytotoxic_t-b_cells       1000       6647
cytotoxic_t-naive_t       1000       6246
pbmcs_68k                 1000       6502
</pre>

<p>
Report the proportion of zeros.
</p>

<div class="org-src-container">
<pre class="src src-ipython">pd.Series({k: (data[k]() == 0).values.mean() <span class="org-keyword">for</span> k <span class="org-keyword">in</span> data})
</pre>
</div>

<pre class="example">
cytotoxic_t            0.914572
b_cells                0.921772
ipsc                   0.291596
cytotoxic_t-b_cells    0.920908
cytotoxic_t-naive_t    0.917528
pbmcs_68k              0.920052
dtype: float64
</pre>
</div>
</div>
</div>

<div id="outline-container-org2d627c5" class="outline-2">
<h2 id="results"><a id="org2d627c5"></a>Results</h2>
<div class="outline-text-2" id="text-results">
</div>
<div id="outline-container-org3e578ad" class="outline-3">
<h3 id="simulation"><a id="org3e578ad"></a>Simulated Poisson example</h3>
<div class="outline-text-3" id="text-simulation">
<p>
Simulate some data where \([\lambda_{ij}]\) is rank 3.
</p>

<div class="org-src-container">
<pre class="src src-ipython">np.random.seed(0)
<span class="org-variable-name">n</span> = 500
<span class="org-variable-name">p</span> = 1000
<span class="org-variable-name">k</span> = 3
<span class="org-variable-name">l</span> = np.random.lognormal(sigma=.5, size=(n, k))
<span class="org-variable-name">f</span> = np.random.lognormal(sigma=.5, size=(p, k))
<span class="org-variable-name">lam</span> = l.dot(f.T)
<span class="org-variable-name">x</span> = np.random.poisson(lam=lam)
</pre>
</div>

<p>
Get the oracle rank of \([\ln \lambda_{ij}]\).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">d</span> = np.linalg.svd(np.log(lam))[1]
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
plt.gcf().set_size_inches(5, 3)
plt.plot(np.arange(n), d, lw=1, c=<span class="org-string">'k'</span>)
plt.xlabel(<span class="org-string">'Singular value'</span>)
plt.ylabel(<span class="org-string">'Magnitude'</span>)
</pre>
</div>

<pre class="example">
Text(0, 0.5, 'Magnitude')

</pre>

<div class="figure">
<p><img src="figure/imputation.org/sim-spectrum.png" alt="sim-spectrum.png">
</p>
</div>

<div class="org-src-container">
<pre class="src src-ipython">np.where(d &lt;= 1e-4)[0].<span class="org-builtin">min</span>()
</pre>
</div>

<pre class="example">
29

</pre>

<p>
Mask 10% of entries at random. For each choice of rank, evaluate the
imputation loss of different methods.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">imputation_res</span> = []
<span class="org-keyword">for</span> rank <span class="org-keyword">in</span> <span class="org-builtin">range</span>(1, 11):
  <span class="org-keyword">for</span> method <span class="org-keyword">in</span> (<span class="org-string">'oracle'</span>, <span class="org-string">'ebpm_point'</span>, <span class="org-string">'wnmf'</span>, <span class="org-string">'wglmpca'</span>):
    <span class="org-keyword">try</span>:
      <span class="org-variable-name">loss</span> = <span class="org-builtin">getattr</span>(scmodes.benchmark, f<span class="org-string">'imputation_score_{method}'</span>)(x, rank=rank, frac=0.1, seed=0)
    <span class="org-keyword">except</span> <span class="org-type">RuntimeError</span>:
      <span class="org-comment-delimiter"># </span><span class="org-comment">WGLMPCA fails often</span>
      <span class="org-variable-name">loss</span> = np.nan
    imputation_res.append([method, rank, loss])
<span class="org-variable-name">imputation_res</span> = pd.DataFrame(imputation_res, columns=[<span class="org-string">'method'</span>, <span class="org-string">'rank'</span>, <span class="org-string">'loss'</span>])
</pre>
</div>

<p>
Plot the results.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(4, 3)
<span class="org-keyword">for</span> i, (k, g) <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(imputation_res.groupby(<span class="org-string">'method'</span>)):
  plt.plot(g[<span class="org-string">'rank'</span>], g[<span class="org-string">'loss'</span>], lw=1, marker=<span class="org-constant">None</span>, c=cm(i), label=k.upper())
plt.axvline(x=3, lw=1, ls=<span class="org-string">':'</span>, c=<span class="org-string">'k'</span>)
plt.legend(frameon=<span class="org-constant">False</span>, loc=<span class="org-string">'center left'</span>, bbox_to_anchor=(1, .5))
plt.xticks(np.arange(1, 11), np.arange(1, 11))
plt.xlabel(<span class="org-string">'Assumed rank'</span>)
plt.ylabel(<span class="org-string">'Poisson loss'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/imputation.org/sim.png" alt="sim.png">
</p>
</div>

<p>
Zoom in on WNMF and WGLMPCA.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cm</span> = plt.get_cmap(<span class="org-string">'Dark2'</span>)
plt.clf()
plt.gcf().set_size_inches(4, 3)
<span class="org-keyword">for</span> i, (k, g) <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(imputation_res.groupby(<span class="org-string">'method'</span>)):
  <span class="org-keyword">if</span> k <span class="org-keyword">in</span> (<span class="org-string">'wnmf'</span>, <span class="org-string">'wglmpca'</span>):
    plt.plot(g[<span class="org-string">'rank'</span>], g[<span class="org-string">'loss'</span>], lw=1, marker=<span class="org-constant">None</span>, c=cm(i), label=k.upper())
plt.axvline(x=3, lw=1, ls=<span class="org-string">':'</span>, c=<span class="org-string">'k'</span>)
plt.legend(frameon=<span class="org-constant">False</span>, loc=<span class="org-string">'center left'</span>, bbox_to_anchor=(1, .5))
plt.xticks(np.arange(1, 11), np.arange(1, 11))
plt.xlabel(<span class="org-string">'Assumed rank'</span>)
plt.ylabel(<span class="org-string">'Poisson loss'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/imputation.org/sim-inset.png" alt="sim-inset.png">
</p>
</div>

<p>
Mask the same 10% of the entries, and look at the predicted values.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> scmodes.benchmark.imputation

<span class="org-variable-name">w</span> = scmodes.benchmark.imputation._mask_entries(x, frac=0.1, seed=0)
<span class="org-variable-name">wnmf_res</span> = scmodes.lra.nmf(x, w=w, rank=3)
<span class="org-comment-delimiter"># </span><span class="org-comment">Hack</span>
<span class="org-variable-name">wglmpca_res</span> = scmodes.lra.glmpca(.1 * x, w=w, rank=3, max_iters=5000)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">wnmf_pred</span> = wnmf_res[0].dot(wnmf_res[1].T)[~w].ravel()
<span class="org-variable-name">wglmpca_pred</span> = np.exp(np.log(10) + wglmpca_res[0].dot(wglmpca_res[1].T))[~w].ravel()
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">pd.Series({
  <span class="org-string">'wnmf'</span>: st.poisson(mu=wnmf_pred).logpmf(x[~w].ravel()).<span class="org-builtin">sum</span>(),
  <span class="org-string">'wglmpca'</span>: st.poisson(mu=wglmpca_pred).logpmf(x[~w].ravel()).<span class="org-builtin">sum</span>()})
</pre>
</div>

<pre class="example">
wnmf      -100876.82653
wglmpca   -101614.49192
dtype: float64
</pre>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
plt.gcf().set_size_inches(3, 3)
plt.scatter(wnmf_pred, wglmpca_pred, s=1, c=<span class="org-string">'k'</span>, alpha=0.1)
plt.plot([0, 25], [0, 25], lw=1, ls=<span class="org-string">':'</span>, c=<span class="org-string">'r'</span>)
plt.xlabel(<span class="org-string">'WNMF predicted value'</span>)
plt.ylabel(<span class="org-string">'WGLMPCA predicted value'</span>)
plt.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/imputation.org/sim-pred.png" alt="sim-pred.png">
</p>
</div>
</div>
</div>

<div id="outline-container-orgfd4d399" class="outline-3">
<h3 id="orgfd4d399">Imputation benchmark</h3>
<div class="outline-text-3" id="text-orgfd4d399">
<div class="org-src-container">
<pre class="src src-sh">sbatch --partition=broadwl -n1 -c28 --exclusive --time=12:00:00 --job-name=imputation-benchmark -a 0-5
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/bash</span>
<span class="org-builtin">source</span> activate scmodes
python &lt;&lt;EOF
<span class="org-sh-heredoc">&lt;&lt;imports&gt;&gt;</span>
import os
import sys
&lt;&lt;data&gt;&gt;
<span class="org-sh-heredoc">tasks = [(k, m) for k in data for m in ('ebpm_point',)]</span>
<span class="org-sh-heredoc">d, m = tasks[int(os.environ['SLURM_ARRAY_TASK_ID'])]</span>
<span class="org-sh-heredoc">x = data[d]()</span>
<span class="org-sh-heredoc">res = scmodes.benchmark.evaluate_imputation(x.values, methods=[m], n_trials=10, max_retries=3)</span>
<span class="org-sh-heredoc">res.to_csv(f'/scratch/midway2/aksarkar/modes/imputation/{d}-{m}.txt.gz', compression='gzip', sep='\t')</span>
<span class="org-sh-heredoc">EOF</span>
</pre>
</div>

<p>
Move the results to permanent storage.
</p>

<div class="org-src-container">
<pre class="src src-sh">rsync -au /scratch/midway2/aksarkar/modes/imputation/ /project2/mstephens/aksarkar/projects/singlecell-modes/data/imputation/
</pre>
</div>

<p>
Read the results.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">methods</span> = [<span class="org-string">'oracle'</span>, <span class="org-string">'ebpm_point'</span>, <span class="org-string">'wnmf'</span>, <span class="org-string">'wglmpca'</span>]
<span class="org-variable-name">imputation_res</span> = (pd.concat({(d, m): pd.read_csv(f<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-modes/data/imputation/{d}-{m}.txt.gz'</span>, sep=<span class="org-string">'\t'</span>, index_col=0)
                            <span class="org-keyword">for</span> d <span class="org-keyword">in</span> data <span class="org-keyword">for</span> m <span class="org-keyword">in</span> methods})
                  .reset_index(level=0)
                  .reset_index(drop=<span class="org-constant">True</span>)
                  .rename({<span class="org-string">'level_0'</span>: <span class="org-string">'dataset'</span>}, axis=1))
</pre>
</div>

<p>
Plot the results.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">labels</span> = [<span class="org-string">'Oracle'</span>, <span class="org-string">'EBPM-Point'</span>, <span class="org-string">'WNMF'</span>, <span class="org-string">'WGLMPCA'</span>]
plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(1, <span class="org-builtin">len</span>(data), sharey=<span class="org-constant">True</span>)
fig.set_size_inches(8, 2.5)
<span class="org-keyword">for</span> a, d, t <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(ax, data, [<span class="org-string">'T cells'</span>, <span class="org-string">'B cells'</span>, <span class="org-string">'iPSC'</span>, <span class="org-string">'T cell/B cell'</span>, <span class="org-string">'Cytotoxic/naive T'</span>, <span class="org-string">'PBMC'</span>]):
  <span class="org-variable-name">g</span> = imputation_res[imputation_res[<span class="org-string">'dataset'</span>] == d]
  a.set_yscale(<span class="org-string">'log'</span>)
  <span class="org-keyword">for</span> x, (m, k) <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(<span class="org-builtin">zip</span>(methods, labels)):
    <span class="org-variable-name">y</span> = g.loc[g[<span class="org-string">'method'</span>] == m]
    a.scatter(x + np.random.normal(size=y.shape[0], scale=0.1), y[<span class="org-string">'loss'</span>], s=2, c=<span class="org-string">'k'</span>, zorder=4)
  a.set_title(t)
  a.grid(c=<span class="org-string">'0.8'</span>, lw=1, axis=<span class="org-string">'x'</span>)
  a.set_xticks(np.arange(4))
  a.set_xticklabels(labels, rotation=90)
  a.set_xlabel(<span class="org-string">'Method'</span>)
ax[0].set_ylabel(<span class="org-string">'Poisson loss'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/imputation.org/pois-loss.png" alt="pois-loss.png">
</p>
</div>

<p>
Plot the improvement over mean imputation.
</p>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(2, 3, sharex=<span class="org-constant">True</span>)
fig.set_size_inches(5, 4)
<span class="org-keyword">for</span> a, d, t <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(ax.ravel(), data, [<span class="org-string">'T cells'</span>, <span class="org-string">'B cells'</span>, <span class="org-string">'iPSC'</span>, <span class="org-string">'T cell/B cell'</span>, <span class="org-string">'Cytotoxic/naive T'</span>, <span class="org-string">'PBMC'</span>]):
  <span class="org-variable-name">g</span> = imputation_res[imputation_res[<span class="org-string">'dataset'</span>] == d]
  <span class="org-variable-name">base</span> = g.loc[g[<span class="org-string">'method'</span>] == <span class="org-string">'ebpm_point'</span>]
  <span class="org-keyword">for</span> x, m <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(methods[2:]):
    <span class="org-variable-name">y</span> = g.loc[g[<span class="org-string">'method'</span>] == m]
    a.scatter(x + np.random.normal(size=y.shape[0], scale=0.1),
              1e5 * (base[<span class="org-string">'loss'</span>].values - y[<span class="org-string">'loss'</span>].values), s=2, c=<span class="org-string">'k'</span>, zorder=4)
  a.axhline(y=0, c=<span class="org-string">'r'</span>, lw=1, ls=<span class="org-string">':'</span>)
  a.set_title(t)
  a.grid(c=<span class="org-string">'0.8'</span>, lw=1, axis=<span class="org-string">'x'</span>)
<span class="org-keyword">for</span> a <span class="org-keyword">in</span> ax[-1]:
  a.set_xticks(np.arange(2))
  a.set_xticklabels([m.upper() <span class="org-keyword">for</span> m <span class="org-keyword">in</span> methods[2:]], rotation=90)
  a.set_xlabel(<span class="org-string">'Method'</span>)
<span class="org-variable-name">a</span> = fig.add_subplot(111, frameon=<span class="org-constant">False</span>)
a.tick_params(labelcolor=<span class="org-string">'none'</span>, top=<span class="org-string">'off'</span>, bottom=<span class="org-string">'off'</span>, left=<span class="org-string">'off'</span>, right=<span class="org-string">'off'</span>)
a.set_ylabel(<span class="org-string">'Improvement in Poisson loss\nover mean imputation'</span>)
fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/imputation.org/pois-loss-improvement.png" alt="pois-loss-improvement.png">
</p>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Abhishek Sarkar</p>
<p class="date">Created: 2019-12-10 Tue 11:09</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
